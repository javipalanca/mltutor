"""
Este m√≥dulo contiene funciones para evaluar modelos de √°rboles de decisi√≥n.
Incluye funciones para calcular m√©tricas y visualizar resultados de clasificaci√≥n y regresi√≥n.
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, mean_squared_error, confusion_matrix,
    classification_report, precision_recall_fscore_support,
    r2_score, mean_absolute_error

)

from utils import get_image_download_link, show_code_with_download


def evaluate_classification_model(y_test, y_pred, class_names):
    """
    Eval√∫a un modelo de clasificaci√≥n y devuelve las m√©tricas principales.

    Parameters:
    -----------
    y_test : array
        Valores reales
    y_pred : array
        Valores predichos
    class_names : list
        Nombres de las clases

    Returns:
    --------
    dict
        Diccionario con las m√©tricas del modelo
    """
    # Calcular m√©tricas
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(
        y_test, y_pred, target_names=class_names, output_dict=True)
    cm = confusion_matrix(y_test, y_pred)

    # Extraer precision y recall promedio ponderado para compatibilidad
    precision = report.get('weighted avg', {}).get('precision', 0.0)
    recall = report.get('weighted avg', {}).get('recall', 0.0)

    return {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "report": report,
        "confusion_matrix": cm,
        "y_pred": y_pred
    }


def evaluate_regression_model(y_test, y_pred):
    """
    Eval√∫a un modelo de regresi√≥n y devuelve las m√©tricas principales.

    Parameters:
    -----------
    y_test : array
        Valores reales
    y_pred : array
        Valores predichos

    Returns:
    --------
    dict
        Diccionario con las m√©tricas del modelo
    """
    # Calcular m√©tricas
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    # np.mean(np.abs(y_test - y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)  # 1 - np.sum((y_test - y_pred)**2) / \
    # np.sum((y_test - np.mean(y_test))**2)

    return {
        "mse": mse,
        "rmse": rmse,
        "mae": mae,
        "r2": r2,
        "y_pred": y_pred
    }


def show_detailed_evaluation(y_test, y_pred, class_names, tree_type):
    """
    Muestra una evaluaci√≥n detallada del modelo con gr√°ficos y explicaciones.

    Parameters:
    -----------
    y_test : array
        Valores reales
    y_pred : array
        Valores predichos
    class_names : list
        Nombres de las clases
    tree_type : str
        Tipo de √°rbol ('Clasificaci√≥n' o 'Regresi√≥n')
    """
    if tree_type == "Clasificaci√≥n":
        # Calcular m√©tricas
        report = classification_report(
            y_test, y_pred, target_names=class_names, output_dict=True)
        report_df = pd.DataFrame(report).transpose()

        # M√©tricas globales
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Exactitud (Accuracy)", f"{report['accuracy']:.4f}",
                      help="Proporci√≥n total de predicciones correctas")
        with col2:
            st.metric("Precisi√≥n media", f"{report['weighted avg']['precision']:.4f}",
                      help="Media ponderada de la precisi√≥n de cada clase")
        with col3:
            st.metric("Exhaustividad media", f"{report['weighted avg']['recall']:.4f}",
                      help="Media ponderada de la exhaustividad de cada clase")
        with col4:
            st.metric("F1-Score medio", f"{report['weighted avg']['f1-score']:.4f}",
                      help="Media arm√≥nica de precisi√≥n y exhaustividad")

        # M√©tricas por clase
        st.markdown("### M√©tricas por clase")

        # Excluir filas avg y accuracy del dataframe
        report_by_class = report_df.drop(
            ['accuracy', 'macro avg', 'weighted avg'], errors='ignore')

        # Matriz de confusi√≥n
        cm = confusion_matrix(y_test, y_pred)
        fig_cm, ax_cm = plt.subplots(figsize=(6, 5))  # Reducir tama√±o
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax_cm,
                    xticklabels=class_names, yticklabels=class_names)
        ax_cm.set_xlabel('Predicci√≥n')
        ax_cm.set_ylabel('Real')
        ax_cm.set_title('Matriz de Confusi√≥n')

        col1, col2 = st.columns([1, 1])
        with col1:
            # Mostrar con tama√±o reducido
            col_inner1, col_inner2, col_inner3 = st.columns([1, 3, 1])
            with col_inner2:
                st.pyplot(fig_cm, use_container_width=True)

            st.markdown(get_image_download_link(
                fig_cm, "matriz_confusion", "üì• Descargar matriz de confusi√≥n"), unsafe_allow_html=True)

            # Mostrar c√≥digo para generar la matriz de confusi√≥n
            code_cm = """
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Calcular la matriz de confusi√≥n
cm = confusion_matrix(y_test, y_pred)

# Crear el gr√°fico
fig, ax = plt.subplots(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax,
            xticklabels=class_names, yticklabels=class_names)
ax.set_xlabel('Predicci√≥n')
ax.set_ylabel('Real')
ax.set_title('Matriz de Confusi√≥n')

# Para mostrar en Streamlit
# st.pyplot(fig)

# Para uso normal en Python/Jupyter
# plt.tight_layout()
# plt.show()
"""
            show_code_with_download(
                code_cm, "C√≥digo para generar la matriz de confusi√≥n", "matriz_confusion.py")

        with col2:
            st.dataframe(report_by_class.style.format({
                'precision': '{:.4f}',
                'recall': '{:.4f}',
                'f1-score': '{:.4f}',
                'support': '{:.0f}'
            }))

            with st.expander("üìä Explicaci√≥n de m√©tricas"):
                st.markdown("""
                - **Precisi√≥n**: De todas las muestras que se predijeron como clase X, ¬øqu√© porcentaje eran realmente X?
                - **Exhaustividad (Recall)**: De todas las muestras que realmente son clase X, ¬øqu√© porcentaje se predijo correctamente?
                - **F1-Score**: Media arm√≥nica de precisi√≥n y exhaustividad. √ötil cuando las clases est√°n desbalanceadas.
                - **Support**: N√∫mero de muestras de cada clase en el conjunto de prueba.
                """)

        # Visualizaci√≥n avanzada - Predicciones correctas e incorrectas
        st.markdown("### Visualizaci√≥n de Predicciones")

        # Crear dataframe con resultados
        results_df = pd.DataFrame({
            'Real': [class_names[x] for x in y_test],
            'Predicci√≥n': [class_names[x] for x in y_pred],
            'Correcto': y_test == y_pred
        })

        # Mostrar algunas muestras
        col_viz1, col_viz2 = st.columns(2)
        with col_viz1:
            st.markdown("#### Muestras correctamente clasificadas")
            correct_samples = results_df[results_df['Correcto']].head(10)
            st.dataframe(correct_samples, height=300)

        with col_viz2:
            st.markdown("#### Muestras incorrectamente clasificadas")
            incorrect_samples = results_df[~results_df['Correcto']].head(10)
            if len(incorrect_samples) > 0:
                st.dataframe(incorrect_samples, height=300)
            else:
                st.info("¬°Todas las muestras fueron clasificadas correctamente!")

        # Gr√°fico de precisi√≥n por clase
        fig_prec, ax_prec = plt.subplots(figsize=(8, 4))  # Reducir altura
        prec_by_class = {
            class_name: report[class_name]['precision'] for class_name in class_names}
        sns.barplot(x=list(prec_by_class.keys()), y=list(
            prec_by_class.values()), ax=ax_prec)
        ax_prec.set_ylim(0, 1)
        ax_prec.set_title('Precisi√≥n por clase')
        ax_prec.set_ylabel('Precisi√≥n')
        ax_prec.set_xlabel('Clase')

        # Mostrar con tama√±o reducido
        col1, col2, col3 = st.columns([1, 3, 1])
        with col2:
            st.pyplot(fig_prec, use_container_width=True)

        st.markdown(get_image_download_link(
            fig_prec, "precision_por_clase", "üì• Descargar gr√°fico de precisi√≥n"), unsafe_allow_html=True)

        # Mostrar c√≥digo para generar el gr√°fico de precisi√≥n por clase
        code_prec = """
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report
import pandas as pd

# Obtener el reporte de clasificaci√≥n
report = classification_report(y_test, y_pred, target_names=class_names, output_dict=True)

# Extraer precisi√≥n por clase
prec_by_class = {
    class_name: report[class_name]['precision'] for class_name in class_names
}

# Crear el gr√°fico
fig, ax = plt.subplots(figsize=(10, 5))
sns.barplot(x=list(prec_by_class.keys()), y=list(prec_by_class.values()), ax=ax)
ax.set_ylim(0, 1)
ax.set_title('Precisi√≥n por clase')
ax.set_ylabel('Precisi√≥n')
ax.set_xlabel('Clase')

# Para mostrar en Streamlit
# st.pyplot(fig)

# Para uso normal en Python/Jupyter
# plt.tight_layout()
# plt.show()
"""
        show_code_with_download(
            code_prec, "C√≥digo para generar el gr√°fico de precisi√≥n", "precision_por_clase.py")
    else:
        # M√©tricas para regresi√≥n
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Error Cuadr√°tico Medio (MSE)", f"{mse:.4f}",
                      help="Promedio de los errores al cuadrado. Penaliza m√°s los errores grandes.")
        with col2:
            st.metric("RMSE", f"{rmse:.4f}",
                      help="Ra√≠z cuadrada del MSE. En las mismas unidades que la variable objetivo.")
        with col3:
            st.metric("Error Absoluto Medio (MAE)", f"{mae:.4f}",
                      help="Promedio de los errores absolutos. Menos sensible a valores at√≠picos.")
        with col4:
            st.metric("R¬≤ Score", f"{r2:.4f}",
                      help="Proporci√≥n de la varianza explicada por el modelo. 1 es predicci√≥n perfecta.")

        # Gr√°fico de predicciones vs valores reales
        fig, ax = plt.subplots(figsize=(6, 5))  # Reducir tama√±o
        scatter = ax.scatter(y_test, y_pred, alpha=0.5,
                             c=np.abs(y_test - y_pred), cmap='viridis')
        ax.plot([y_test.min(), y_test.max()], [
                y_test.min(), y_test.max()], 'r--')
        ax.set_xlabel('Valores reales')
        ax.set_ylabel('Predicciones')
        ax.set_title('Predicciones vs Valores reales')
        plt.colorbar(scatter, ax=ax, label='Error absoluto')

        # Mostrar con tama√±o reducido
        col1, col2, col3 = st.columns([1, 3, 1])
        with col2:
            st.pyplot(fig, use_container_width=True)

        st.markdown(get_image_download_link(
            fig, "predicciones_vs_reales", "üì• Descargar gr√°fico"), unsafe_allow_html=True)

        # Mostrar el c√≥digo para generar el gr√°fico de predicciones vs valores reales
        code_pred = """
import matplotlib.pyplot as plt
import numpy as np

# Crear el gr√°fico
fig, ax = plt.subplots(figsize=(8, 6))
scatter = ax.scatter(y_test, y_pred, alpha=0.5,
                    c=np.abs(y_test - y_pred), cmap='viridis')
ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
ax.set_xlabel('Valores reales')
ax.set_ylabel('Predicciones')
ax.set_title('Predicciones vs Valores reales')
plt.colorbar(scatter, ax=ax, label='Error absoluto')

# Para mostrar en Streamlit
# st.pyplot(fig)

# Para uso normal en Python/Jupyter
# plt.tight_layout()
# plt.show()
"""
        show_code_with_download(
            code_pred, "C√≥digo para generar este gr√°fico", "predicciones_vs_reales.py")

        # Distribuci√≥n de errores
        fig_err, ax_err = plt.subplots(figsize=(6, 4))  # Reducir tama√±o
        errors = y_test - y_pred
        sns.histplot(errors, kde=True, ax=ax_err)
        ax_err.axvline(x=0, color='r', linestyle='--')
        ax_err.set_title('Distribuci√≥n de errores')
        ax_err.set_xlabel('Error (Real - Predicci√≥n)')

        # Mostrar con tama√±o reducido
        col1, col2, col3 = st.columns([1, 3, 1])
        with col2:
            st.pyplot(fig_err, use_container_width=True)

        st.markdown(get_image_download_link(
            fig_err, "distribucion_errores", "üì• Descargar gr√°fico"), unsafe_allow_html=True)

        # Mostrar el c√≥digo para generar el gr√°fico de distribuci√≥n de errores
        code_err = """
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Calcular los errores
errors = y_test - y_pred

# Crear el gr√°fico
fig, ax = plt.subplots(figsize=(8, 6))
sns.histplot(errors, kde=True, ax=ax)
ax.axvline(x=0, color='r', linestyle='--')
ax.set_title('Distribuci√≥n de errores')
ax.set_xlabel('Error (Real - Predicci√≥n)')

# Para mostrar en Streamlit
# st.pyplot(fig)

# Para uso normal en Python/Jupyter
# plt.tight_layout()
# plt.show()
"""
        show_code_with_download(
            code_err, "C√≥digo para generar este gr√°fico", "distribucion_errores.py")

        with st.expander("üìä Explicaci√≥n de m√©tricas de regresi√≥n"):
            st.markdown("""
            - **MSE (Error Cuadr√°tico Medio)**: Promedio de los errores al cuadrado. Penaliza m√°s los errores grandes.
            - **RMSE (Ra√≠z del Error Cuadr√°tico Medio)**: Ra√≠z cuadrada del MSE. Est√° en las mismas unidades que la variable objetivo.
            - **MAE (Error Absoluto Medio)**: Promedio de los valores absolutos de los errores. Menos sensible a valores at√≠picos que MSE.
            - **R¬≤ Score**: Indica qu√© proporci√≥n de la varianza en la variable dependiente es predecible. 1 es predicci√≥n perfecta, 0 significa que el modelo no es mejor que predecir la media.
            """)


def show_prediction_path(tree_model, X_new, feature_names, class_names=None):
    """
    Muestra el camino de decisi√≥n para una predicci√≥n espec√≠fica.

    Parameters:
    -----------
    tree_model : DecisionTreeClassifier o DecisionTreeRegressor
        Modelo entrenado
    X_new : array
        Ejemplo para predecir
    feature_names : list
        Nombres de las caracter√≠sticas
    class_names : list, optional
        Nombres de las clases (solo para clasificaci√≥n)
    """
    # Convertir a numpy array si no lo es ya
    X_new = np.asarray(X_new)

    # Asegurarse de que X_new tenga el formato correcto (2D array con un solo ejemplo)
    if X_new.ndim > 2:
        # Si tiene m√°s de 2 dimensiones, aplanamos a 2D
        X_new = X_new.reshape(1, -1)
    elif X_new.ndim == 1:
        # Si es un vector 1D, lo convertimos a 2D
        X_new = X_new.reshape(1, -1)

    # Obtener informaci√≥n del √°rbol
    feature_idx = tree_model.tree_.feature
    threshold = tree_model.tree_.threshold

    try:
        # Construir el camino de decisi√≥n
        node_indicator = tree_model.decision_path(X_new)
        leaf_id = tree_model.apply(X_new)

        # Obtener los nodos en el camino
        node_index = node_indicator.indices[node_indicator.indptr[0]:node_indicator.indptr[1]]

        path_explanation = []
        for node_id in node_index:
            # Detener si es un nodo hoja
            if leaf_id[0] == node_id:
                continue

            # Obtener la caracter√≠stica y el umbral de la decisi√≥n
            feature_id = feature_idx[node_id]
            feature_name = feature_names[feature_id]
            threshold_value = threshold[node_id]

            # Comprobar si la muestra va por la izquierda o derecha
            # Usamos X_new[0, feature_id] porque X_new es un array 2D con un solo ejemplo
            if X_new[0, feature_id] <= threshold_value:
                path_explanation.append(
                    f"- {feature_name} = {X_new[0, feature_id]:.4f} ‚â§ {threshold_value:.4f} ‚úÖ")
            else:
                path_explanation.append(
                    f"- {feature_name} = {X_new[0, feature_id]:.4f} > {threshold_value:.4f} ‚úÖ")

        # Mostrar el camino de decisi√≥n
        st.markdown("\n".join(path_explanation))

        # Mostrar el c√≥digo que genera este camino de decisi√≥n
        code_path = """
import numpy as np

def mostrar_camino_decision(tree_model, X_nuevo, feature_names, class_names=None):
    \"\"\"
    Muestra el camino de decisi√≥n para un ejemplo espec√≠fico.
    
    Parameters:
    -----------
    tree_model : DecisionTreeClassifier o DecisionTreeRegressor
        Modelo de √°rbol de decisi√≥n entrenado
    X_nuevo : array
        Ejemplo para predecir (debe ser un solo ejemplo)
    feature_names : list
        Nombres de las caracter√≠sticas
    class_names : list, optional
        Nombres de las clases (solo para clasificaci√≥n)
    \"\"\"
    # Asegurar que X_nuevo sea un array numpy 2D con una sola fila
    X_nuevo = np.asarray(X_nuevo).reshape(1, -1)
    
    # Obtener informaci√≥n del √°rbol
    feature_idx = tree_model.tree_.feature
    threshold = tree_model.tree_.threshold
    
    # Construir el camino de decisi√≥n
    node_indicator = tree_model.decision_path(X_nuevo)
    leaf_id = tree_model.apply(X_nuevo)
    
    # Obtener los nodos en el camino
    node_index = node_indicator.indices[node_indicator.indptr[0]:node_indicator.indptr[1]]
    
    # Mostrar el camino paso a paso
    print("Camino de decisi√≥n:")
    for node_id in node_index:
        # Detener si es un nodo hoja
        if leaf_id[0] == node_id:
            continue
            
        # Obtener la caracter√≠stica y el umbral de la decisi√≥n
        feature_id = feature_idx[node_id]
        feature_name = feature_names[feature_id]
        threshold_value = threshold[node_id]
        
        # Comprobar si la muestra va por la izquierda o derecha
        if X_nuevo[0, feature_id] <= threshold_value:
            print(f"- {feature_name} = {X_nuevo[0, feature_id]:.4f} ‚â§ {threshold_value:.4f}")
        else:
            print(f"- {feature_name} = {X_nuevo[0, feature_id]:.4f} > {threshold_value:.4f}")
    
    # Mostrar la predicci√≥n final
    prediccion = tree_model.predict(X_nuevo)[0]
    if hasattr(tree_model, 'classes_') and class_names:
        print(f"Predicci√≥n final: {class_names[prediccion]}")
    else:
        print(f"Predicci√≥n final: {prediccion:.4f}")

# Ejemplo de uso:
# mostrar_camino_decision(tree_model, X_nuevo, feature_names, class_names)
"""
        show_code_with_download(
            code_path, "C√≥digo para generar el camino de decisi√≥n", "camino_decision.py")
    except Exception as e:
        st.error(f"Error al mostrar el camino de decisi√≥n: {str(e)}")
        st.info(
            "Intenta reformatear los datos de entrada o verificar que el modelo sea compatible.")
