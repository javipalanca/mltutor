import streamlit as st
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.preprocessing import label_binarize
from sklearn.metrics import (
    roc_curve,
    auc,
    average_precision_score,
    precision_recall_curve,
    roc_auc_score,
    precision_score,
    recall_score,
    f1_score,
    accuracy_score,
)
from itertools import cycle


def plot_roc_curve(y_test, y_pred_proba, average="macro", class_names=None):
    """
    Plots the ROC curve and Precision-Recall curve for the given true labels and predicted probabilities.
    Supports both binary and multi-class classification with macro/micro averaging.

    Parameters:
    -----------
    y_test : array-like
        True labels
    y_pred_proba : array-like
        Predicted probabilities
    average : str, default="macro"
        Averaging method for multi-class ('macro', 'micro')
    class_names : list, optional
        Names of the classes
    """
    n_classes = len(np.unique(y_test))

    if n_classes == 2:
        # Clasificaci√≥n binaria
        _plot_binary_roc_pr(y_test, y_pred_proba)
    else:
        # Clasificaci√≥n multiclase
        _plot_multiclass_roc_pr(y_test, y_pred_proba,
                                average, class_names, n_classes)

    # A√±adir explicaci√≥n detallada al final
    _add_detailed_explanation()


def _plot_binary_roc_pr(y_test, y_pred_proba):
    """
    Plots ROC and Precision-Recall curves for binary classification.
    """
    # Crear subplots para ROC y Precision-Recall
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    # Curva ROC
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba[:, 1])
    roc_auc = auc(fpr, tpr)

    ax1.plot(fpr, tpr, color='darkorange', lw=2,
             label=f'Curva ROC (AUC = {roc_auc:.3f})')
    ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--',
             label='Clasificador Aleatorio')

    ax1.set_xlim([0.0, 1.0])
    ax1.set_ylim([0.0, 1.05])
    ax1.set_xlabel('Tasa de Falsos Positivos', fontsize=12)
    ax1.set_ylabel('Tasa de Verdaderos Positivos', fontsize=12)
    ax1.set_title('Curva ROC', fontsize=14, fontweight='bold')
    ax1.legend(loc="lower right")
    ax1.grid(True, alpha=0.3)

    # Curva Precision-Recall
    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba[:, 1])
    avg_precision = average_precision_score(y_test, y_pred_proba[:, 1])

    ax2.plot(recall, precision, color='darkgreen', lw=2,
             label=f'Curva P-R (AP = {avg_precision:.3f})')
    ax2.axhline(y=np.sum(y_test)/len(y_test), color='navy', lw=2, linestyle='--',
                label=f'Baseline ({np.sum(y_test)/len(y_test):.3f})')

    ax2.set_xlim([0.0, 1.0])
    ax2.set_ylim([0.0, 1.05])
    ax2.set_xlabel('Recall (Sensibilidad)', fontsize=12)
    ax2.set_ylabel('Precision (Precisi√≥n)', fontsize=12)
    ax2.set_title('Curva Precision-Recall', fontsize=14, fontweight='bold')
    ax2.legend(loc="lower left")
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()

    st.markdown("### üìà Curvas de Rendimiento")

    # Mostrar con 80% del ancho
    col1, col2, col3 = st.columns([0.1, 0.8, 0.1])
    with col2:
        st.pyplot(fig, use_container_width=True)

    _display_binary_interpretation(roc_auc, avg_precision, y_test)


def _plot_multiclass_roc_pr(y_test, y_pred_proba, average, class_names, n_classes):
    """
    Plots ROC and Precision-Recall curves for multi-class classification with macro/micro averaging.
    """
    # Opciones de visualizaci√≥n para multiclase
    st.markdown("### ‚öôÔ∏è Opciones de Visualizaci√≥n")

    col1, col2 = st.columns(2)
    with col1:
        show_individual = st.checkbox(
            "Mostrar curvas por clase individual", value=True)
    with col2:
        averaging_methods = st.multiselect(
            "M√©todos de promediado:",
            ["macro", "micro"],
            default=["macro", "micro"] if average in [
                "macro", "micro"] else ["macro"]
        )

    if not averaging_methods:
        averaging_methods = ["macro"]  # Default fallback

    # Preparar datos para multiclase
    if class_names is None:
        class_names = [f"Clase {i}" for i in range(n_classes)]

    # Binarizar las etiquetas para One-vs-Rest
    y_test_bin = label_binarize(y_test, classes=range(n_classes))
    if n_classes == 2:
        y_test_bin = np.hstack([1 - y_test_bin, y_test_bin])

    # Crear subplots
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))

    # Colores para las clases
    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green',
                    'purple', 'brown', 'pink', 'gray', 'olive'])

    # Diccionarios para almacenar valores de AUC y AP
    roc_auc_scores = {}
    ap_scores = {}

    # Curvas ROC por clase individual
    if show_individual:
        for i, color in zip(range(n_classes), colors):
            fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])
            roc_auc = auc(fpr, tpr)
            roc_auc_scores[f'clase_{i}'] = roc_auc

            ax1.plot(fpr, tpr, color=color, lw=2, alpha=0.7,
                     label=f'{class_names[i]} (AUC = {roc_auc:.3f})')

            # Precision-Recall por clase
            precision, recall, _ = precision_recall_curve(
                y_test_bin[:, i], y_pred_proba[:, i])
            ap = average_precision_score(y_test_bin[:, i], y_pred_proba[:, i])
            ap_scores[f'clase_{i}'] = ap

            ax2.plot(recall, precision, color=color, lw=2, alpha=0.7,
                     label=f'{class_names[i]} (AP = {ap:.3f})')

    # Curvas promedio
    for avg_method in averaging_methods:
        if avg_method == "micro":
            # Calcular curvas micro-promedio
            fpr_micro, tpr_micro, _ = roc_curve(
                y_test_bin.ravel(), y_pred_proba.ravel())
            roc_auc_micro = auc(fpr_micro, tpr_micro)
            roc_auc_scores['micro'] = roc_auc_micro

            precision_micro, recall_micro, _ = precision_recall_curve(
                y_test_bin.ravel(), y_pred_proba.ravel())
            ap_micro = average_precision_score(
                y_test_bin.ravel(), y_pred_proba.ravel())
            ap_scores['micro'] = ap_micro

            ax1.plot(fpr_micro, tpr_micro, color='deeppink', linestyle=':', linewidth=4,
                     label=f'Micro-promedio (AUC = {roc_auc_micro:.3f})')
            ax2.plot(recall_micro, precision_micro, color='deeppink', linestyle=':', linewidth=4,
                     label=f'Micro-promedio (AP = {ap_micro:.3f})')

        elif avg_method == "macro":
            # Calcular curvas macro-promedio
            all_fpr = np.unique(np.concatenate([roc_curve(y_test_bin[:, i], y_pred_proba[:, i])[0]
                                               for i in range(n_classes)]))
            mean_tpr = np.zeros_like(all_fpr)

            for i in range(n_classes):
                fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])
                mean_tpr += np.interp(all_fpr, fpr, tpr)

            mean_tpr /= n_classes
            roc_auc_macro = auc(all_fpr, mean_tpr)
            roc_auc_scores['macro'] = roc_auc_macro

            ap_macro = np.mean([average_precision_score(y_test_bin[:, i], y_pred_proba[:, i])
                               for i in range(n_classes)])
            ap_scores['macro'] = ap_macro

            ax1.plot(all_fpr, mean_tpr, color='navy', linestyle='--', linewidth=4,
                     label=f'Macro-promedio (AUC = {roc_auc_macro:.3f})')

            # Para precision-recall macro, usamos el promedio simple de AP
            ax2.axhline(y=ap_macro, color='navy', linestyle='--', linewidth=4,
                        label=f'Macro-promedio (AP = {ap_macro:.3f})')

    # L√≠nea de referencia aleatoria
    ax1.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.5,
             label='Clasificador Aleatorio')

    # Configurar axes ROC
    ax1.set_xlim([0.0, 1.0])
    ax1.set_ylim([0.0, 1.05])
    ax1.set_xlabel('Tasa de Falsos Positivos', fontsize=12)
    ax1.set_ylabel('Tasa de Verdaderos Positivos', fontsize=12)
    ax1.set_title('Curvas ROC Multi-clase', fontsize=14, fontweight='bold')
    ax1.legend(loc="lower right", fontsize=10)
    ax1.grid(True, alpha=0.3)

    # Configurar axes Precision-Recall
    ax2.set_xlim([0.0, 1.0])
    ax2.set_ylim([0.0, 1.05])
    ax2.set_xlabel('Recall (Sensibilidad)', fontsize=12)
    ax2.set_ylabel('Precision (Precisi√≥n)', fontsize=12)
    ax2.set_title('Curvas Precision-Recall Multi-clase',
                  fontsize=14, fontweight='bold')
    ax2.legend(loc="lower left", fontsize=10)
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()

    st.markdown("### üìà Curvas de Rendimiento Multi-clase")

    # Mostrar figura
    col1, col2, col3 = st.columns([0.05, 0.9, 0.05])
    with col2:
        st.pyplot(fig, use_container_width=True)

    _display_multiclass_interpretation(
        roc_auc_scores, ap_scores, averaging_methods, class_names)


def _display_binary_interpretation(roc_auc, avg_precision, y_test):
    """
    Display interpretation for binary classification metrics.
    """
    # Interpretaci√≥n de las curvas
    st.markdown("### üìã Interpretaci√≥n de las Curvas")

    col1, col2 = st.columns(2)
    with col1:
        st.markdown("**Curva ROC:**")
        if roc_auc >= 0.9:
            st.success(f"üéØ Excelente discriminaci√≥n (AUC = {roc_auc:.3f})")
        elif roc_auc >= 0.8:
            st.info(f"üëç Buena discriminaci√≥n (AUC = {roc_auc:.3f})")
        elif roc_auc >= 0.7:
            st.warning(f"‚ö†Ô∏è Discriminaci√≥n moderada (AUC = {roc_auc:.3f})")
        else:
            st.error(f"‚ùå Discriminaci√≥n pobre (AUC = {roc_auc:.3f})")

    with col2:
        st.markdown("**Curva Precision-Recall:**")
        baseline_precision = np.sum(y_test)/len(y_test)
        if avg_precision >= baseline_precision + 0.3:
            st.success(f"üéØ Excelente (AP = {avg_precision:.3f})")
        elif avg_precision >= baseline_precision + 0.1:
            st.info(
                f"üëç Buena mejora sobre baseline (AP = {avg_precision:.3f})")
        elif avg_precision >= baseline_precision:
            st.warning(f"‚ö†Ô∏è Mejora marginal (AP = {avg_precision:.3f})")
        else:
            st.error(f"‚ùå Por debajo del baseline (AP = {avg_precision:.3f})")


def _display_multiclass_interpretation(roc_auc_scores, ap_scores, averaging_methods, class_names):
    """
    Display interpretation for multi-class classification metrics.
    """
    st.markdown("### üìã Interpretaci√≥n de M√©tricas Multi-clase")

    # Mostrar m√©tricas por m√©todo de promediado
    for avg_method in averaging_methods:
        if avg_method in roc_auc_scores:
            st.markdown(f"#### {avg_method.title()}-promedio")

            col1, col2 = st.columns(2)

            with col1:
                roc_auc = roc_auc_scores[avg_method]
                st.markdown("**ROC AUC:**")
                if roc_auc >= 0.9:
                    st.success(f"üéØ Excelente (AUC = {roc_auc:.3f})")
                elif roc_auc >= 0.8:
                    st.info(f"üëç Buena (AUC = {roc_auc:.3f})")
                elif roc_auc >= 0.7:
                    st.warning(f"‚ö†Ô∏è Moderada (AUC = {roc_auc:.3f})")
                else:
                    st.error(f"‚ùå Pobre (AUC = {roc_auc:.3f})")

            with col2:
                if avg_method in ap_scores:
                    ap = ap_scores[avg_method]
                    st.markdown("**Average Precision:**")
                    if ap >= 0.8:
                        st.success(f"üéØ Excelente (AP = {ap:.3f})")
                    elif ap >= 0.6:
                        st.info(f"üëç Buena (AP = {ap:.3f})")
                    elif ap >= 0.4:
                        st.warning(f"‚ö†Ô∏è Moderada (AP = {ap:.3f})")
                    else:
                        st.error(f"‚ùå Pobre (AP = {ap:.3f})")

    # Tabla de m√©tricas por clase
    if any(key.startswith('clase_') for key in roc_auc_scores.keys()):
        st.markdown("#### üìä M√©tricas por Clase")

        metrics_data = []
        for i, class_name in enumerate(class_names):
            roc_key = f'clase_{i}'
            if roc_key in roc_auc_scores:
                metrics_data.append({
                    'Clase': class_name,
                    'ROC AUC': round(roc_auc_scores[roc_key], 3),
                    'Average Precision': round(ap_scores[roc_key], 3)
                })

        if metrics_data:
            df_metrics = pd.DataFrame(metrics_data)
            st.dataframe(df_metrics, use_container_width=True)

    # Explicaci√≥n de macro vs micro
    with st.expander("‚ÑπÔ∏è ¬øMacro vs Micro promedio?", expanded=False):
        st.markdown("""
        **Macro-promedio:**
        - Calcula la m√©trica para cada clase independientemente
        - Luego promedia los resultados (todas las clases tienen el mismo peso)
        - √ötil cuando todas las clases son igualmente importantes
        - M√°s sensible al rendimiento en clases minoritarias

        **Micro-promedio:**
        - Agrega las contribuciones de todas las clases para calcular la m√©trica
        - Da m√°s peso a las clases con m√°s muestras
        - √ötil cuando las clases m√°s frecuentes son m√°s importantes
        - Menos sensible a clases desbalanceadas

        **¬øCu√°l usar?**
        - **Macro** cuando todas las clases son igualmente importantes
        - **Micro** cuando algunas clases son m√°s importantes que otras
        - **Ambos** para una evaluaci√≥n completa
        """)


def _add_detailed_explanation():
    """
    Add detailed explanation about ROC and Precision-Recall curves.
    """
    # Explicaci√≥n detallada sobre las curvas de rendimiento
    with st.expander("‚ÑπÔ∏è ¬øC√≥mo interpretar las Curvas de Rendimiento?", expanded=False):
        st.markdown("""
        **Curva ROC (Receiver Operating Characteristic)**

        **¬øQu√© muestra?**
        - **Eje X:** Tasa de Falsos Positivos (FPR) = FP / (FP + TN)
        - **Eje Y:** Tasa de Verdaderos Positivos (TPR) = TP / (TP + FN) = Sensibilidad/Recall
        - **L√≠nea diagonal:** Rendimiento de un clasificador aleatorio
        - **AUC (√Årea Bajo la Curva):** M√©trica resumen del rendimiento

        **Interpretaci√≥n:**
        - **AUC = 1.0:** Clasificador perfecto
        - **AUC = 0.9-1.0:** Excelente discriminaci√≥n
        - **AUC = 0.8-0.9:** Buena discriminaci√≥n
        - **AUC = 0.7-0.8:** Discriminaci√≥n aceptable
        - **AUC = 0.5:** Equivalente a adivinar al azar
        - **AUC < 0.5:** Peor que adivinar (pero se puede invertir)

        **¬øCu√°ndo usar ROC?**
        - Cuando las clases est√°n relativamente balanceadas
        - Para comparar modelos r√°pidamente
        - Cuando te importa el rendimiento general

        ---

        **Curva Precision-Recall (P-R)**

        **¬øQu√© muestra?**
        - **Eje X:** Recall (Sensibilidad) = TP / (TP + FN)
        - **Eje Y:** Precision (Precisi√≥n) = TP / (TP + FP)
        - **L√≠nea horizontal:** Baseline (proporci√≥n de casos positivos)
        - **AP (Average Precision):** M√©trica resumen del rendimiento

        **Interpretaci√≥n:**
        - **AP alto:** Buen balance entre precisi√≥n y recall
        - **Curva cerca del √°ngulo superior derecho:** Excelente rendimiento
        - **Por encima del baseline:** Mejor que una predicci√≥n aleatoria

        **¬øCu√°ndo usar P-R?**
        - ‚úÖ **Clases desbalanceadas** (muchos m√°s negativos que positivos)
        - ‚úÖ Cuando los **falsos positivos son costosos**
        - ‚úÖ Para datasets con **pocos casos positivos**
        - ‚úÖ En problemas como **detecci√≥n de fraude, diagn√≥stico m√©dico**

        **Comparaci√≥n ROC vs P-R:**
        - **ROC** es m√°s optimista con clases desbalanceadas
        - **P-R** es m√°s conservadora y realista
        - **P-R** se enfoca m√°s en el rendimiento de la clase minoritaria
        - Usar **ambas** para una evaluaci√≥n completa

        ---

        **Multi-clase: Macro vs Micro**

        **Estrategia One-vs-Rest:**
        - Cada clase se trata como positiva vs todas las dem√°s como negativas
        - Se calculan m√©tricas para cada clase por separado
        - Luego se combinan usando macro o micro promedio

        **Macro-promedio:**
        - Promedio simple de las m√©tricas de cada clase
        - Todas las clases tienen el mismo peso
        - M√°s sensible a clases minoritarias
        - √ötil cuando todas las clases son igualmente importantes

        **Micro-promedio:**
        - Agregaci√≥n global de TP, FP, FN de todas las clases
        - Las clases con m√°s muestras tienen m√°s peso
        - Menos sensible a desbalance de clases
        - √ötil cuando algunas clases son m√°s importantes
        """)


def plot_threshold_analysis(y_true, y_pred_proba, class_names):
    # Verificar que tenemos datos v√°lidos
    if y_pred_proba is not None and len(y_pred_proba) > 0:
        unique_classes = np.unique(y_true)

        # Para clasificaci√≥n binaria
        if len(unique_classes) == 2:
            # Explicaci√≥n detallada sobre distribuci√≥n de probabilidades
            with st.expander("‚ÑπÔ∏è ¬øC√≥mo interpretar la Distribuci√≥n de Probabilidades?", expanded=False):
                st.markdown("""
                    **¬øQu√© muestra este gr√°fico?**
                    
                    Este histograma muestra c√≥mo el modelo asigna probabilidades a cada muestra del conjunto de prueba, 
                    separado por la clase real a la que pertenece cada muestra.
                    
                    **Elementos del gr√°fico:**
                    - **Histograma azul:** Distribuci√≥n de probabilidades para muestras que realmente pertenecen a la clase positiva
                    - **Histograma rojo:** Distribuci√≥n de probabilidades para muestras que realmente pertenecen a la clase negativa  
                    - **L√≠nea roja vertical:** Umbral de decisi√≥n (0.5 por defecto)
                    - **Eje X:** Probabilidad asignada por el modelo (0 = clase negativa, 1 = clase positiva)
                    - **Eje Y:** Cantidad de muestras
                    
                    **Interpretaci√≥n ideal:**
                    - ‚úÖ **Buena separaci√≥n:** Los histogramas no se superponen mucho
                    - ‚úÖ **Clase negativa:** Concentrada cerca de 0 (izquierda)
                    - ‚úÖ **Clase positiva:** Concentrada cerca de 1 (derecha)
                    - ‚úÖ **Pocas muestras cerca del umbral (0.5):** Indica confianza en las predicciones
                    
                    **Problemas a identificar:**
                    - ‚ö†Ô∏è **Mucha superposici√≥n:** Indica dificultad para separar las clases
                    - ‚ö†Ô∏è **Concentraci√≥n en el centro (0.3-0.7):** El modelo est√° inseguro
                    - ‚ö†Ô∏è **Distribuci√≥n uniforme:** El modelo no est√° aprendiendo patrones √∫tiles
                    
                    **Aplicaciones pr√°cticas:**
                    - Identificar si el modelo est√° confiado en sus predicciones
                    - Evaluar si cambiar el umbral de decisi√≥n podr√≠a mejorar el rendimiento
                    - Detectar casos donde el modelo necesita m√°s datos o caracter√≠sticas
                    """)
            fig, ax = plt.subplots(figsize=(12, 6))

            # Obtener probabilidades de la clase positiva
            prob_class_1 = y_pred_proba[:, 1]

            # Separar por clase real - usar los valores √∫nicos reales
            mask_class_0 = (y_true == unique_classes[0])
            mask_class_1 = (y_true == unique_classes[1])

            prob_class_0_real = prob_class_1[mask_class_0]
            prob_class_1_real = prob_class_1[mask_class_1]

            # Crear histogramas solo si hay datos
            if len(prob_class_0_real) > 0:
                ax.hist(prob_class_0_real, bins=20, alpha=0.7,
                        label=f'Clase {class_names[0] if class_names and len(class_names) > 0 else unique_classes[0]} (Real)',
                        color='lightcoral', edgecolor='black')

            if len(prob_class_1_real) > 0:
                ax.hist(prob_class_1_real, bins=20, alpha=0.7,
                        label=f'Clase {class_names[1] if class_names and len(class_names) > 1 else unique_classes[1]} (Real)',
                        color='lightblue', edgecolor='black')

            # L√≠nea del umbral de decisi√≥n
            ax.axvline(x=0.5, color='red', linestyle='--',
                       linewidth=2, label='Umbral de decisi√≥n (0.5)')

            # Configurar el gr√°fico
            ax.set_xlabel(
                'Probabilidad de Clase Positiva', fontsize=12)
            ax.set_ylabel('Frecuencia', fontsize=12)
            ax.set_title('Distribuci√≥n de Probabilidades Predichas por Clase Real',
                         fontsize=14, fontweight='bold')
            ax.legend()
            ax.grid(True, alpha=0.3)

            # Asegurar l√≠mites apropiados
            ax.set_xlim(0, 1)

            plt.tight_layout()

            # Mostrar el gr√°fico
            col1, col2, col3 = st.columns([0.1, 0.8, 0.1])
            with col2:
                st.pyplot(fig, use_container_width=True)

            # Limpiar la figura
            plt.close(fig)

            # An√°lisis de separaci√≥n
            if len(prob_class_0_real) > 0 and len(prob_class_1_real) > 0:
                # Contar solapamiento en la zona de incertidumbre (0.3-0.7)
                overlap_0 = np.sum(
                    (prob_class_0_real > 0.3) & (prob_class_0_real < 0.7))
                overlap_1 = np.sum(
                    (prob_class_1_real > 0.3) & (prob_class_1_real < 0.7))
                total_overlap = overlap_0 + overlap_1

                overlap_percentage = total_overlap / len(y_true)

                # M√©tricas adicionales de separaci√≥n
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Muestras en zona incierta",
                              f"{total_overlap}/{len(y_true)}")
                with col2:
                    st.metric("Porcentaje de incertidumbre",
                              f"{overlap_percentage:.1%}")
                with col3:
                    conf_threshold = 0.8  # 80% de confianza
                    high_conf = np.sum((prob_class_1 < 0.2) | (
                        prob_class_1 > conf_threshold))
                    st.metric("Predicciones confiables",
                              f"{high_conf}/{len(y_true)}")

                # Interpretaci√≥n
                if overlap_percentage < 0.2:
                    st.success(
                        "‚úÖ Excelente separaci√≥n entre clases - El modelo est√° muy confiado en sus predicciones")
                elif overlap_percentage < 0.4:
                    st.info("üëç Buena separaci√≥n entre clases")
                else:
                    st.warning(
                        "‚ö†Ô∏è Las clases se superponen significativamente - Considera ajustar el umbral de decisi√≥n")

        elif len(unique_classes) > 2:
            # Para clasificaci√≥n multiclase
            st.info(
                "**Nota:** Clasificaci√≥n multiclase detectada. Mostrando distribuci√≥n de probabilidades para cada clase.")
            # Se crea un histograma diferente para cada clase en figuras separadas.
            with st.expander("‚ÑπÔ∏è ¬øC√≥mo interpretar la Distribuci√≥n de Probabilidades Multiclase?", expanded=False):
                st.markdown("""
                La distribuci√≥n de probabilidades para cada clase muestra la confianza del modelo al predecir la clase a la que pertenece una muestra. 
                En un escenario de clasificaci√≥n multiclase, el modelo asigna una probabilidad a cada clase posible, y la clase predicha suele ser la de mayor probabilidad.

                **Interpretaci√≥n:**
                - Cada barra en cada histograma representa la frecuencia de las probabilidades predichas para una clase espec√≠fica.
                - Un modelo bien calibrado tendr√° una alta concentraci√≥n de muestras en los extremos (cerca de 0 o 1) para la clase correcta.
                - Distribuciones m√°s planas o con picos intermedios sugieren que el modelo presenta incertidumbre o confusi√≥n entre clases.

                **Consejos:**
                - Presta atenci√≥n a las clases con solapamiento en sus distribuciones, ya que esto puede indicar √°reas de incertidumbre.
                - Considera ajustar el umbral de decisi√≥n para mejorar la precisi√≥n en clases espec√≠ficas.
                """)

            n_classes = len(unique_classes)
            n_cols = min(3, n_classes)
            n_rows = (n_classes + n_cols - 1) // n_cols

            fig, axes = plt.subplots(
                n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))

            # Manejar caso de una sola clase
            if n_classes == 1:
                axes = [axes]
            elif n_rows == 1:
                axes = axes if n_cols > 1 else [axes]
            else:
                axes = axes.flatten()

            for i, class_val in enumerate(unique_classes):
                if i < len(axes):
                    ax_sub = axes[i]

                    # Probabilidades para esta clase
                    class_probs = y_pred_proba[:, i]

                    ax_sub.hist(class_probs, bins=20, alpha=0.7,
                                color=plt.cm.Set3(i), edgecolor='black')

                    class_label = class_names[i] if class_names and i < len(
                        class_names) else f"Clase {class_val}"
                    ax_sub.set_title(
                        f'Probabilidades para {class_label}')
                    ax_sub.set_xlabel('Probabilidad')
                    ax_sub.set_ylabel('Frecuencia')
                    ax_sub.grid(True, alpha=0.3)
                    ax_sub.set_xlim(0, 1)

            # Ocultar subplots vac√≠os
            for i in range(n_classes, len(axes)):
                axes[i].set_visible(False)

            plt.tight_layout()

            col1, col2, col3 = st.columns([0.1, 0.8, 0.1])
            with col2:
                st.pyplot(fig, use_container_width=True)

            plt.close(fig)
        else:
            st.error(
                "Error: Datos de clasificaci√≥n insuficientes para crear la visualizaci√≥n")

    else:
        st.error(
            "Error: No hay probabilidades predichas disponibles. Aseg√∫rate de que el modelo est√© entrenado correctamente.")

    # An√°lisis de umbrales de decisi√≥n para clasificaci√≥n binaria
    if len(np.unique(y_true)) == 2:
        st.markdown("### üéØ An√°lisis de Umbrales de Decisi√≥n")

        # Explicaci√≥n detallada sobre umbrales de decisi√≥n
        with st.expander("‚ÑπÔ∏è ¬øC√≥mo interpretar el An√°lisis de Umbrales?", expanded=False):
            st.markdown("""
            **¬øQu√© es el umbral de decisi√≥n?**
            
            El umbral de decisi√≥n es el valor que determina cu√°ndo el modelo clasifica una muestra como 
            positiva o negativa. Por defecto, este umbral es **0.5**:
            - **Probabilidad ‚â• 0.5** ‚Üí Clase Positiva
            - **Probabilidad < 0.5** ‚Üí Clase Negativa
            
            **¬øPor qu√© cambiar el umbral?**
            
            El umbral por defecto (0.5) no siempre es √≥ptimo. Dependiendo del problema, 
            puede ser beneficioso ajustarlo:
            
            **üìà Umbral m√°s alto (0.6, 0.7, 0.8):**
            - ‚úÖ **Mayor Precisi√≥n:** Menos falsos positivos
            - ‚úÖ **Predicciones m√°s conservadoras:** Solo clasifica como positivo cuando est√° muy seguro
            - ‚ö†Ô∏è **Menor Recall:** Puede perder casos positivos reales
            - **√ötil cuando:** Los falsos positivos son muy costosos (ej: diagn√≥stico m√©dico, inversiones)
            
            **üìâ Umbral m√°s bajo (0.3, 0.4):**
            - ‚úÖ **Mayor Recall:** Detecta m√°s casos positivos reales
            - ‚úÖ **Predicciones m√°s sensibles:** No se pierde tantos casos positivos
            - ‚ö†Ô∏è **Menor Precisi√≥n:** M√°s falsos positivos
            - **√ötil cuando:** Los falsos negativos son muy costosos (ej: detecci√≥n de fraude, seguridad)
            
            **M√©tricas mostradas:**
            - **Accuracy:** Porcentaje total de predicciones correctas
            - **Precision:** De las predicciones positivas, cu√°ntas son correctas
            - **Recall:** De los casos positivos reales, cu√°ntos detectamos
            - **F1-Score:** Balance entre precisi√≥n y recall
            
            **¬øC√≥mo elegir el umbral √≥ptimo?**
            1. **Maximizar F1-Score:** Balance general entre precisi√≥n y recall
            2. **Maximizar Precision:** Si los falsos positivos son costosos
            3. **Maximizar Recall:** Si los falsos negativos son costosos
            4. **Considerar el contexto:** Costos reales de errores en tu dominio
            
            **Ejemplo pr√°ctico:**
            - **Email spam:** Prefiere falsos positivos (email importante en spam) que falsos negativos
            - **Diagn√≥stico m√©dico:** Prefiere falsos positivos (m√°s pruebas) que falsos negativos (enfermedad no detectada)
            - **Recomendaciones:** Balance entre no molestar (precisi√≥n) y no perder oportunidades (recall)
            """)

        # Calcular m√©tricas para diferentes umbrales
        thresholds = np.arange(0.1, 1.0, 0.1)
        threshold_metrics = []

        for threshold in thresholds:
            y_pred_thresh = (
                y_pred_proba[:, 1] >= threshold).astype(int)

            if len(np.unique(y_pred_thresh)) > 1:  # Evitar divisi√≥n por cero
                precision = precision_score(
                    y_true, y_pred_thresh, zero_division=0)
                recall = recall_score(
                    y_true, y_pred_thresh, zero_division=0)
                f1 = f1_score(y_true, y_pred_thresh,
                              zero_division=0)
                accuracy = accuracy_score(y_true, y_pred_thresh)

                threshold_metrics.append({
                    'Umbral': threshold,
                    'Accuracy': accuracy,
                    'Precision': precision,
                    'Recall': recall,
                    'F1-Score': f1
                })

        if threshold_metrics:
            df_thresholds = pd.DataFrame(threshold_metrics)

            # Encontrar el mejor umbral por F1-Score
            best_f1_idx = df_thresholds['F1-Score'].idxmax()
            best_threshold = df_thresholds.loc[best_f1_idx, 'Umbral']

            col1, col2 = st.columns(2)

            with col1:
                st.metric("Umbral Actual", "0.50")
                st.metric("Umbral √ìptimo (F1)",
                          f"{best_threshold:.2f}")

                if abs(best_threshold - 0.5) > 0.1:
                    st.info(
                        f"üí° Considera ajustar el umbral a {best_threshold:.2f} para mejorar el F1-Score")

            with col2:
                # Mostrar tabla de umbrales (seleccionados)
                display_thresholds = df_thresholds[df_thresholds['Umbral'].isin(
                    [0.3, 0.5, 0.7])].copy()
                for col in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:
                    display_thresholds[col] = display_thresholds[col].apply(
                        lambda x: f"{x:.3f}")

                st.markdown("**Comparaci√≥n de Umbrales:**")
                st.dataframe(
                    display_thresholds, hide_index=True, use_container_width=True)
