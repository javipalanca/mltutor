import pandas as pd
import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.inspection import DecisionBoundaryDisplay

from utils import get_image_download_link, show_code_with_download
from algorithms.code_examples import generate_decision_boundary_code


def plot_decision_boundary(model_2d, X, y, feature_names, class_names):
    st.markdown("### Selecci√≥n de Caracter√≠sticas")
    st.markdown("Selecciona 2 caracter√≠sticas para visualizar:")

    col1, col2 = st.columns(2)

    with col1:
        feature1 = st.selectbox(
            "Primera caracter√≠stica:",
            feature_names,
            index=0,
            key="viz_feature1"
        )

    with col2:
        feature2 = st.selectbox(
            "Segunda caracter√≠stica:",
            feature_names,
            index=min(1, len(feature_names) - 1),
            key="viz_feature2"
        )

    if feature1 != feature2:
        # Obtener √≠ndices de las caracter√≠sticas seleccionadas
        feature_idx = [feature_names.index(
            feature1), feature_names.index(feature2)]

        with st.spinner("Generando frontera de decisi√≥n..."):

            try:
                # Extraer las caracter√≠sticas seleccionadas
                if hasattr(X, 'iloc'):  # DataFrame
                    X_2d = X.iloc[:, feature_idx].values
                else:  # numpy array
                    X_2d = X[:, feature_idx]

                # Entrenar un modelo con solo estas 2 caracter√≠sticas
                model_2d.fit(X_2d, y)

                # Crear la visualizaci√≥n
                fig, ax = plt.subplots(figsize=(10, 8))

                # Crear malla de puntos con resoluci√≥n adaptativa
                # Reducir resoluci√≥n para KNN para evitar problemas de memoria
                n_samples = len(X_2d)
                if hasattr(model_2d, '_estimator_type') and 'KNeighbors' in str(type(model_2d)):
                    # Para KNN, usar resoluci√≥n m√°s baja para evitar OOM
                    if n_samples > 1000:
                        h = 0.1  # Resoluci√≥n muy baja para datasets grandes
                    elif n_samples > 500:
                        h = 0.05  # Resoluci√≥n baja para datasets medianos
                    else:
                        h = 0.02  # Resoluci√≥n normal para datasets peque√±os
                else:
                    h = 0.02  # Resoluci√≥n normal para otros algoritmos

                x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1
                y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1

                # Limitar el rango para evitar mallas gigantes
                x_range = x_max - x_min
                y_range = y_max - y_min
                max_range = max(x_range, y_range)

                # Si el rango es muy grande, limitarlo
                if max_range > 100:
                    # M√°ximo 200 puntos por dimensi√≥n
                    h = max(h, max_range / 200)

                xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                                     np.arange(y_min, y_max, h))

                # Verificar el tama√±o de la malla antes de continuar
                mesh_size = xx.shape[0] * xx.shape[1]
                if mesh_size > 50000:  # L√≠mite de seguridad
                    st.warning(
                        f"‚ö†Ô∏è La malla es muy grande ({mesh_size:,} puntos). Reduciendo resoluci√≥n...")
                    h = h * 2  # Duplicar el paso para reducir puntos
                    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                                         np.arange(y_min, y_max, h))
                    mesh_size = xx.shape[0] * xx.shape[1]
                    st.info(f"Nueva resoluci√≥n: {mesh_size:,} puntos")

                # Predecir en la malla con manejo de memoria
                try:
                    with st.spinner(f"Calculando predicciones en {mesh_size:,} puntos..."):
                        Z = model_2d.predict(np.c_[xx.ravel(), yy.ravel()])
                        Z = Z.reshape(xx.shape)
                except MemoryError:
                    st.error(
                        "‚ùå Error de memoria. El dataset es demasiado grande para esta resoluci√≥n.")
                    st.info(
                        "üí° Sugerencias: Prueba con un dataset m√°s peque√±o o usa un algoritmo menos intensivo como Decision Trees.")
                    return

                # Frontera de decisi√≥n para clasificaci√≥n
                #########################################
                n_classes = len(np.unique(y))

                # Usar diferentes mapas de colores seg√∫n el n√∫mero de clases
                if n_classes == 2:
                    contour = ax.contourf(
                        xx, yy, Z, alpha=0.3, cmap='RdBu', levels=50)
                    scatter_cmap = 'RdBu'
                else:
                    contour = ax.contourf(
                        xx, yy, Z, alpha=0.3, cmap='Set3', levels=n_classes)
                    scatter_cmap = 'Set3'

                # Scatter plot de los datos
                scatter = ax.scatter(X_2d[:, 0], X_2d[:, 1], c=y,
                                     cmap=scatter_cmap, edgecolor='black', s=50, alpha=0.8)

                # Leyenda para clasificaci√≥n
                if class_names and n_classes <= 10:
                    import matplotlib.patches as mpatches
                    if n_classes == 2:
                        colors = ['#d7191c', '#2c7bb6']  # RdBu colors
                    else:
                        colors = plt.cm.Set3(
                            np.linspace(0, 1, n_classes))

                    patches = [mpatches.Patch(color=colors[i],
                                              label=class_names[i])
                               for i in range(min(len(class_names), n_classes))]
                    ax.legend(handles=patches,
                              loc='best', title='Clases')

                ax.set_title(
                    f'Frontera de Decisi√≥n para {feature1} vs {feature2}')

                ax.set_xlabel(feature1)
                ax.set_ylabel(feature2)
                ax.grid(True, alpha=0.3)

                st.pyplot(fig)

                # Enlace para descargar
                st.markdown(
                    get_image_download_link(
                        fig, "frontera_decision", "üì• Descargar visualizaci√≥n de frontera"),
                    unsafe_allow_html=True
                )

                # Advertencia sobre dimensionalidad
                if len(feature_names) > 2:
                    st.warning("""
                    ‚ö†Ô∏è Esta visualizaci√≥n solo muestra 2 caracter√≠sticas seleccionadas. Se entrena un nuevo modelo 
                    simplificado solo con estas 2 caracter√≠sticas para poder visualizar la frontera de decisi√≥n. 
                    El modelo real utiliza todas las caracter√≠sticas y puede tener diferentes decisiones.
                    """)

                # Mostrar c√≥digo para generar esta visualizaci√≥n
                code_boundary = generate_decision_boundary_code(
                    feature_names, class_names)

                show_code_with_download(
                    code_boundary, "C√≥digo para generar la frontera de decisi√≥n", "frontera_decision.py"
                )
            except Exception as e:
                st.error(
                    f"Error al crear la visualizaci√≥n de frontera de decisi√≥n: {str(e)}"
                )


def plot_decision_surface(model_2d, feature_names, X, y):
    st.markdown("### Selecci√≥n de Caracter√≠sticas")
    st.markdown("Selecciona 2 caracter√≠sticas para visualizar:")

    col1, col2 = st.columns(2)

    with col1:
        feature1 = st.selectbox(
            "Primera caracter√≠stica:",
            feature_names,
            index=0,
            key="viz_feature1"
        )

    with col2:
        feature2 = st.selectbox(
            "Segunda caracter√≠stica:",
            feature_names,
            index=min(1, len(feature_names) - 1),
            key="viz_feature2"
        )

    if feature1 != feature2:
        with st.spinner("Generando superficie de decisi√≥n..."):
            # Obtener √≠ndices de las caracter√≠sticas seleccionadas
            feature_idx = [feature_names.index(
                feature1), feature_names.index(feature2)]

            # Extraer las caracter√≠sticas seleccionadas
            if hasattr(X, 'iloc'):  # DataFrame
                X_2d = X.iloc[:, feature_idx].values
            else:  # numpy array
                X_2d = X[:, feature_idx]

            # Entrenar un modelo con solo estas 2 caracter√≠sticas
            model_2d.fit(X_2d, y)

            # Crear la visualizaci√≥n
            fig, ax = plt.subplots(figsize=(10, 8))

            # Crear malla de puntos con resoluci√≥n adaptativa
            # Reducir resoluci√≥n para KNN para evitar problemas de memoria
            n_samples = len(X_2d)
            if hasattr(model_2d, '_estimator_type') and 'KNeighbors' in str(type(model_2d)):
                # Para KNN, usar resoluci√≥n m√°s baja para evitar OOM
                if n_samples > 1000:
                    h = 0.1  # Resoluci√≥n muy baja para datasets grandes
                elif n_samples > 500:
                    h = 0.05  # Resoluci√≥n baja para datasets medianos
                else:
                    h = 0.02  # Resoluci√≥n normal para datasets peque√±os
            else:
                h = 0.02  # Resoluci√≥n normal para otros algoritmos

            x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1
            y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1

            # Limitar el rango para evitar mallas gigantes
            x_range = x_max - x_min
            y_range = y_max - y_min
            max_range = max(x_range, y_range)

            # Si el rango es muy grande, limitarlo
            if max_range > 100:
                h = max(h, max_range / 200)  # M√°ximo 200 puntos por dimensi√≥n

            xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                                 np.arange(y_min, y_max, h))

            # Verificar el tama√±o de la malla antes de continuar
            mesh_size = xx.shape[0] * xx.shape[1]
            if mesh_size > 50000:  # L√≠mite de seguridad
                st.warning(
                    f"‚ö†Ô∏è La malla es muy grande ({mesh_size:,} puntos). Reduciendo resoluci√≥n...")
                h = h * 2  # Duplicar el paso para reducir puntos
                xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                                     np.arange(y_min, y_max, h))
                mesh_size = xx.shape[0] * xx.shape[1]
                st.info(f"Nueva resoluci√≥n: {mesh_size:,} puntos")

            # Predecir en la malla con manejo de memoria
            try:
                with st.spinner(f"Calculando predicciones en {mesh_size:,} puntos..."):
                    Z = model_2d.predict(np.c_[xx.ravel(), yy.ravel()])
                    Z = Z.reshape(xx.shape)
            except MemoryError:
                st.error(
                    "‚ùå Error de memoria. El dataset es demasiado grande para KNN con esta resoluci√≥n.")
                st.info(
                    "üí° Sugerencias: Prueba con un dataset m√°s peque√±o o usa un algoritmo menos intensivo como Decision Trees.")
                return

            # Superficie de predicci√≥n para regresi√≥n
            #########################################
            contour = ax.contourf(
                xx, yy, Z, alpha=0.6, cmap='viridis', levels=20)
            scatter = ax.scatter(X_2d[:, 0], X_2d[:, 1], c=y,
                                 cmap='viridis', edgecolor='black', s=50, alpha=0.8)

            # Barra de colores
            cbar = plt.colorbar(scatter, ax=ax, shrink=0.8)
            cbar.set_label('Valor Objetivo')

            ax.set_title(
                f'Superficie de Predicci√≥n para {feature1} vs {feature2}')

            ax.set_xlabel(feature1)
            ax.set_ylabel(feature2)
            ax.grid(True, alpha=0.3)

            st.pyplot(fig)
