"""
Este mÃ³dulo contiene funciones para evaluar modelos de Ã¡rboles de decisiÃ³n.
Incluye funciones para calcular mÃ©tricas y visualizar resultados de clasificaciÃ³n y regresiÃ³n.
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, mean_squared_error, confusion_matrix,
    classification_report, precision_recall_fscore_support,
    r2_score, mean_absolute_error

)

from utils import get_image_download_link, show_code_with_download
from algorithms.code_examples import (
    CONFUSION_MATRIX_CODE,
    PRECISION_CODE,
    PRED_VS_REAL_CODE,
    ERROR_DISTRIBUTION_CODE,
    DECISION_PATH_CODE,
)


def evaluate_classification_model(y_test, y_pred, class_names):
    """
    EvalÃºa un modelo de clasificaciÃ³n y devuelve las mÃ©tricas principales.

    Parameters:
    -----------
    y_test : array
        Valores reales
    y_pred : array
        Valores predichos
    class_names : list
        Nombres de las clases

    Returns:
    --------
    dict
        Diccionario con las mÃ©tricas del modelo
    """
    # Calcular mÃ©tricas
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(
        y_test, y_pred, target_names=class_names, output_dict=True)
    cm = confusion_matrix(y_test, y_pred)

    # Extraer precision y recall promedio ponderado para compatibilidad
    precision = report.get('weighted avg', {}).get('precision', 0.0)
    recall = report.get('weighted avg', {}).get('recall', 0.0)

    return {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "report": report,
        "confusion_matrix": cm,
        "y_pred": y_pred
    }


def evaluate_regression_model(y_test, y_pred):
    """
    EvalÃºa un modelo de regresiÃ³n y devuelve las mÃ©tricas principales.

    Parameters:
    -----------
    y_test : array
        Valores reales
    y_pred : array
        Valores predichos

    Returns:
    --------
    dict
        Diccionario con las mÃ©tricas del modelo
    """
    # Calcular mÃ©tricas
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    # np.mean(np.abs(y_test - y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)  # 1 - np.sum((y_test - y_pred)**2) / \
    # np.sum((y_test - np.mean(y_test))**2)

    return {
        "mse": mse,
        "rmse": rmse,
        "mae": mae,
        "r2": r2,
        "y_pred": y_pred
    }


def show_detailed_evaluation(y_test, y_pred, class_names, model_type):
    """
    Muestra una evaluaciÃ³n detallada del modelo con grÃ¡ficos y explicaciones.

    Parameters:
    -----------
    y_test : array
        Valores reales
    y_pred : array
        Valores predichos
    class_names : list
        Nombres de las clases
    model_type : str
        Tipo de modelo ('ClasificaciÃ³n' o 'RegresiÃ³n')
    """
    if model_type == "ClasificaciÃ³n":
        # Preparar etiquetas y nombres de clase de forma robusta
        if class_names is None:
            labels = sorted(np.unique(y_test))
            target_names = [str(l) for l in labels]
        else:
            # Si class_names estÃ¡ presente, asumir que las clases son 0..n-1
            try:
                labels = list(range(len(class_names)))
                target_names = [str(n) for n in class_names]
            except Exception:
                labels = sorted(np.unique(y_test))
                target_names = [str(l) for l in labels]

        # Calcular mÃ©tricas usando labels y target_names
        report = classification_report(
            y_test, y_pred, labels=labels, target_names=target_names, output_dict=True)
        report_df = pd.DataFrame(report).transpose()

        # MÃ©tricas globales
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            acc_value = report.get('accuracy', 0.0)
            st.metric("Exactitud (Accuracy)", f"{acc_value:.4f}",
                      help="ProporciÃ³n total de predicciones correctas")
            # Indicador de calidad
            if acc_value >= 0.9:
                st.success("Excelente", icon="ğŸŒŸ")
            elif acc_value >= 0.8:
                st.info("Muy bueno", icon="ğŸ‘")
            elif acc_value >= 0.7:
                st.warning("Bueno", icon="âš ï¸")
            else:
                st.warning("Necesita mejora", icon="âŒ")
        with col2:
            st.metric("PrecisiÃ³n media", f"{report.get('weighted avg', {}).get('precision', 0.0):.4f}",
                      help="Media ponderada de la precisiÃ³n de cada clase")
        with col3:
            st.metric("Exhaustividad media (Recall)", f"{report.get('weighted avg', {}).get('recall', 0.0):.4f}",
                      help="Media ponderada de la exhaustividad de cada clase")
        with col4:
            st.metric("F1-Score medio", f"{report.get('weighted avg', {}).get('f1-score', 0.0):.4f}",
                      help="Media armÃ³nica de precisiÃ³n y exhaustividad")
            f1_score = report.get('weighted avg', {}).get('f1-score', 0.0)
            if f1_score >= 0.8:
                st.success("ğŸŒŸ Excelente balance")
            elif f1_score >= 0.7:
                st.info("ğŸ‘ Buen balance")
            else:
                st.warning("âš ï¸ Balance mejorable")

        # MÃ©tricas por clase
        st.markdown("### âš–ï¸ Matriz de ConfusiÃ³n")

        # Excluir filas avg y accuracy del dataframe
        report_by_class = report_df.drop(
            ['accuracy', 'macro avg', 'weighted avg'], errors='ignore')

        # Matriz de confusiÃ³n
        cm = confusion_matrix(y_test, y_pred, labels=labels)
        fig_cm, ax_cm = plt.subplots(figsize=(6, 5))  # Reducir tamaÃ±o
        xticks = target_names if target_names is not None else None
        yticks = target_names if target_names is not None else None
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax_cm,
                    xticklabels=xticks, yticklabels=yticks)
        ax_cm.set_xlabel('PredicciÃ³n')
        ax_cm.set_ylabel('Real')
        ax_cm.set_title('Matriz de ConfusiÃ³n')

        col1, col2 = st.columns([1, 1])
        with col1:
            # Mostrar con tamaÃ±o reducido
            col_inner1, col_inner2, col_inner3 = st.columns([1, 3, 1])
            with col_inner2:
                st.pyplot(fig_cm, use_container_width=True)

            st.markdown(get_image_download_link(
                fig_cm, "matriz_confusion", "ğŸ“¥ Descargar matriz de confusiÃ³n"), unsafe_allow_html=True)

            # Mostrar cÃ³digo para generar la matriz de confusiÃ³n
            code_cm = CONFUSION_MATRIX_CODE
            show_code_with_download(
                code_cm, "CÃ³digo para generar la matriz de confusiÃ³n", "matriz_confusion.py")

        with col2:
            st.dataframe(report_by_class.style.format({
                'precision': '{:.4f}',
                'recall': '{:.4f}',
                'f1-score': '{:.4f}',
                'support': '{:.0f}'
            }))

            with st.expander("ğŸ“Š ExplicaciÃ³n de mÃ©tricas"):
                st.markdown("""
                 **Accuracy (Exactitud):**
                    - Porcentaje de predicciones correctas del total
                    - Rango: 0 a 1 (0% a 100%)
                    - **InterpretaciÃ³n:** Valores mÃ¡s altos = mejor modelo
                    - **Cuidado:** Puede ser engaÃ±osa con clases desbalanceadas
                    
                    **Precision (PrecisiÃ³n):**
                    - De todas las predicciones positivas, cuÃ¡ntas fueron correctas
                    - FÃ³rmula: VP / (VP + FP)
                    - Importante cuando los falsos positivos son costosos
                    - **InterpretaciÃ³n:** Valores mÃ¡s altos = mejor modelo
                    
                    **Recall (Sensibilidad o Exhaustividad):**
                    - De todos los casos positivos reales, cuÃ¡ntos detectÃ³ el modelo
                    - FÃ³rmula: VP / (VP + FN)
                    - Importante cuando los falsos negativos son costosos
                    - **InterpretaciÃ³n:** Valores mÃ¡s altos = mejor modelo
                    
                    **F1-Score:**
                    - Media armÃ³nica entre precisiÃ³n y recall
                    - FÃ³rmula: 2 Ã— (PrecisiÃ³n Ã— Recall) / (PrecisiÃ³n + Recall)
                    - Ãštil cuando necesitas balance entre precisiÃ³n y recall
                    - **InterpretaciÃ³n:** Valores mÃ¡s altos = mejor balance
                    
                    **Curva ROC:**
                    - Muestra el rendimiento en diferentes umbrales de decisiÃ³n
                    - AUC (Ãrea bajo la curva): 0.5 = aleatorio, 1.0 = perfecto
                    
                    **Curva Precision-Recall:**
                    - Especialmente Ãºtil para clases desbalanceadas
                    - Muestra el trade-off entre precisiÃ³n y recall
                    
                    **VP = Verdaderos Positivos, FP = Falsos Positivos, FN = Falsos Negativos**
                """)

        # VisualizaciÃ³n avanzada - Predicciones correctas e incorrectas
        st.markdown("### VisualizaciÃ³n de Predicciones")

        # Construir mapping seguro label -> name
        label_to_name = {labels[i]: target_names[i]
                         for i in range(len(labels))}

        # Crear dataframe con resultados
        results_df = pd.DataFrame({
            'Real': [label_to_name.get(v, str(v)) for v in y_test],
            'PredicciÃ³n': [label_to_name.get(v, str(v)) for v in y_pred],
            'Correcto': (np.array(y_test) == np.array(y_pred))
        })

        # Mostrar algunas muestras
        col_viz1, col_viz2 = st.columns(2)
        with col_viz1:
            st.markdown("#### Muestras correctamente clasificadas")
            correct_samples = results_df[results_df['Correcto']].head(10)
            st.dataframe(correct_samples, height=300)

        with col_viz2:
            st.markdown("#### Muestras incorrectamente clasificadas")
            incorrect_samples = results_df[~results_df['Correcto']].head(10)
            if len(incorrect_samples) > 0:
                st.dataframe(incorrect_samples, height=300)
            else:
                st.info("Â¡Todas las muestras fueron clasificadas correctamente!")

        # GrÃ¡fico de precisiÃ³n por clase
        fig_prec, ax_prec = plt.subplots(figsize=(8, 4))  # Reducir altura
        prec_by_class = {}
        for name in target_names:
            prec_by_class[name] = report.get(name, {}).get('precision', 0.0)
        sns.barplot(x=list(prec_by_class.keys()), y=list(
            prec_by_class.values()), ax=ax_prec)
        ax_prec.set_ylim(0, 1)
        ax_prec.set_title('PrecisiÃ³n por clase')
        ax_prec.set_ylabel('PrecisiÃ³n')
        ax_prec.set_xlabel('Clase')

        # Mostrar con tamaÃ±o reducido
        col1, col2, col3 = st.columns([1, 3, 1])
        with col2:
            st.pyplot(fig_prec, use_container_width=True)

        st.markdown(get_image_download_link(
            fig_prec, "precision_por_clase", "ğŸ“¥ Descargar grÃ¡fico de precisiÃ³n"), unsafe_allow_html=True)

        # Mostrar cÃ³digo para generar el grÃ¡fico de precisiÃ³n por clase
        code_prec = PRECISION_CODE
        show_code_with_download(
            code_prec, "CÃ³digo para generar el grÃ¡fico de precisiÃ³n", "precision_por_clase.py")
    else:
        # MÃ©tricas para regresiÃ³n
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Error CuadrÃ¡tico Medio (MSE)", f"{mse:.4f}",
                      help="Promedio de los errores al cuadrado. Penaliza mÃ¡s los errores grandes.")
        with col2:
            st.metric("RMSE", f"{rmse:.4f}",
                      help="RaÃ­z cuadrada del MSE. En las mismas unidades que la variable objetivo.")
        with col3:
            st.metric("Error Absoluto Medio (MAE)", f"{mae:.4f}",
                      help="Promedio de los errores absolutos. Menos sensible a valores atÃ­picos.")
        with col4:
            st.metric("RÂ² Score", f"{r2:.4f}",
                      help="ProporciÃ³n de la varianza explicada por el modelo. 1 es predicciÃ³n perfecta.")

        # GrÃ¡fico de predicciones vs valores reales
        fig, ax = plt.subplots(figsize=(6, 5))  # Reducir tamaÃ±o
        scatter = ax.scatter(y_test, y_pred, alpha=0.5,
                             c=np.abs(y_test - y_pred), cmap='viridis')
        ax.plot([y_test.min(), y_test.max()], [
                y_test.min(), y_test.max()], 'r--')
        ax.set_xlabel('Valores reales')
        ax.set_ylabel('Predicciones')
        ax.set_title('Predicciones vs Valores reales')
        plt.colorbar(scatter, ax=ax, label='Error absoluto')

        # Mostrar con tamaÃ±o reducido
        col1, col2, col3 = st.columns([1, 3, 1])
        with col2:
            st.pyplot(fig, use_container_width=True)

        st.markdown(get_image_download_link(
            fig, "predicciones_vs_reales", "ğŸ“¥ Descargar grÃ¡fico"), unsafe_allow_html=True)

        # Mostrar el cÃ³digo para generar el grÃ¡fico de predicciones vs valores reales
        code_pred = PRED_VS_REAL_CODE
        show_code_with_download(
            code_pred, "CÃ³digo para generar este grÃ¡fico", "predicciones_vs_reales.py")

        # DistribuciÃ³n de errores
        fig_err, ax_err = plt.subplots(figsize=(6, 4))  # Reducir tamaÃ±o
        errors = y_test - y_pred
        sns.histplot(errors, kde=True, ax=ax_err)
        ax_err.axvline(x=0, color='r', linestyle='--')
        ax_err.set_title('DistribuciÃ³n de errores')
        ax_err.set_xlabel('Error (Real - PredicciÃ³n)')

        # Mostrar con tamaÃ±o reducido
        col1, col2, col3 = st.columns([1, 3, 1])
        with col2:
            st.pyplot(fig_err, use_container_width=True)

        st.markdown(get_image_download_link(
            fig_err, "distribucion_errores", "ğŸ“¥ Descargar grÃ¡fico"), unsafe_allow_html=True)

        # Mostrar el cÃ³digo para generar el grÃ¡fico de distribuciÃ³n de errores
        code_err = ERROR_DISTRIBUTION_CODE
        show_code_with_download(
            code_err, "CÃ³digo para generar este grÃ¡fico", "distribucion_errores.py")

        with st.expander("ğŸ“Š ExplicaciÃ³n de mÃ©tricas de regresiÃ³n"):
            st.markdown("""
            **RÂ² Score (Coeficiente de DeterminaciÃ³n):**
            - Mide quÃ© tan bien el modelo explica la variabilidad de los datos
            - Indica quÃ© proporciÃ³n de la varianza en la variable dependiente es predecible. 1 es predicciÃ³n perfecta, 0 significa que el modelo no es mejor que predecir la media.
            - Rango: 0 a 1 (valores negativos indican un modelo muy malo)
            - **InterpretaciÃ³n:**
                - RÂ² = 1.0: El modelo explica perfectamente toda la variabilidad
                - RÂ² = 0.8: El modelo explica el 80% de la variabilidad (muy bueno)
                - RÂ² = 0.5: El modelo explica el 50% de la variabilidad (moderado)
                - RÂ² = 0.0: El modelo no explica nada de la variabilidad
            
            **MAE (Error Absoluto Medio):**
            - Promedio de las diferencias absolutas entre valores reales y predichos
            - Se expresa en las mismas unidades que la variable objetivo
            - **InterpretaciÃ³n:** Valores mÃ¡s bajos = mejor modelo
            
            **RMSE (RaÃ­z del Error CuadrÃ¡tico Medio):**
            - Similar al MAE pero penaliza mÃ¡s los errores grandes
            - Se expresa en las mismas unidades que la variable objetivo
            - **InterpretaciÃ³n:** Valores mÃ¡s bajos = mejor modelo
            """)

    # Mostrar interpretaciÃ³n contextual
    st.markdown("### ğŸ” InterpretaciÃ³n de Resultados")

    if model_type != "ClasificaciÃ³n":
        # MÃ©tricas para regresiÃ³n
        mse = mean_squared_error(y_test, y_pred)
        rmse_value = np.sqrt(mse)
        mae_value = mean_absolute_error(y_test, y_pred)
        r2_value = r2_score(y_test, y_pred)

        interpretation = "**Resumen del Modelo:**\n"
        interpretation += f"- Tu modelo de regresiÃ³n lineal explica **{r2_value*100:.1f}%** de la variabilidad en los datos\n"
        interpretation += f"- En promedio, las predicciones se desvÃ­an **{mae_value:.2f} unidades** del valor real (MAE)\n"
        interpretation += f"- La raÃ­z del error cuadrÃ¡tico medio es **{rmse_value:.2f} unidades** (RMSE)\n"

        if rmse_value > mae_value * 1.5:
            interpretation += "\n- âš ï¸ El RMSE es significativamente mayor que el MAE, lo que indica la presencia de algunos errores grandes"

        st.info(interpretation)

    else:
        acc_value = report["accuracy"]
        prec_value = report['weighted avg']["precision"]
        rec_value = report['weighted avg']["recall"]
        f1_value = report['weighted avg']['f1-score']

        interpretation = "**Resumen del Modelo:**\n"
        interpretation += f"- Tu modelo clasifica correctamente **{acc_value*100:.1f}%** de los casos\n"
        interpretation += f"- De las predicciones positivas, **{prec_value*100:.1f}%** son correctas (PrecisiÃ³n)\n"
        interpretation += f"- Detecta **{rec_value*100:.1f}%** de todos los casos positivos reales (Recall)\n"

        if f1_value > 0:
            interpretation += f"- El F1-Score (balance entre precisiÃ³n y recall) es **{f1_value:.3f}**\n\n"

        # AnÃ¡lisis de balance entre precisiÃ³n y recall
        if abs(prec_value - rec_value) > 0.1:
            if prec_value > rec_value:
                interpretation += "- âš–ï¸ **Balance:** El modelo es mÃ¡s preciso pero menos sensible (mÃ¡s conservador)\n"
                interpretation += "- ğŸ’¡ **Sugerencia:** Si es importante detectar todos los casos positivos, considera ajustar el umbral de decisiÃ³n\n\n"
            else:
                interpretation += "- âš–ï¸ **Balance:** El modelo es mÃ¡s sensible pero menos preciso (mÃ¡s liberal)\n"
                interpretation += "- ğŸ’¡ **Sugerencia:** Si es importante evitar falsos positivos, considera ajustar el umbral de decisiÃ³n\n\n"
        else:
            interpretation += "- âš–ï¸ **Balance:** Buen equilibrio entre precisiÃ³n y recall\n\n"

        # AnÃ¡lisis especÃ­fico del accuracy
        if acc_value < 0.6:
            interpretation += "ğŸ” **Recomendaciones para mejorar:**\n"
            interpretation += "- Revisar la calidad y cantidad de datos de entrenamiento\n"
            interpretation += "- Considerar ingenierÃ­a de caracterÃ­sticas adicionales\n"
            interpretation += "- Probar diferentes algoritmos de clasificaciÃ³n\n"
            interpretation += "- Verificar si hay desbalance de clases\n"

        st.info(interpretation)


def show_prediction_path(tree_model, X_new, feature_names, class_names=None):
    """
    Muestra el camino de decisiÃ³n para una predicciÃ³n especÃ­fica.

    Parameters:
    -----------
    tree_model : DecisionTreeClassifier o DecisionTreeRegressor
        Modelo entrenado
    X_new : array
        Ejemplo para predecir
    feature_names : list
        Nombres de las caracterÃ­sticas
    class_names : list, optional
        Nombres de las clases (solo para clasificaciÃ³n)
    """
    # Convertir a numpy array si no lo es ya
    X_new = np.asarray(X_new)

    # Asegurarse de que X_new tenga el formato correcto (2D array con un solo ejemplo)
    if X_new.ndim > 2:
        # Si tiene mÃ¡s de 2 dimensiones, aplanamos a 2D
        X_new = X_new.reshape(1, -1)
    elif X_new.ndim == 1:
        # Si es un vector 1D, lo convertimos a 2D
        X_new = X_new.reshape(1, -1)

    # Obtener informaciÃ³n del Ã¡rbol
    feature_idx = tree_model.tree_.feature
    threshold = tree_model.tree_.threshold

    try:
        # Construir el camino de decisiÃ³n
        node_indicator = tree_model.decision_path(X_new)
        leaf_id = tree_model.apply(X_new)

        # Obtener los nodos en el camino
        node_index = node_indicator.indices[node_indicator.indptr[0]
            :node_indicator.indptr[1]]

        path_explanation = []
        for node_id in node_index:
            # Detener si es un nodo hoja
            if leaf_id[0] == node_id:
                continue

            # Obtener la caracterÃ­stica y el umbral de la decisiÃ³n
            feature_id = feature_idx[node_id]
            feature_name = feature_names[feature_id]
            threshold_value = threshold[node_id]

            # Comprobar si la muestra va por la izquierda o derecha
            # Usamos X_new[0, feature_id] porque X_new es un array 2D con un solo ejemplo
            if X_new[0, feature_id] <= threshold_value:
                path_explanation.append(
                    f"- {feature_name} = {X_new[0, feature_id]:.4f} â‰¤ {threshold_value:.4f} âœ…")
            else:
                path_explanation.append(
                    f"- {feature_name} = {X_new[0, feature_id]:.4f} > {threshold_value:.4f} âœ…")

        # Mostrar el camino de decisiÃ³n
        st.markdown("\n".join(path_explanation))

        # Mostrar el cÃ³digo que genera este camino de decisiÃ³n
        code_path = DECISION_PATH_CODE
        show_code_with_download(
            code_path, "CÃ³digo para generar el camino de decisiÃ³n", "camino_decision.py")
    except Exception as e:
        st.error(f"Error al mostrar el camino de decisiÃ³n: {str(e)}")
        st.info(
            "Intenta reformatear los datos de entrada o verificar que el modelo sea compatible.")


def neural_network_diagnostics(history, config):
    st.markdown("### ğŸ” DiagnÃ³stico del Entrenamiento")

    # Calcular mÃ©tricas diagnÃ³sticas
    diagnostics = analyze_training_diagnostics(history, config)

    # Mostrar diagnÃ³sticos en pestaÃ±as
    st.markdown("### ğŸ¯ Estado General")
    show_general_health(diagnostics, history, config)

    st.markdown("### ğŸ“Š Tendencias")
    show_trend_analysis(diagnostics, history)

    st.markdown("### âš ï¸ Alertas")
    show_training_alerts(diagnostics, history, config)

    # SECCIÃ“N 5: RECOMENDACIONES INTELIGENTES
    st.markdown("### ğŸ’¡ Recomendaciones")
    recommendations = generate_training_recommendations(
        diagnostics, history, config)

    if recommendations['excellent']:
        st.success("ğŸŒŸ " + recommendations['excellent'])
    elif recommendations['good']:
        st.info("âœ… " + recommendations['good'])
    elif recommendations['warning']:
        st.warning("âš ï¸ " + recommendations['warning'])
    elif recommendations['critical']:
        st.error("ğŸš¨ " + recommendations['critical'])

    # SECCIÃ“N 6: ACCIONES SUGERIDAS
    if recommendations.get('actions'):
        with st.expander("ğŸ”§ Acciones Sugeridas", expanded=False):
            for action in recommendations['actions']:
                st.markdown(f"â€¢ {action}")


def analyze_training_diagnostics(history, config):
    """Analiza el historial INCLUYENDO mÃ©tricas de rendimiento real."""
    diagnostics = {}

    loss_values = history.history['loss']
    epochs = len(loss_values)

    # 1. AnÃ¡lisis de convergencia de pÃ©rdida (como antes)
    if epochs >= 10:
        early_loss = np.mean(loss_values[:epochs//4])
        late_loss = np.mean(loss_values[-epochs//4:])
        convergence_rate = (early_loss - late_loss) / early_loss
    else:
        convergence_rate = (loss_values[0] - loss_values[-1]) / loss_values[0]

    diagnostics['convergence_rate'] = convergence_rate
    diagnostics['loss_converged'] = convergence_rate > 0.01

    # 2. AnÃ¡lisis de sobreajuste (como antes)
    if 'val_loss' in history.history:
        val_loss = history.history['val_loss']
        train_loss = loss_values

        window = min(5, epochs//2)
        recent_gap = np.mean(val_loss[-window:]) - \
            np.mean(train_loss[-window:])
        relative_gap = recent_gap / np.mean(train_loss[-window:])

        diagnostics['overfitting_gap'] = relative_gap
        diagnostics['is_overfitting'] = relative_gap > 0.15
    else:
        diagnostics['overfitting_gap'] = 0
        diagnostics['is_overfitting'] = False

    # 3. AnÃ¡lisis de estabilidad (como antes)
    if epochs >= 5:
        recent_losses = loss_values[-5:]
        stability = np.std(recent_losses) / np.mean(recent_losses)
        diagnostics['stability'] = stability
        diagnostics['loss_stable'] = stability < 0.05
    else:
        diagnostics['stability'] = float('inf')
        diagnostics['loss_stable'] = False

    # ğŸš€ 4. NUEVO: ANÃLISIS DE RENDIMIENTO REAL
    if config['task_type'] == 'ClasificaciÃ³n':
        if 'accuracy' in history.history:
            # Accuracy de entrenamiento
            train_acc = history.history['accuracy'][-1]
            diagnostics['final_train_accuracy'] = train_acc

            # Accuracy de validaciÃ³n (si existe)
            if 'val_accuracy' in history.history:
                val_acc = history.history['val_accuracy'][-1]
                diagnostics['final_val_accuracy'] = val_acc

                # ğŸ¯ CRITERIOS REALISTAS DE CALIDAD
                diagnostics['good_train_accuracy'] = train_acc > 0.7
                diagnostics['good_val_accuracy'] = val_acc > 0.7
                diagnostics['excellent_val_accuracy'] = val_acc > 0.85

                # Detectar sobreajuste por accuracy
                acc_gap = train_acc - val_acc
                diagnostics['accuracy_overfitting'] = acc_gap > 0.15

            else:
                # Solo tenemos accuracy de entrenamiento
                diagnostics['good_train_accuracy'] = train_acc > 0.7
                diagnostics['good_val_accuracy'] = False  # No disponible
                diagnostics['excellent_val_accuracy'] = False
                diagnostics['accuracy_overfitting'] = False
        else:
            # No hay mÃ©tricas de accuracy
            diagnostics['final_train_accuracy'] = None
            diagnostics['good_train_accuracy'] = False
            diagnostics['good_val_accuracy'] = False
            diagnostics['excellent_val_accuracy'] = False
            diagnostics['accuracy_overfitting'] = False

    else:  # RegresiÃ³n
        if 'mae' in history.history:
            final_mae = history.history['mae'][-1]
            diagnostics['final_mae'] = final_mae

            if 'val_mae' in history.history:
                val_mae = history.history['val_mae'][-1]
                diagnostics['final_val_mae'] = val_mae

                # Para regresiÃ³n, necesitamos contexto del rango de datos
                # Por ahora, usamos heurÃ­sticas generales
                diagnostics['good_mae'] = val_mae < np.mean(
                    history.history['mae'][:5])
            else:
                diagnostics['good_mae'] = final_mae < np.mean(
                    history.history['mae'][:5])
        else:
            diagnostics['good_mae'] = False

    # ğŸ¯ 5. EVALUACIÃ“N INTEGRAL DE CALIDAD
    # Ahora consideramos TANTO la curva de pÃ©rdida COMO el rendimiento real

    if config['task_type'] == 'ClasificaciÃ³n':
        # Para clasificaciÃ³n, el accuracy es lo mÃ¡s importante
        if diagnostics.get('good_val_accuracy', False):
            diagnostics['overall_quality'] = 'excellent' if diagnostics.get(
                'excellent_val_accuracy', False) else 'good'
        elif diagnostics.get('good_train_accuracy', False):
            diagnostics['overall_quality'] = 'moderate'  # Solo bueno en train
        else:
            diagnostics['overall_quality'] = 'poor'  # Accuracy bajo
    else:
        # Para regresiÃ³n
        if diagnostics.get('good_mae', False):
            diagnostics['overall_quality'] = 'good'
        else:
            diagnostics['overall_quality'] = 'poor'

    # 6. Combinar criterios de pÃ©rdida Y rendimiento
    diagnostics['converged'] = (
        diagnostics['loss_converged'] and
        diagnostics['overall_quality'] in ['excellent', 'good']
    )

    diagnostics['is_stable'] = (
        diagnostics['loss_stable'] and
        diagnostics['overall_quality'] != 'poor'
    )

    return diagnostics


def show_general_health(diagnostics, history, config):
    """Muestra el estado general INCLUYENDO rendimiento real."""

    health_score = 0
    total_checks = 0

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("**ğŸ“‹ Checklist de Salud:**")

        # 1. Convergencia (pÃ©rdida + rendimiento)
        if diagnostics['converged']:
            st.success("âœ… Modelo convergiÃ³ con buen rendimiento")
            health_score += 1
        else:
            if diagnostics['loss_converged']:
                st.warning("âš ï¸ PÃ©rdida convergiÃ³ pero rendimiento bajo")
            else:
                st.error("âŒ Modelo no convergiÃ³ suficientemente")
        total_checks += 1

        # 2. Sobreajuste (pÃ©rdida + accuracy)
        overfitting_detected = diagnostics['is_overfitting'] or diagnostics.get(
            'accuracy_overfitting', False)
        if not overfitting_detected:
            st.success("âœ… Sin signos de sobreajuste")
            health_score += 1
        else:
            if diagnostics['is_overfitting']:
                st.error(
                    f"âŒ Sobreajuste en pÃ©rdida (gap: {diagnostics['overfitting_gap']*100:.1f}%)")
            if diagnostics.get('accuracy_overfitting', False):
                train_acc = diagnostics.get('final_train_accuracy', 0)
                val_acc = diagnostics.get('final_val_accuracy', 0)
                st.error(
                    f"âŒ Sobreajuste en accuracy (train: {train_acc:.3f}, val: {val_acc:.3f})")
        total_checks += 1

        # 3. Estabilidad
        if diagnostics['is_stable']:
            st.success("âœ… Entrenamiento estable")
            health_score += 1
        else:
            st.warning(
                f"âš ï¸ Entrenamiento inestable (CV: {diagnostics['stability']*100:.1f}%)")
        total_checks += 1

        # ğŸš€ 4. NUEVO: RENDIMIENTO REAL
        if config['task_type'] == 'ClasificaciÃ³n':
            if diagnostics.get('excellent_val_accuracy', False):
                st.success("ğŸŒŸ Excelente accuracy de validaciÃ³n")
                health_score += 1
            elif diagnostics.get('good_val_accuracy', False):
                st.success("âœ… Buen accuracy de validaciÃ³n")
                health_score += 1
            elif diagnostics.get('good_train_accuracy', False):
                st.warning("âš ï¸ Solo buen accuracy en entrenamiento")
            else:
                st.error("âŒ Accuracy bajo (modelo no estÃ¡ aprendiendo bien)")
        else:
            if diagnostics.get('good_mae', False):
                st.success("âœ… Error de regresiÃ³n aceptable")
                health_score += 1
            else:
                st.error("âŒ Error de regresiÃ³n alto")
        total_checks += 1

    with col2:
        st.markdown("**ğŸ¯ PuntuaciÃ³n de Salud:**")

        health_percentage = (health_score / total_checks) * 100

        # ğŸ¯ NUEVA LÃ“GICA: Considerar rendimiento real
        overall_quality = diagnostics.get('overall_quality', 'poor')

        if overall_quality == 'excellent' and health_percentage >= 75:
            st.success(f"ğŸŒŸ Excelente: {health_percentage:.0f}%")
            health_status = "ğŸŒŸ Modelo listo para producciÃ³n"
        elif overall_quality == 'good' and health_percentage >= 60:
            st.info(f"ğŸ‘ Bueno: {health_percentage:.0f}%")
            health_status = "ğŸ‘ Modelo con buen rendimiento"
        elif overall_quality == 'moderate':
            st.warning(f"âš ï¸ Moderado: {health_percentage:.0f}%")
            health_status = "âš ï¸ Modelo necesita validaciÃ³n adicional"
        else:
            st.error(f"ğŸš¨ CrÃ­tico: {health_percentage:.0f}%")
            health_status = "ğŸš¨ Modelo no estÃ¡ funcionando correctamente"

        st.info(health_status)

        # Mostrar mÃ©tricas especÃ­ficas
        if config['task_type'] == 'ClasificaciÃ³n':
            if 'final_val_accuracy' in diagnostics and diagnostics['final_val_accuracy'] is not None:
                val_acc = diagnostics['final_val_accuracy']
                delta_color = "normal" if val_acc > 0.7 else "inverse"
                st.metric("ğŸ¯ Accuracy ValidaciÃ³n",
                          f"{val_acc:.3f}",
                          delta_color=delta_color,
                          help="MÃ©trica mÃ¡s importante para clasificaciÃ³n")
            elif 'final_train_accuracy' in diagnostics and diagnostics['final_train_accuracy'] is not None:
                train_acc = diagnostics['final_train_accuracy']
                st.metric("ğŸ¯ Accuracy Entrenamiento",
                          f"{train_acc:.3f}",
                          help="Solo disponible accuracy de entrenamiento")
        else:
            if 'final_val_mae' in diagnostics:
                st.metric("ğŸ“ MAE ValidaciÃ³n",
                          f"{diagnostics['final_val_mae']:.4f}",
                          help="Error promedio en validaciÃ³n")


def generate_training_recommendations(diagnostics, history, config):
    """Genera recomendaciones basadas en pÃ©rdida Y rendimiento real."""

    recommendations = {
        'excellent': None,
        'good': None,
        'warning': None,
        'critical': None,
        'actions': []
    }

    # ğŸ¯ NUEVA LÃ“GICA: Priorizar rendimiento real sobre curvas
    overall_quality = diagnostics.get('overall_quality', 'poor')

    # Contar problemas reales
    real_issues = []

    if not diagnostics['converged']:
        real_issues.append('convergencia')
    if diagnostics['is_overfitting'] or diagnostics.get('accuracy_overfitting', False):
        real_issues.append('sobreajuste')
    if not diagnostics['is_stable']:
        real_issues.append('estabilidad')
    if overall_quality == 'poor':
        real_issues.append('rendimiento_bajo')

    # Generar recomendaciones basadas en problemas reales
    if overall_quality == 'excellent' and len(real_issues) == 0:
        recommendations['excellent'] = "Â¡Entrenamiento excelente! Modelo con alto rendimiento y buenas curvas."
        recommendations['actions'] = [
            "âœ… El modelo estÃ¡ listo para producciÃ³n",
            "ğŸ“Š Considera hacer validaciÃ³n cruzada para confirmar robustez",
            "ğŸš€ Puedes proceder a hacer predicciones con confianza"
        ]
    elif overall_quality in ['good'] and len(real_issues) <= 1:
        recommendations['good'] = "Entrenamiento bueno con rendimiento satisfactorio."

        # Acciones especÃ­ficas basadas en el problema
        if 'sobreajuste' in real_issues:
            recommendations['actions'].extend([
                "ğŸ”§ AÃ±adir mÃ¡s regularizaciÃ³n (dropout, L1/L2)",
                "ğŸ“ˆ Aumentar datos de entrenamiento si es posible"
            ])
        elif 'estabilidad' in real_issues:
            recommendations['actions'].extend([
                "ğŸ“‰ Reducir learning rate para mayor estabilidad",
                "ğŸ“¦ Aumentar batch size"
            ])
        else:
            recommendations['actions'].append("ğŸ‘ Continuar con este enfoque")

    elif overall_quality == 'moderate' or len(real_issues) == 2:
        recommendations['warning'] = "Rendimiento moderado. El modelo funciona pero tiene limitaciones importantes."
        recommendations['actions'].extend([
            "ğŸ“Š Revisar datos de validaciÃ³n - pueden no ser representativos",
            "ğŸ”„ Considerar reentrenar con diferentes hiperparÃ¡metros",
            "ğŸ¯ Evaluar si la arquitectura es apropiada para el problema"
        ])
    else:
        # Rendimiento crÃ­tico
        if overall_quality == 'poor':
            recommendations['critical'] = f"âš ï¸ CRÃTICO: El modelo tiene muy bajo rendimiento (accuracy â‰¤ 70%). Las curvas pueden verse bien pero el modelo no estÃ¡ aprendiendo correctamente."
        else:
            recommendations['critical'] = "El entrenamiento tiene mÃºltiples problemas serios."

        recommendations['actions'].extend([
            "ğŸš¨ PRIORIDAD: Revisar datos de entrada y preprocesamiento",
            "ğŸ—ï¸ Simplificar arquitectura del modelo",
            "ğŸ“š Verificar que el problema sea realmente solucionable con estos datos",
            "ğŸ”„ Considerar cambiar completamente de enfoque"
        ])

    # AÃ±adir acciones especÃ­ficas para problemas de rendimiento
    if overall_quality == 'poor':
        recommendations['actions'].insert(
            0, "ğŸ¯ El modelo no estÃ¡ aprendiendo patrones Ãºtiles - revisar datos y arquitectura")

    return recommendations


def show_trend_analysis(diagnostics, history):
    """Muestra anÃ¡lisis de tendencias."""

    loss_values = history.history['loss']
    epochs = len(loss_values)

    st.markdown("### ğŸ“ˆ AnÃ¡lisis de Tendencias por Fase")
    with st.expander("ğŸ“š Â¿QuÃ© significan las tendencias de pÃ©rdida?", expanded=False):
        st.markdown("""
        ### ğŸ¯ **InterpretaciÃ³n de Tendencias por Fase:**
        
        **ğŸŸ¢ Fase Inicial (Primeras Ã©pocas):**
        - **Descendente rÃ¡pido:** âœ… Excelente - El modelo estÃ¡ aprendiendo patrones bÃ¡sicos
        - **Descendente lento:** âš ï¸ Learning rate muy bajo o datos complejos
        - **Ascendente:** ğŸš¨ Learning rate muy alto o problema en los datos
        
        **ğŸŸ¡ Fase Media (Ã‰pocas intermedias):**
        - **Descendente sostenido:** âœ… Aprendizaje progresivo saludable
        - **Plateau temprano:** âš ï¸ Posible saturaciÃ³n o learning rate muy bajo
        - **Fluctuaciones:** ğŸ“Š Normal, pero pueden indicar batch size pequeÃ±o
        
        **ğŸ”´ Fase Final (Ãšltimas Ã©pocas):**
        - **Estabilizado:** ğŸ¯ Ideal - Convergencia alcanzada
        - **Descendente:** ğŸ“ˆ AÃºn aprendiendo - Considera mÃ¡s Ã©pocas
        - **Ascendente:** ğŸš¨ Sobreajuste - Detener entrenamiento antes
        
        ### ğŸ“Š **Patrones de Calidad:**
        - **Curva logarÃ­tmica suave:** PatrÃ³n ideal de aprendizaje
        - **Escalones descendentes:** Learning rate scheduling efectivo
        - **Zigzag descendente:** Normal con batch gradient descent
        - **Valle en U:** Posible learning rate muy alto inicialmente
        """)
    if epochs >= 10:
        col1, col2, col3 = st.columns(3)
        # Dividir en segmentos para anÃ¡lisis
        early_segment = loss_values[:epochs//3]
        middle_segment = loss_values[epochs//3:2*epochs//3]
        late_segment = loss_values[2*epochs//3:]

        early_trend = np.polyfit(
            range(len(early_segment)), early_segment, 1)[0]
        middle_trend = np.polyfit(
            range(len(middle_segment)), middle_segment, 1)[0]
        late_trend = np.polyfit(
            range(len(late_segment)), late_segment, 1)[0]

        with col1:
            # InterpretaciÃ³n detallada para fase inicial
            direction = "Descendente" if early_trend < 0 else "Ascendente"

            if early_trend < -0.1:
                trend_quality = "ğŸš€ Excelente"
                trend_help = "Aprendizaje inicial muy efectivo"
            elif early_trend < -0.01:
                trend_quality = "âœ… Bueno"
                trend_help = "Aprendizaje inicial satisfactorio"
            elif early_trend < 0:
                trend_quality = "âš ï¸ Lento"
                trend_help = "Aprendizaje inicial lento - considera aumentar learning rate"
            else:
                trend_quality = "ğŸš¨ Problema"
                trend_help = "PÃ©rdida aumentando - revisar learning rate y datos"

            st.metric("ğŸŸ¢ Inicio (33%)",
                      f"{early_trend:.6f}",
                      f"{direction} - {trend_quality}",
                      help=trend_help)
        with col2:
            # InterpretaciÃ³n detallada para fase media
            direction = "Descendente" if middle_trend < 0 else "Ascendente"

            if middle_trend < -0.01:
                trend_quality = "ğŸ“ˆ Progresando"
                trend_help = "Aprendizaje continuo saludable"
            elif middle_trend < 0:
                trend_quality = "ğŸ“Š Lento"
                trend_help = "Aprendizaje desacelerando - normal en fases medias"
            elif abs(middle_trend) < 0.001:
                trend_quality = "ğŸ¯ Plateau"
                trend_help = "Posible convergencia temprana"
            else:
                trend_quality = "âš ï¸ Subiendo"
                trend_help = "PÃ©rdida aumentando - posible sobreajuste"

            st.metric("ğŸŸ¡ Medio (33%)",
                      f"{middle_trend:.6f}",
                      f"{direction} - {trend_quality}",
                      help=trend_help)
        with col3:
            # InterpretaciÃ³n detallada para fase final
            direction = "Descendente" if late_trend < 0 else "Ascendente"

            if abs(late_trend) < 0.001:
                trend_quality = "ğŸ¯ Convergido"
                trend_help = "Excelente - modelo estabilizado"
            elif late_trend < -0.01:
                trend_quality = "ğŸ“ˆ Mejorando"
                trend_help = "AÃºn aprendiendo - considera mÃ¡s Ã©pocas"
            elif late_trend < 0:
                trend_quality = "ğŸ“Š Lento"
                trend_help = "Mejora marginal - cerca de convergencia"
            else:
                trend_quality = "ğŸš¨ Sobreajuste"
                trend_help = "PÃ©rdida aumentando - detener entrenamiento"

            st.metric("ğŸ”´ Final (33%)",
                      f"{late_trend:.6f}",
                      f"{direction} - {trend_quality}",
                      help=trend_help)

         # AnÃ¡lisis comparativo entre fases
        st.markdown("### ğŸ”„ AnÃ¡lisis Comparativo")

        col_comp1, col_comp2 = st.columns(2)

        with col_comp1:
            st.markdown("**ğŸ“Š Velocidad de Aprendizaje:**")

            # Comparar velocidades de aprendizaje
            speeds = [abs(early_trend), abs(middle_trend), abs(late_trend)]
            phase_names = ["Inicial", "Media", "Final"]
            fastest_phase = phase_names[speeds.index(max(speeds))]

            st.info(f"ğŸƒâ€â™‚ï¸ **Fase mÃ¡s activa:** {fastest_phase}")

            # Detectar aceleraciÃ³n o desaceleraciÃ³n
            if abs(early_trend) > abs(middle_trend) > abs(late_trend):
                st.success("âœ… DesaceleraciÃ³n natural - PatrÃ³n ideal")
            elif abs(late_trend) > abs(early_trend):
                st.warning("âš ï¸ AceleraciÃ³n tardÃ­a - Revisar parÃ¡metros")
            else:
                st.info("ğŸ“Š PatrÃ³n mixto de aprendizaje")

        with col_comp2:
            st.markdown("**ğŸ¯ Consistencia:**")

            # AnÃ¡lisis de consistencia
            trend_consistency = np.std([early_trend, middle_trend, late_trend])

            if trend_consistency < 0.01:
                st.success("ğŸ¯ Alta consistencia - Aprendizaje estable")
            elif trend_consistency < 0.1:
                st.info("ğŸ“Š Consistencia moderada - Normal")
            else:
                st.warning("âš ï¸ Baja consistencia - Aprendizaje irregular")

            # Detectar patrones especÃ­ficos
            if early_trend < 0 and middle_trend < 0 and late_trend >= 0:
                st.error("ğŸš¨ PatrÃ³n de sobreajuste detectado")
            elif all(t < 0 for t in [early_trend, middle_trend, late_trend]):
                st.success("âœ… Mejora sostenida en todas las fases")
    else:
        st.info("ğŸ“Š Historial muy corto para anÃ¡lisis detallado")
        st.markdown("""
        **ğŸ” Para un anÃ¡lisis completo necesitas:**
        - âœ… Al menos 10 Ã©pocas de entrenamiento
        - ğŸ“Š Datos de validaciÃ³n (recomendado)
        - ğŸ¯ MÃ©tricas adicionales segÃºn el tipo de problema
        """)

    # SecciÃ³n de patrones detectados con mÃ¡s detalle
    st.markdown("### ğŸ”„ Patrones de Comportamiento Detectados")

    # Detectar patrones mÃ¡s especÃ­ficos
    patterns = []
    recommendations = []

    # AnÃ¡lisis de plateau
    if diagnostics.get('plateau', False):
        patterns.append(
            "ğŸ¯ **Plateau alcanzado:** El modelo ha llegado a su lÃ­mite de aprendizaje")
        recommendations.append(
            "ğŸ’¡ Considera early stopping o cambiar arquitectura")

    # AnÃ¡lisis de convergencia
    convergence_rate = diagnostics.get('convergence_rate', 0)
    if convergence_rate > 0.5:
        patterns.append(
            "ğŸš€ **Convergencia rÃ¡pida:** Excelente capacidad de aprendizaje")
        recommendations.append("âœ… ParÃ¡metros bien ajustados")
    elif convergence_rate > 0.1:
        patterns.append(
            "ğŸ“ˆ **Convergencia moderada:** Aprendizaje progresivo saludable")
        recommendations.append("ğŸ‘ Rendimiento satisfactorio")
    elif convergence_rate > 0.01:
        patterns.append("ğŸŒ **Convergencia lenta:** Aprendizaje gradual")
        recommendations.append(
            "âš ï¸ Considera aumentar learning rate o revisar datos")
    else:
        patterns.append(
            "âŒ **Sin convergencia:** Modelo no estÃ¡ aprendiendo efectivamente")
        recommendations.append("ğŸš¨ Revisar completamente configuraciÃ³n y datos")

    # AnÃ¡lisis de sobreajuste
    if diagnostics.get('is_overfitting', False):
        overfitting_gap = diagnostics.get('overfitting_gap', 0) * 100
        patterns.append(
            f"ğŸ“Š **Sobreajuste progresivo:** Gap train/val del {overfitting_gap:.1f}%")
        recommendations.append("ğŸ›‘ Implementar regularizaciÃ³n o early stopping")

    # AnÃ¡lisis de estabilidad
    if not diagnostics.get('is_stable', True):
        stability = diagnostics.get('stability', 0) * 100
        patterns.append(
            f"ğŸ“ˆ **Entrenamiento inestable:** Variabilidad del {stability:.1f}%")
        recommendations.append("ğŸ”§ Reducir learning rate o aumentar batch size")

    # Mostrar patrones y recomendaciones
    if patterns:
        col_pat1, col_pat2 = st.columns(2)

        with col_pat1:
            st.markdown("**ğŸ” Patrones Identificados:**")
            for pattern in patterns:
                st.markdown(f"â€¢ {pattern}")

        with col_pat2:
            st.markdown("**ğŸ’¡ Recomendaciones:**")
            for recommendation in recommendations:
                st.markdown(f"â€¢ {recommendation}")
    else:
        st.success(
            "ğŸ‰ No se detectaron patrones problemÃ¡ticos - Â¡Entrenamiento saludable!")

    # SecciÃ³n de contexto educativo adicional
    with st.expander("ğŸ“ Contexto Educativo: Â¿CÃ³mo interpretar estos nÃºmeros?", expanded=False):
        st.markdown("""
        ### ğŸ“ **Entendiendo los Valores de Tendencia:**
        
        **Valores Negativos (Descendente):**
        - `-0.1` o menor: ğŸš€ Aprendizaje muy rÃ¡pido
        - `-0.01` a `-0.1`: âœ… Aprendizaje saludable  
        - `-0.001` a `-0.01`: ğŸ“Š Aprendizaje gradual
        - `-0.0001` a `-0.001`: ğŸ¯ Convergencia fina
        
        **Valores Cerca de Cero:**
        - `-0.0001` a `+0.0001`: ğŸ¯ Convergencia ideal
        
        **Valores Positivos (Ascendente):**
        - `+0.0001` a `+0.001`: âš ï¸ Ligero deterioro
        - `+0.001` a `+0.01`: ğŸš¨ Problema moderado
        - `+0.01` o mayor: ğŸ’¥ Problema grave
        
        ### ğŸ”¬ **Factores que Afectan las Tendencias:**
        
        **Learning Rate:**
        - Muy alto â†’ Oscilaciones o divergencia
        - Muy bajo â†’ Convergencia lenta
        - Optimal â†’ Descenso suave y rÃ¡pido
        
        **Batch Size:**
        - PequeÃ±o â†’ MÃ¡s ruido, convergencia irregular
        - Grande â†’ Menos ruido, convergencia suave
        - Optimal â†’ Balance entre velocidad y estabilidad
        
        **Arquitectura del Modelo:**
        - Muy simple â†’ Plateau temprano (underfitting)
        - Muy compleja â†’ Sobreajuste tardÃ­o
        - Apropiada â†’ Convergencia saludable sin sobreajuste
        """)


def show_training_alerts(diagnostics, history, config):
    """Muestra alertas y problemas detectados."""

    alerts = []

    # Alertas crÃ­ticas
    if not diagnostics['converged']:
        alerts.append({
            'level': 'error',
            'message': 'Modelo no convergiÃ³ suficientemente',
            'action': 'Aumentar nÃºmero de Ã©pocas o ajustar learning rate'
        })

    if diagnostics['is_overfitting']:
        alerts.append({
            'level': 'error',
            'message': f'Sobreajuste detectado (gap: {diagnostics["overfitting_gap"]*100:.1f}%)',
            'action': 'Reducir complejidad del modelo, aÃ±adir regularizaciÃ³n o mÃ¡s datos'
        })

    # Alertas de advertencia
    if not diagnostics['is_stable']:
        alerts.append({
            'level': 'warning',
            'message': f'Entrenamiento inestable (variabilidad: {diagnostics["stability"]*100:.1f}%)',
            'action': 'Reducir learning rate o aumentar batch size'
        })

    if diagnostics.get('plateau', False):
        alerts.append({
            'level': 'info',
            'message': 'Plateau detectado en las Ãºltimas Ã©pocas',
            'action': 'El modelo puede haber alcanzado su lÃ­mite de aprendizaje'
        })

    # Mostrar alertas
    if not alerts:
        st.success("ğŸ‰ Â¡No se detectaron problemas significativos!")
    else:
        for alert in alerts:
            if alert['level'] == 'error':
                st.error(f"ğŸš¨ **{alert['message']}**\nğŸ’¡ {alert['action']}")
            elif alert['level'] == 'warning':
                st.warning(f"âš ï¸ **{alert['message']}**\nğŸ’¡ {alert['action']}")
            else:
                st.info(f"â„¹ï¸ **{alert['message']}**\nğŸ’¡ {alert['action']}")
