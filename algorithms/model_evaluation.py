"""
Este m√≥dulo contiene funciones para evaluar modelos de √°rboles de decisi√≥n.
Incluye funciones para calcular m√©tricas y visualizar resultados de clasificaci√≥n y regresi√≥n.
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, mean_squared_error, confusion_matrix,
    classification_report, precision_recall_fscore_support,
    r2_score, mean_absolute_error

)

from utils import get_image_download_link, show_code_with_download
from algorithms.code_examples import (
    CONFUSION_MATRIX_CODE,
    PRECISION_CODE,
    PRED_VS_REAL_CODE,
    ERROR_DISTRIBUTION_CODE,
    DECISION_PATH_CODE,
)


def evaluate_classification_model(y_test, y_pred, class_names):
    """
    Eval√∫a un modelo de clasificaci√≥n y devuelve las m√©tricas principales.

    Parameters:
    -----------
    y_test : array
        Valores reales
    y_pred : array
        Valores predichos
    class_names : list
        Nombres de las clases

    Returns:
    --------
    dict
        Diccionario con las m√©tricas del modelo
    """
    # Calcular m√©tricas
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(
        y_test, y_pred, target_names=class_names, output_dict=True)
    cm = confusion_matrix(y_test, y_pred)

    # Extraer precision y recall promedio ponderado para compatibilidad
    precision = report.get('weighted avg', {}).get('precision', 0.0)
    recall = report.get('weighted avg', {}).get('recall', 0.0)

    return {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "report": report,
        "confusion_matrix": cm,
        "y_pred": y_pred
    }


def evaluate_regression_model(y_test, y_pred):
    """
    Eval√∫a un modelo de regresi√≥n y devuelve las m√©tricas principales.

    Parameters:
    -----------
    y_test : array
        Valores reales
    y_pred : array
        Valores predichos

    Returns:
    --------
    dict
        Diccionario con las m√©tricas del modelo
    """
    # Calcular m√©tricas
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    # np.mean(np.abs(y_test - y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)  # 1 - np.sum((y_test - y_pred)**2) / \
    # np.sum((y_test - np.mean(y_test))**2)

    return {
        "mse": mse,
        "rmse": rmse,
        "mae": mae,
        "r2": r2,
        "y_pred": y_pred
    }


def show_detailed_evaluation(y_test, y_pred, class_names, model_type):
    """
    Muestra una evaluaci√≥n detallada del modelo con gr√°ficos y explicaciones.

    Parameters:
    -----------
    y_test : array
        Valores reales
    y_pred : array
        Valores predichos
    class_names : list
        Nombres de las clases
    model_type : str
        Tipo de modelo ('Clasificaci√≥n' o 'Regresi√≥n')
    """
    if model_type == "Clasificaci√≥n":
        # Calcular m√©tricas
        report = classification_report(
            y_test, y_pred, target_names=class_names, output_dict=True)
        report_df = pd.DataFrame(report).transpose()

        # M√©tricas globales
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            acc_value = report['accuracy']
            st.metric("Exactitud (Accuracy)", f"{acc_value:.4f}",
                      help="Proporci√≥n total de predicciones correctas")
            # Indicador de calidad
            if acc_value >= 0.9:
                st.success("Excelente", icon="üåü")
            elif acc_value >= 0.8:
                st.info("Muy bueno", icon="üëç")
            elif acc_value >= 0.7:
                st.warning("Bueno", icon="‚ö†Ô∏è")
            else:
                st.warning("Necesita mejora", icon="‚ùå")
        with col2:
            st.metric("Precisi√≥n media", f"{report['weighted avg']['precision']:.4f}",
                      help="Media ponderada de la precisi√≥n de cada clase")
        with col3:
            st.metric("Exhaustividad media (Recall)", f"{report['weighted avg']['recall']:.4f}",
                      help="Media ponderada de la exhaustividad de cada clase")
        with col4:
            st.metric("F1-Score medio", f"{report['weighted avg']['f1-score']:.4f}",
                      help="Media arm√≥nica de precisi√≥n y exhaustividad")
            f1_score = report['weighted avg']['f1-score']
            if f1_score >= 0.8:
                st.success("üåü Excelente balance")
            elif f1_score >= 0.7:
                st.info("üëç Buen balance")
            else:
                st.warning("‚ö†Ô∏è Balance mejorable")

        # M√©tricas por clase
        st.markdown("### ‚öñÔ∏è Matriz de Confusi√≥n")

        # Excluir filas avg y accuracy del dataframe
        report_by_class = report_df.drop(
            ['accuracy', 'macro avg', 'weighted avg'], errors='ignore')

        # Matriz de confusi√≥n
        cm = confusion_matrix(y_test, y_pred)
        fig_cm, ax_cm = plt.subplots(figsize=(6, 5))  # Reducir tama√±o
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax_cm,
                    xticklabels=class_names, yticklabels=class_names)
        ax_cm.set_xlabel('Predicci√≥n')
        ax_cm.set_ylabel('Real')
        ax_cm.set_title('Matriz de Confusi√≥n')

        col1, col2 = st.columns([1, 1])
        with col1:
            # Mostrar con tama√±o reducido
            col_inner1, col_inner2, col_inner3 = st.columns([1, 3, 1])
            with col_inner2:
                st.pyplot(fig_cm, use_container_width=True)

            st.markdown(get_image_download_link(
                fig_cm, "matriz_confusion", "üì• Descargar matriz de confusi√≥n"), unsafe_allow_html=True)

            # Mostrar c√≥digo para generar la matriz de confusi√≥n
            code_cm = CONFUSION_MATRIX_CODE
            show_code_with_download(
                code_cm, "C√≥digo para generar la matriz de confusi√≥n", "matriz_confusion.py")

        with col2:
            st.dataframe(report_by_class.style.format({
                'precision': '{:.4f}',
                'recall': '{:.4f}',
                'f1-score': '{:.4f}',
                'support': '{:.0f}'
            }))

            with st.expander("üìä Explicaci√≥n de m√©tricas"):
                st.markdown("""
                 **Accuracy (Exactitud):**
                    - Porcentaje de predicciones correctas del total
                    - Rango: 0 a 1 (0% a 100%)
                    - **Interpretaci√≥n:** Valores m√°s altos = mejor modelo
                    - **Cuidado:** Puede ser enga√±osa con clases desbalanceadas
                    
                    **Precision (Precisi√≥n):**
                    - De todas las predicciones positivas, cu√°ntas fueron correctas
                    - F√≥rmula: VP / (VP + FP)
                    - Importante cuando los falsos positivos son costosos
                    - **Interpretaci√≥n:** Valores m√°s altos = mejor modelo
                    
                    **Recall (Sensibilidad o Exhaustividad):**
                    - De todos los casos positivos reales, cu√°ntos detect√≥ el modelo
                    - F√≥rmula: VP / (VP + FN)
                    - Importante cuando los falsos negativos son costosos
                    - **Interpretaci√≥n:** Valores m√°s altos = mejor modelo
                    
                    **F1-Score:**
                    - Media arm√≥nica entre precisi√≥n y recall
                    - F√≥rmula: 2 √ó (Precisi√≥n √ó Recall) / (Precisi√≥n + Recall)
                    - √ötil cuando necesitas balance entre precisi√≥n y recall
                    - **Interpretaci√≥n:** Valores m√°s altos = mejor balance
                    
                    **Curva ROC:**
                    - Muestra el rendimiento en diferentes umbrales de decisi√≥n
                    - AUC (√Årea bajo la curva): 0.5 = aleatorio, 1.0 = perfecto
                    
                    **Curva Precision-Recall:**
                    - Especialmente √∫til para clases desbalanceadas
                    - Muestra el trade-off entre precisi√≥n y recall
                    
                    **VP = Verdaderos Positivos, FP = Falsos Positivos, FN = Falsos Negativos**
                """)

        # Visualizaci√≥n avanzada - Predicciones correctas e incorrectas
        st.markdown("### Visualizaci√≥n de Predicciones")

        # Crear dataframe con resultados
        results_df = pd.DataFrame({
            'Real': [class_names[x] for x in y_test],
            'Predicci√≥n': [class_names[x] for x in y_pred],
            'Correcto': y_test == y_pred
        })

        # Mostrar algunas muestras
        col_viz1, col_viz2 = st.columns(2)
        with col_viz1:
            st.markdown("#### Muestras correctamente clasificadas")
            correct_samples = results_df[results_df['Correcto']].head(10)
            st.dataframe(correct_samples, height=300)

        with col_viz2:
            st.markdown("#### Muestras incorrectamente clasificadas")
            incorrect_samples = results_df[~results_df['Correcto']].head(10)
            if len(incorrect_samples) > 0:
                st.dataframe(incorrect_samples, height=300)
            else:
                st.info("¬°Todas las muestras fueron clasificadas correctamente!")

        # Gr√°fico de precisi√≥n por clase
        fig_prec, ax_prec = plt.subplots(figsize=(8, 4))  # Reducir altura
        prec_by_class = {
            class_name: report[class_name]['precision'] for class_name in class_names}
        sns.barplot(x=list(prec_by_class.keys()), y=list(
            prec_by_class.values()), ax=ax_prec)
        ax_prec.set_ylim(0, 1)
        ax_prec.set_title('Precisi√≥n por clase')
        ax_prec.set_ylabel('Precisi√≥n')
        ax_prec.set_xlabel('Clase')

        # Mostrar con tama√±o reducido
        col1, col2, col3 = st.columns([1, 3, 1])
        with col2:
            st.pyplot(fig_prec, use_container_width=True)

        st.markdown(get_image_download_link(
            fig_prec, "precision_por_clase", "üì• Descargar gr√°fico de precisi√≥n"), unsafe_allow_html=True)

        # Mostrar c√≥digo para generar el gr√°fico de precisi√≥n por clase
        code_prec = PRECISION_CODE
        show_code_with_download(
            code_prec, "C√≥digo para generar el gr√°fico de precisi√≥n", "precision_por_clase.py")
    else:
        # M√©tricas para regresi√≥n
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Error Cuadr√°tico Medio (MSE)", f"{mse:.4f}",
                      help="Promedio de los errores al cuadrado. Penaliza m√°s los errores grandes.")
        with col2:
            st.metric("RMSE", f"{rmse:.4f}",
                      help="Ra√≠z cuadrada del MSE. En las mismas unidades que la variable objetivo.")
        with col3:
            st.metric("Error Absoluto Medio (MAE)", f"{mae:.4f}",
                      help="Promedio de los errores absolutos. Menos sensible a valores at√≠picos.")
        with col4:
            st.metric("R¬≤ Score", f"{r2:.4f}",
                      help="Proporci√≥n de la varianza explicada por el modelo. 1 es predicci√≥n perfecta.")

        # Gr√°fico de predicciones vs valores reales
        fig, ax = plt.subplots(figsize=(6, 5))  # Reducir tama√±o
        scatter = ax.scatter(y_test, y_pred, alpha=0.5,
                             c=np.abs(y_test - y_pred), cmap='viridis')
        ax.plot([y_test.min(), y_test.max()], [
                y_test.min(), y_test.max()], 'r--')
        ax.set_xlabel('Valores reales')
        ax.set_ylabel('Predicciones')
        ax.set_title('Predicciones vs Valores reales')
        plt.colorbar(scatter, ax=ax, label='Error absoluto')

        # Mostrar con tama√±o reducido
        col1, col2, col3 = st.columns([1, 3, 1])
        with col2:
            st.pyplot(fig, use_container_width=True)

        st.markdown(get_image_download_link(
            fig, "predicciones_vs_reales", "üì• Descargar gr√°fico"), unsafe_allow_html=True)

        # Mostrar el c√≥digo para generar el gr√°fico de predicciones vs valores reales
        code_pred = PRED_VS_REAL_CODE
        show_code_with_download(
            code_pred, "C√≥digo para generar este gr√°fico", "predicciones_vs_reales.py")

        # Distribuci√≥n de errores
        fig_err, ax_err = plt.subplots(figsize=(6, 4))  # Reducir tama√±o
        errors = y_test - y_pred
        sns.histplot(errors, kde=True, ax=ax_err)
        ax_err.axvline(x=0, color='r', linestyle='--')
        ax_err.set_title('Distribuci√≥n de errores')
        ax_err.set_xlabel('Error (Real - Predicci√≥n)')

        # Mostrar con tama√±o reducido
        col1, col2, col3 = st.columns([1, 3, 1])
        with col2:
            st.pyplot(fig_err, use_container_width=True)

        st.markdown(get_image_download_link(
            fig_err, "distribucion_errores", "üì• Descargar gr√°fico"), unsafe_allow_html=True)

        # Mostrar el c√≥digo para generar el gr√°fico de distribuci√≥n de errores
        code_err = ERROR_DISTRIBUTION_CODE
        show_code_with_download(
            code_err, "C√≥digo para generar este gr√°fico", "distribucion_errores.py")

        with st.expander("üìä Explicaci√≥n de m√©tricas de regresi√≥n"):
            st.markdown("""
            **R¬≤ Score (Coeficiente de Determinaci√≥n):**
            - Mide qu√© tan bien el modelo explica la variabilidad de los datos
            - Indica qu√© proporci√≥n de la varianza en la variable dependiente es predecible. 1 es predicci√≥n perfecta, 0 significa que el modelo no es mejor que predecir la media.
            - Rango: 0 a 1 (valores negativos indican un modelo muy malo)
            - **Interpretaci√≥n:**
                - R¬≤ = 1.0: El modelo explica perfectamente toda la variabilidad
                - R¬≤ = 0.8: El modelo explica el 80% de la variabilidad (muy bueno)
                - R¬≤ = 0.5: El modelo explica el 50% de la variabilidad (moderado)
                - R¬≤ = 0.0: El modelo no explica nada de la variabilidad
            
            **MAE (Error Absoluto Medio):**
            - Promedio de las diferencias absolutas entre valores reales y predichos
            - Se expresa en las mismas unidades que la variable objetivo
            - **Interpretaci√≥n:** Valores m√°s bajos = mejor modelo
            
            **RMSE (Ra√≠z del Error Cuadr√°tico Medio):**
            - Similar al MAE pero penaliza m√°s los errores grandes
            - Se expresa en las mismas unidades que la variable objetivo
            - **Interpretaci√≥n:** Valores m√°s bajos = mejor modelo
            """)

    # Mostrar interpretaci√≥n contextual
    st.markdown("### üîç Interpretaci√≥n de Resultados")

    if model_type != "Clasificaci√≥n":
        # M√©tricas para regresi√≥n
        mse = mean_squared_error(y_test, y_pred)
        rmse_value = np.sqrt(mse)
        mae_value = mean_absolute_error(y_test, y_pred)
        r2_value = r2_score(y_test, y_pred)

        interpretation = "**Resumen del Modelo:**\n"
        interpretation += f"- Tu modelo de regresi√≥n lineal explica **{r2_value*100:.1f}%** de la variabilidad en los datos\n"
        interpretation += f"- En promedio, las predicciones se desv√≠an **{mae_value:.2f} unidades** del valor real (MAE)\n"
        interpretation += f"- La ra√≠z del error cuadr√°tico medio es **{rmse_value:.2f} unidades** (RMSE)\n"

        if rmse_value > mae_value * 1.5:
            interpretation += "\n- ‚ö†Ô∏è El RMSE es significativamente mayor que el MAE, lo que indica la presencia de algunos errores grandes"

        st.info(interpretation)

    else:
        acc_value = report["accuracy"]
        prec_value = report['weighted avg']["precision"]
        rec_value = report['weighted avg']["recall"]
        f1_value = report['weighted avg']['f1-score']

        interpretation = "**Resumen del Modelo:**\n"
        interpretation += f"- Tu modelo clasifica correctamente **{acc_value*100:.1f}%** de los casos\n"
        interpretation += f"- De las predicciones positivas, **{prec_value*100:.1f}%** son correctas (Precisi√≥n)\n"
        interpretation += f"- Detecta **{rec_value*100:.1f}%** de todos los casos positivos reales (Recall)\n"

        if f1_value > 0:
            interpretation += f"- El F1-Score (balance entre precisi√≥n y recall) es **{f1_value:.3f}**\n\n"

        # An√°lisis de balance entre precisi√≥n y recall
        if abs(prec_value - rec_value) > 0.1:
            if prec_value > rec_value:
                interpretation += "- ‚öñÔ∏è **Balance:** El modelo es m√°s preciso pero menos sensible (m√°s conservador)\n"
                interpretation += "- üí° **Sugerencia:** Si es importante detectar todos los casos positivos, considera ajustar el umbral de decisi√≥n\n\n"
            else:
                interpretation += "- ‚öñÔ∏è **Balance:** El modelo es m√°s sensible pero menos preciso (m√°s liberal)\n"
                interpretation += "- üí° **Sugerencia:** Si es importante evitar falsos positivos, considera ajustar el umbral de decisi√≥n\n\n"
        else:
            interpretation += "- ‚öñÔ∏è **Balance:** Buen equilibrio entre precisi√≥n y recall\n\n"

        # An√°lisis espec√≠fico del accuracy
        if acc_value < 0.6:
            interpretation += "üîç **Recomendaciones para mejorar:**\n"
            interpretation += "- Revisar la calidad y cantidad de datos de entrenamiento\n"
            interpretation += "- Considerar ingenier√≠a de caracter√≠sticas adicionales\n"
            interpretation += "- Probar diferentes algoritmos de clasificaci√≥n\n"
            interpretation += "- Verificar si hay desbalance de clases\n"

        st.info(interpretation)


def show_prediction_path(tree_model, X_new, feature_names, class_names=None):
    """
    Muestra el camino de decisi√≥n para una predicci√≥n espec√≠fica.

    Parameters:
    -----------
    tree_model : DecisionTreeClassifier o DecisionTreeRegressor
        Modelo entrenado
    X_new : array
        Ejemplo para predecir
    feature_names : list
        Nombres de las caracter√≠sticas
    class_names : list, optional
        Nombres de las clases (solo para clasificaci√≥n)
    """
    # Convertir a numpy array si no lo es ya
    X_new = np.asarray(X_new)

    # Asegurarse de que X_new tenga el formato correcto (2D array con un solo ejemplo)
    if X_new.ndim > 2:
        # Si tiene m√°s de 2 dimensiones, aplanamos a 2D
        X_new = X_new.reshape(1, -1)
    elif X_new.ndim == 1:
        # Si es un vector 1D, lo convertimos a 2D
        X_new = X_new.reshape(1, -1)

    # Obtener informaci√≥n del √°rbol
    feature_idx = tree_model.tree_.feature
    threshold = tree_model.tree_.threshold

    try:
        # Construir el camino de decisi√≥n
        node_indicator = tree_model.decision_path(X_new)
        leaf_id = tree_model.apply(X_new)

        # Obtener los nodos en el camino
        node_index = node_indicator.indices[node_indicator.indptr[0]:node_indicator.indptr[1]]

        path_explanation = []
        for node_id in node_index:
            # Detener si es un nodo hoja
            if leaf_id[0] == node_id:
                continue

            # Obtener la caracter√≠stica y el umbral de la decisi√≥n
            feature_id = feature_idx[node_id]
            feature_name = feature_names[feature_id]
            threshold_value = threshold[node_id]

            # Comprobar si la muestra va por la izquierda o derecha
            # Usamos X_new[0, feature_id] porque X_new es un array 2D con un solo ejemplo
            if X_new[0, feature_id] <= threshold_value:
                path_explanation.append(
                    f"- {feature_name} = {X_new[0, feature_id]:.4f} ‚â§ {threshold_value:.4f} ‚úÖ")
            else:
                path_explanation.append(
                    f"- {feature_name} = {X_new[0, feature_id]:.4f} > {threshold_value:.4f} ‚úÖ")

        # Mostrar el camino de decisi√≥n
        st.markdown("\n".join(path_explanation))

        # Mostrar el c√≥digo que genera este camino de decisi√≥n
        code_path = DECISION_PATH_CODE
        show_code_with_download(
            code_path, "C√≥digo para generar el camino de decisi√≥n", "camino_decision.py")
    except Exception as e:
        st.error(f"Error al mostrar el camino de decisi√≥n: {str(e)}")
        st.info(
            "Intenta reformatear los datos de entrada o verificar que el modelo sea compatible.")
