"""
Este mÃ³dulo contiene funciones para la carga y gestiÃ³n de datos en la aplicaciÃ³n MLTutor.
Incluye funciones para cargar conjuntos de datos integrados, cargar archivos CSV y procesar datos.
"""

import pandas as pd
import numpy as np
import streamlit as st
import os
from sklearn.datasets import load_iris, load_wine, load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
from additional_datasets import load_additional_dataset


def load_builtin_dataset(dataset_name):
    """
    Carga un conjunto de datos integrado segÃºn el nombre.

    Parameters:
    -----------
    dataset_name : str
        Nombre del conjunto de datos ('Iris', 'Vino', 'CÃ¡ncer', 'Titanic', 'Propinas', 'Viviendas California', 'PingÃ¼inos')

    Returns:
    --------
    X : DataFrame
        CaracterÃ­sticas
    y : Series
        Variable objetivo
    feature_names : list
        Nombres de las caracterÃ­sticas
    class_names : list
        Nombres de las clases
    dataset_info : str
        InformaciÃ³n sobre el conjunto de datos
    """
    if "Iris" in dataset_name:
        data = load_iris()
        X = pd.DataFrame(data.data, columns=data.feature_names)
        y = pd.Series(data.target, name="species")
        feature_names = data.feature_names.tolist() if hasattr(
            data.feature_names, 'tolist') else list(data.feature_names)
        class_names = data.target_names.tolist()
        dataset_info = "Dataset Iris: 150 muestras, 4 caracterÃ­sticas, 3 clases de flores"
        task_type = "ClasificaciÃ³n"

    elif "Vino" in dataset_name:
        data = load_wine()
        X = pd.DataFrame(data.data, columns=data.feature_names)
        y = pd.Series(data.target, name="wine_type")
        feature_names = data.feature_names.tolist() if hasattr(
            data.feature_names, 'tolist') else list(data.feature_names)
        class_names = data.target_names.tolist()
        dataset_info = "Dataset Vino: 178 muestras, 13 caracterÃ­sticas, 3 tipos de vino"
        task_type = "ClasificaciÃ³n"

    elif "CÃ¡ncer" in dataset_name:
        data = load_breast_cancer()
        X = pd.DataFrame(data.data, columns=data.feature_names)
        y = pd.Series(data.target, name="malignant")
        feature_names = data.feature_names.tolist() if hasattr(
            data.feature_names, 'tolist') else list(data.feature_names)
        class_names = data.target_names.tolist()
        dataset_info = "Dataset CÃ¡ncer: 569 muestras, 30 caracterÃ­sticas, diagnÃ³stico binario"
        task_type = "ClasificaciÃ³n"

    elif "Titanic" in dataset_name:
        X, y, feature_names, class_names, dataset_info, task_type = load_additional_dataset(
            dataset_name)

    elif "Propinas" in dataset_name:
        X, y, feature_names, class_names, dataset_info, task_type = load_additional_dataset(
            dataset_name)

    elif "Viviendas California" in dataset_name:
        X, y, feature_names, class_names, dataset_info, task_type = load_additional_dataset(
            dataset_name)

    elif "PingÃ¼inos" in dataset_name:
        X, y, feature_names, class_names, dataset_info, task_type = load_additional_dataset(
            dataset_name)

    else:
        raise ValueError(f"Conjunto de datos '{dataset_name}' no reconocido")

    return X, y, feature_names, class_names, dataset_info, task_type


def load_dataset_from_file(file_path, target_column=None, task_type="auto"):
    """
    Carga un conjunto de datos desde un archivo CSV.

    Parameters:
    -----------
    file_path : str
        Ruta al archivo CSV
    target_column : str, optional
        Nombre de la columna objetivo. Si es None, se usa la Ãºltima columna.
    task_type : str, default="auto"
        Tipo de tarea ('ClasificaciÃ³n', 'RegresiÃ³n', 'auto')

    Returns:
    --------
    X : DataFrame
        CaracterÃ­sticas
    y : Series
        Variable objetivo
    feature_names : list
        Nombres de las caracterÃ­sticas
    class_names : list or None
        Nombres de las clases (solo para clasificaciÃ³n)
    dataset_info : str
        InformaciÃ³n sobre el conjunto de datos
    task_type : str
        Tipo de tarea ('ClasificaciÃ³n' o 'RegresiÃ³n')
    """
    # Cargar el dataset
    try:
        df = pd.read_csv(file_path)
    except Exception as e:
        raise ValueError(f"Error al cargar el archivo: {str(e)}")

    # Determinar la columna objetivo
    if target_column is None:
        target_column = df.columns[-1]

    if target_column not in df.columns:
        raise ValueError(
            f"Columna objetivo '{target_column}' no encontrada en el dataset")

    # Separar caracterÃ­sticas y objetivo
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Codificar automÃ¡ticamente caracterÃ­sticas categÃ³ricas (string/object)
    categorical_features = X.select_dtypes(include=['object']).columns
    if len(categorical_features) > 0:
        from sklearn.preprocessing import LabelEncoder
        for col in categorical_features:
            le = LabelEncoder()
            X[col] = le.fit_transform(X[col].astype(str))

    # Codificar variable objetivo si es categÃ³rica
    if y.dtype == 'object':
        from sklearn.preprocessing import LabelEncoder
        le_target = LabelEncoder()
        y = pd.Series(le_target.fit_transform(y.astype(str)), name=target_column)

    # Obtener nombres de caracterÃ­sticas
    feature_names = X.columns.tolist()

    # Determinar tipo de tarea si es automÃ¡tico
    if task_type == "auto":
        # Si y tiene pocos valores Ãºnicos (menos del 5% del total), asumimos clasificaciÃ³n
        if len(y.unique()) < max(5, len(y) * 0.05):
            task_type = "ClasificaciÃ³n"
        else:
            task_type = "RegresiÃ³n"

    # Para clasificaciÃ³n, determinar nombres de clases
    if task_type == "ClasificaciÃ³n":
        class_values = sorted(y.unique())
        # Intentar usar etiquetas si estÃ¡n disponibles
        if hasattr(y, 'cat') and hasattr(y.cat, 'categories'):
            class_names = y.cat.categories.tolist()
        else:
            # Si son nÃºmeros enteros, usar nombres genÃ©ricos
            if all(isinstance(val, (int, np.integer)) for val in class_values):
                class_names = [f"Clase {i}" for i in class_values]
            else:
                class_names = [str(val) for val in class_values]
    else:
        class_names = None

    # Generar informaciÃ³n del dataset
    n_samples, n_features = X.shape
    if task_type == "ClasificaciÃ³n":
        n_classes = len(class_names)
        dataset_info = f"Dataset personalizado: {n_samples} muestras, {n_features} caracterÃ­sticas, {n_classes} clases"
    else:
        dataset_info = f"Dataset personalizado: {n_samples} muestras, {n_features} caracterÃ­sticas, regresiÃ³n"

    return X, y, feature_names, class_names, dataset_info, task_type


def list_sample_datasets():
    """
    Lista los conjuntos de datos de ejemplo disponibles en el directorio 'data/sample_datasets'.

    Returns:
    --------
    list
        Lista de rutas a los archivos CSV de ejemplo
    """
    sample_dir = os.path.join(os.path.dirname(
        os.path.abspath(__file__)), "data", "sample_datasets")
    if not os.path.exists(sample_dir):
        return []

    return [
        os.path.join(sample_dir, f)
        for f in os.listdir(sample_dir)
        if f.endswith('.csv')
    ]


def preprocess_data(X, y, test_size=0.3, random_state=42):
    """
    Divide los datos en conjuntos de entrenamiento y prueba.
    Maneja automÃ¡ticamente caracterÃ­sticas categÃ³ricas restantes.

    Parameters:
    -----------
    X : DataFrame o array
        CaracterÃ­sticas
    y : Series o array
        Variable objetivo
    test_size : float, default=0.3
        ProporciÃ³n de datos para prueba
    random_state : int, default=42
        Semilla para reproducibilidad

    Returns:
    --------
    X_train, X_test, y_train, y_test : arrays
        Conjuntos de entrenamiento y prueba
    """
    # Convertir a DataFrame si es necesario para manejar tipos de datos
    if isinstance(X, np.ndarray):
        X = pd.DataFrame(X)
    
    # Codificar cualquier caracterÃ­stica categÃ³rica restante
    categorical_features = X.select_dtypes(include=['object']).columns
    if len(categorical_features) > 0:
        X = X.copy()  # Evitar modificar el original
        for col in categorical_features:
            le = LabelEncoder()
            X[col] = le.fit_transform(X[col].astype(str))
    
    # Codificar variable objetivo si es necesario
    if hasattr(y, 'dtype') and y.dtype == 'object':
        le_target = LabelEncoder()
        y = le_target.fit_transform(y.astype(str))
    
    return train_test_split(X, y, test_size=test_size, random_state=random_state)


def load_data(dataset_option):
    """
    Carga un conjunto de datos segÃºn la opciÃ³n seleccionada.

    Parameters:
    -----------
    dataset_option : str
        Nombre del conjunto de datos a cargar

    Returns:
    --------
    X : DataFrame
        CaracterÃ­sticas
    y : Series
        Variable objetivo
    feature_names : list
        Nombres de las caracterÃ­sticas
    class_names : list o None
        Nombres de las clases (para clasificaciÃ³n)
    dataset_info : str
        InformaciÃ³n sobre el conjunto de datos
    task_type : str
        Tipo de tarea
    """
    # Lista de datasets integrados (tanto formato antiguo como nuevo)
    dataset_mapping = {
        "Iris (clasificaciÃ³n de flores)": "ğŸŒ¸ Iris - ClasificaciÃ³n de flores",
        "Vino (clasificaciÃ³n de vinos)": "ğŸ· Vino - ClasificaciÃ³n de vinos",
        "CÃ¡ncer de mama (diagnÃ³stico)": "ğŸ”¬ CÃ¡ncer - DiagnÃ³stico binario",
        "ğŸŒ¸ Iris - ClasificaciÃ³n de flores": "ğŸŒ¸ Iris - ClasificaciÃ³n de flores",
        "ğŸ· Vino - ClasificaciÃ³n de vinos": "ğŸ· Vino - ClasificaciÃ³n de vinos",
        "ğŸ”¬ CÃ¡ncer - DiagnÃ³stico binario": "ğŸ”¬ CÃ¡ncer - DiagnÃ³stico binario",
        "ğŸš¢ Titanic - Supervivencia": "ğŸš¢ Titanic - Supervivencia",
        "ğŸ’° Propinas - PredicciÃ³n de propinas": "ğŸ’° Propinas - PredicciÃ³n de propinas",
        "ğŸ  Viviendas California - Precios": "ğŸ  Viviendas California - Precios",
        "ğŸ§ PingÃ¼inos - ClasificaciÃ³n de especies": "ğŸ§ PingÃ¼inos - ClasificaciÃ³n de especies"
    }

    # Comprobar si es un dataset CSV personalizado cargado
    import streamlit as st
    if hasattr(st, 'session_state') and 'csv_datasets' in st.session_state and dataset_option in st.session_state.csv_datasets:
        csv_info = st.session_state.csv_datasets[dataset_option]
        return load_dataset_from_file(
            csv_info['file_path'],
            csv_info['target_col'],
            csv_info['task_type']
        )

    # Normalizar el nombre del dataset
    if dataset_option in dataset_mapping:
        normalized_name = dataset_mapping[dataset_option]
        return load_builtin_dataset(normalized_name)

    # Comprobar si es un conjunto de datos personalizado (CSV) - compatibilidad con formato anterior
    if dataset_option.endswith('.csv'):
        file_path = dataset_option
        if not os.path.exists(file_path):
            # Comprobar si estÃ¡ en el directorio de muestras
            sample_dir = os.path.join(os.path.dirname(
                os.path.abspath(__file__)), "data", "sample_datasets")
            sample_path = os.path.join(sample_dir, os.path.basename(file_path))
            if os.path.exists(sample_path):
                file_path = sample_path

        return load_dataset_from_file(file_path)

    # Si llegamos aquÃ­, el conjunto de datos no se reconoce
    raise ValueError(f"Conjunto de datos '{dataset_option}' no reconocido")


def create_dataset_selector(show_predefined=True):
    """
    Crea un selector de conjuntos de datos mejorado para la interfaz de usuario.

    Parameters:
    -----------
    show_predefined : bool, default=True
        Si mostrar la opciÃ³n de datasets predefinidos

    Returns:
    --------
    str or tuple
        Si se carga un archivo CSV: tuple (ruta_archivo, columna_objetivo, tipo_tarea)
        Si no: nombre del dataset seleccionado
    """
    st.subheader("ğŸ“Š SelecciÃ³n de Dataset")

    # Opciones de datasets expandidas
    builtin_options = [
        "ğŸŒ¸ Iris - ClasificaciÃ³n de flores",
        "ğŸ· Vino - ClasificaciÃ³n de vinos",
        "ğŸ”¬ CÃ¡ncer - DiagnÃ³stico binario",
        "ğŸš¢ Titanic - Supervivencia",
        "ğŸ’° Propinas - PredicciÃ³n de propinas",
        "ğŸ  Viviendas California - Precios",
        "ğŸ§ PingÃ¼inos - ClasificaciÃ³n de especies"
    ]

    # MÃ©todo de carga
    if show_predefined:
        data_source = st.radio(
            "Fuente de datos:",
            ["Datasets predefinidos", "Cargar archivo CSV"],
            help="Selecciona si quieres usar un dataset incluido o cargar tu propio archivo CSV"
        )
    else:
        data_source = "Cargar archivo CSV"
        st.info("ğŸ’¡ En esta secciÃ³n puedes cargar tu propio archivo CSV para anÃ¡lisis personalizado")

    if data_source == "Cargar archivo CSV":
        st.markdown("### ğŸ“ Cargar Archivo CSV")

        uploaded_file = st.file_uploader(
            "Selecciona un archivo CSV:",
            type=['csv'],
            help="El archivo debe estar en formato CSV con encabezados"
        )

        if uploaded_file is not None:
            try:
                # Leer el archivo
                df = pd.read_csv(uploaded_file)

                # Validaciones bÃ¡sicas
                if df.empty:
                    st.error("âŒ El archivo estÃ¡ vacÃ­o")
                    return None

                if len(df.columns) < 2:
                    st.error(
                        "âŒ El archivo debe tener al menos 2 columnas (caracterÃ­sticas + objetivo)")
                    return None

                # Mostrar vista previa
                st.success(
                    f"âœ… Archivo cargado exitosamente: {df.shape[0]} filas, {df.shape[1]} columnas")

                with st.expander("ğŸ‘€ Vista previa del dataset", expanded=True):
                    st.dataframe(df.head(), use_container_width=True)

                    # InformaciÃ³n del dataset
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Filas", df.shape[0])
                    with col2:
                        st.metric("Columnas", df.shape[1])
                    with col3:
                        missing_values = df.isnull().sum().sum()
                        st.metric("Valores faltantes", missing_values)

                # ConfiguraciÃ³n
                st.markdown("### âš™ï¸ ConfiguraciÃ³n")

                col1, col2 = st.columns(2)

                with col1:
                    target_col = st.selectbox(
                        "Columna objetivo:",
                        df.columns.tolist(),
                        index=len(df.columns)-1,
                        help="Selecciona la columna que contiene la variable a predecir"
                    )

                with col2:
                    # Detectar tipo de tarea automÃ¡ticamente
                    if target_col:
                        unique_values = df[target_col].nunique()
                        total_values = len(df[target_col])

                        if unique_values <= 20 and unique_values < total_values * 0.1:
                            suggested_task = "ClasificaciÃ³n"
                        else:
                            suggested_task = "RegresiÃ³n"
                    else:
                        suggested_task = "ClasificaciÃ³n"

                    task_options = ["auto", "ClasificaciÃ³n", "RegresiÃ³n"]
                    default_idx = task_options.index(
                        suggested_task) if suggested_task in task_options else 0

                    task_type = st.selectbox(
                        "Tipo de tarea:",
                        task_options,
                        index=default_idx,
                        help="'auto' detecta automÃ¡ticamente, o selecciona manualmente"
                    )

                # InformaciÃ³n adicional sobre la columna objetivo
                if target_col:
                    st.markdown("### ğŸ¯ InformaciÃ³n de la Variable Objetivo")
                    col1, col2 = st.columns(2)

                    with col1:
                        st.write(f"**Columna:** {target_col}")
                        st.write(
                            f"**Valores Ãºnicos:** {df[target_col].nunique()}")
                        st.write(f"**Tipo de datos:** {df[target_col].dtype}")

                    with col2:
                        if df[target_col].nunique() <= 10:
                            st.write("**DistribuciÃ³n de valores:**")
                            value_counts = df[target_col].value_counts()
                            for val, count in value_counts.items():
                                st.write(
                                    f"â€¢ {val}: {count} ({count/len(df)*100:.1f}%)")

                # Guardar en archivo temporal
                import tempfile
                temp_file = tempfile.NamedTemporaryFile(
                    mode='w+', delete=False, suffix='.csv')
                df.to_csv(temp_file.name, index=False)
                temp_file.close()

                # AÃ±adir dataset a la lista de datasets disponibles en session_state
                dataset_name = f"ğŸ“„ {uploaded_file.name}"
                if 'csv_datasets' not in st.session_state:
                    st.session_state.csv_datasets = {}
                
                st.session_state.csv_datasets[dataset_name] = {
                    'file_path': temp_file.name,
                    'target_col': target_col,
                    'task_type': task_type,
                    'original_name': uploaded_file.name
                }

                # Actualizar el dataset seleccionado para usar el nuevo CSV
                st.session_state.selected_dataset = dataset_name

                st.success(f"âœ… Dataset '{uploaded_file.name}' aÃ±adido a la lista de datasets disponibles")

                return temp_file.name, target_col, task_type

            except Exception as e:
                st.error(f"âŒ Error al procesar el archivo: {str(e)}")
                return None

        else:
            st.info("ğŸ‘† Por favor, carga un archivo CSV para continuar")
            return None

    else:  # Datasets predefinidos
        st.markdown("### ğŸ¯ Datasets Disponibles")
        dataset_option = st.selectbox(
            "Selecciona un dataset:",
            builtin_options,
            help="Estos datasets estÃ¡n listos para usar y son perfectos para aprender Machine Learning"
        )

        # Mostrar informaciÃ³n del dataset seleccionado
        dataset_info = {
            "ğŸŒ¸ Iris - ClasificaciÃ³n de flores": "150 muestras â€¢ 4 caracterÃ­sticas â€¢ 3 especies de iris â€¢ ClasificaciÃ³n clÃ¡sica",
            "ğŸ· Vino - ClasificaciÃ³n de vinos": "178 muestras â€¢ 13 caracterÃ­sticas quÃ­micas â€¢ 3 clases de vino â€¢ ClasificaciÃ³n",
            "ğŸ”¬ CÃ¡ncer - DiagnÃ³stico binario": "569 muestras â€¢ 30 caracterÃ­sticas â€¢ Benigno/Maligno â€¢ ClasificaciÃ³n mÃ©dica",
            "ğŸš¢ Titanic - Supervivencia": "891 pasajeros â€¢ 11 caracterÃ­sticas â€¢ Supervivencia â€¢ ClasificaciÃ³n histÃ³rica",
            "ğŸ’° Propinas - PredicciÃ³n de propinas": "244 cuentas â€¢ 6 caracterÃ­sticas â€¢ Monto de propina â€¢ RegresiÃ³n",
            "ğŸ  Viviendas California - Precios": "20,640 distritos â€¢ 8 caracterÃ­sticas â€¢ Precio vivienda â€¢ RegresiÃ³n",
            "ğŸ§ PingÃ¼inos - ClasificaciÃ³n de especies": "333 pingÃ¼inos â€¢ 6 caracterÃ­sticas â€¢ 3 especies â€¢ ClasificaciÃ³n biolÃ³gica"
        }

        if dataset_option in dataset_info:
            st.info(f"â„¹ï¸ {dataset_info[dataset_option]}")

        return dataset_option
