import tensorflow as tf
import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

from dataset.dataset_manager import load_data
from dataset.dataset_tab import run_select_dataset, show_dataset_info, run_explore_dataset_tab
from utils import create_info_box, get_image_download_link, show_code_with_download
from algorithms.code_examples import generate_neural_network_architecture_code, generate_neural_network_evaluation_code
from algorithms.model_training import train_neural_network
from algorithms.model_evaluation import show_detailed_evaluation, neural_network_diagnostics
from algorithms.export import show_neural_network_export
from viz.nn import (
    evaluate_nn,
    show_neural_network_evaluation,
    create_neural_network_visualization,
    calculate_network_parameters,
    show_neural_network_visualizations,
    show_training_history_tab,
    show_weights_analysis_tab,
    show_decision_surface_tab,
    show_layer_activations_tab
)
from ui import create_button_panel, create_prediction_interface
from apps.navbar import navbar


def run_neural_networks_app():
    """Ejecuta la aplicaci√≥n espec√≠fica de redes neuronales."""
    st.header("üß† Redes Neuronales")
    st.markdown(
        "Aprende sobre redes neuronales artificiales de forma visual e interactiva")

    # Informaci√≥n sobre redes neuronales
    with st.expander("‚ÑπÔ∏è ¬øQu√© son las Redes Neuronales?", expanded=False):
        st.markdown("""
        Las redes neuronales artificiales son modelos computacionales inspirados en el funcionamiento del cerebro humano.
        Est√°n compuestas por nodos (neuronas) interconectados que procesan informaci√≥n de manera paralela.

        **Caracter√≠sticas principales:**
        - **Neuronas**: Unidades b√°sicas que reciben entradas, las procesan y generan salidas
        - **Capas**: Organizan las neuronas en estructuras jer√°rquicas (entrada, ocultas, salida)
        - **Pesos y Sesgos**: Par√°metros que se ajustan durante el entrenamiento
        - **Funciones de Activaci√≥n**: Determinan la salida de cada neurona
        - **Backpropagation**: Algoritmo para entrenar la red ajustando pesos y sesgos

        **Ventajas:**
        - Pueden modelar relaciones no lineales complejas
        - Excelentes para reconocimiento de patrones
        - Adaptables a diferentes tipos de problemas
        - Capaces de aproximar cualquier funci√≥n continua

        **Desventajas:**
        - Requieren grandes cantidades de datos
        - Pueden ser "cajas negras" (dif√≠ciles de interpretar)
        - Propensos al sobreajuste
        - Requieren mucho poder computacional
        """)

    # Sistema de pesta√±as
    tab_names = [
        "üìä Datos",
        "üèóÔ∏è Arquitectura",
        "‚öôÔ∏è Entrenamiento",
        "üìà Evaluaci√≥n",
        "üéØ Visualizaci√≥n",
        "üîÆ Predicciones",
        "üíæ Exportar"
    ]

    # Inicializar estado de pesta√±as si no existe
    if 'active_tab_nn' not in st.session_state:
        st.session_state.active_tab_nn = 0

    # Crear pesta√±as visuales personalizadas
    cols = st.columns(len(tab_names))
    for i, (col, tab_name) in enumerate(zip(cols, tab_names)):
        with col:
            if st.button(
                tab_name,
                key=f"tab_nn_{i}",
                use_container_width=True,
                type="primary" if st.session_state.active_tab_nn == i else "secondary"
            ):
                st.session_state.active_tab_nn = i
                st.rerun()

    st.markdown("---")

    ###########################################
    #     Pesta√±a de Datos                    #
    ###########################################
    tips = """
        üéì **Tips para Redes Neuronales:**
        - Las redes neuronales funcionan mejor con **datos normalizados** (valores entre 0 y 1 o -1 y 1)
        - Necesitan **suficientes datos** para entrenar bien (m√≠nimo 100 ejemplos por clase)
        - Son excelentes para **patrones complejos** y **relaciones no lineales**
        - Pueden funcionar tanto para **clasificaci√≥n** como para **regresi√≥n**
        """
    if st.session_state.active_tab_nn == 0:
        st.header("üìä Selecci√≥n y Preparaci√≥n de Datos")
        st.info(tips)
        run_select_dataset()
        run_explore_dataset_tab()

        # Botones de navegaci√≥n
        navbar("active_tab_nn", None, "Continuar a Arquitectura")
    else:
        # Para otras pesta√±as, mostrar qu√© dataset est√° seleccionado actualmente
        if hasattr(st.session_state, 'selected_dataset'):
            st.info(
                f"üìä **Dataset actual:** {st.session_state.selected_dataset}")
            st.markdown("---")

    ###########################################
    #     Pesta√±a de Arquitectura             #
    ###########################################
    if st.session_state.active_tab_nn == 1:
        st.header("üèóÔ∏è Dise√±o de la Arquitectura de la Red")
        st.markdown("### üéõÔ∏è Configuraci√≥n de la Red Neuronal")

        try:
            # Cargar datos usando la funci√≥n load_data com√∫n
            dataset_name = st.session_state.selected_dataset
            X, y, feature_names, class_names, dataset_info, task_type = load_data(
                dataset_name)

            # Crear DataFrame
            df = X  # pd.DataFrame(X, columns=feature_names)

            df = df.reset_index(drop=True)
            y = y.reset_index(drop=True)
            # Determinar el nombre de la columna objetivo
            if task_type == "Clasificaci√≥n":
                # Para clasificaci√≥n, usar el nombre de la variable objetivo del dataset_info
                target_col = 'target'  # Nombre por defecto
                if hasattr(dataset_info, 'target_names'):
                    target_col = 'target'
                df[target_col] = y
            else:
                # Para regresi√≥n
                target_col = 'target'
                df[target_col] = y

            # Almacenar informaci√≥n del dataset
            st.session_state.nn_df = df
            st.session_state.nn_target_col = target_col
            st.session_state.nn_feature_names = feature_names
            st.session_state.nn_class_names = class_names
            st.session_state.nn_task_type = task_type
            st.session_state.nn_dataset_info = dataset_info

        except Exception as e:
            st.error(f"Error al cargar el dataset: {e}")

        # Preparar datos b√°sicos para mostrar dimensiones
        X = df.drop(columns=[target_col])
        y = df[target_col]
        input_size = X.shape[1]

        if task_type == "Clasificaci√≥n":
            num_classes = len(y.unique())
            if num_classes == 2:
                output_size = 1  # Para clasificaci√≥n binaria
                st.info(
                    f"üìä **Entrada**: {input_size} caracter√≠sticas ‚Üí **Salida**: {output_size} neurona (clasificaci√≥n binaria)")
            else:
                output_size = num_classes  # Para clasificaci√≥n multiclase
                st.info(
                    f"üìä **Entrada**: {input_size} caracter√≠sticas ‚Üí **Salida**: {output_size} clases")
        else:
            output_size = 1
            st.info(
                f"üìä **Entrada**: {input_size} caracter√≠sticas ‚Üí **Salida**: {output_size} valor num√©rico")

        # Tips sobre dimensiones
        with st.expander("üí° ¬øC√≥mo decidir el tama√±o de la red?"):
            st.markdown(f"""
            **Reglas generales para tu dataset ({df.shape[0]} muestras, {input_size} caracter√≠sticas):**
            
            üî¢ **Neuronas por capa oculta:**
            - Peque√±o: {input_size//2} - {input_size} neuronas
            - Mediano: {input_size} - {input_size*2} neuronas  
            - Grande: {input_size*2} - {input_size*4} neuronas
            
            üìö **N√∫mero de capas:**
            - 1-2 capas: Problemas simples, linealmente separables
            - 2-3 capas: Problemas moderadamente complejos (recomendado para empezar)
            - 4+ capas: Problemas muy complejos (requiere muchos datos)
            
            ‚öñÔ∏è **Balance capacidad vs. datos:**
            - M√°s par√°metros que datos ‚Üí riesgo de sobreajuste
            - Tu dataset: {df.shape[0]} muestras, mant√©n par√°metros < {df.shape[0]//10}
            """)

        # Configuraci√≥n de arquitectura
        col1, col2 = st.columns([1, 1])

        with col1:
            st.markdown("#### ‚öôÔ∏è Configuraci√≥n de Capas")

            # N√∫mero de capas ocultas con explicaci√≥n
            num_hidden_layers = st.slider(
                "N√∫mero de capas ocultas",
                min_value=1, max_value=5, value=2,
                help="üí° M√°s capas = mayor capacidad de aprender patrones complejos, pero tambi√©n mayor riesgo de sobreajuste"
            )

            # Sugerencia basada en el n√∫mero de capas
            if num_hidden_layers == 1:
                st.caption(
                    "üü¶ **1 capa**: Ideal para problemas linealmente separables")
            elif num_hidden_layers == 2:
                st.caption(
                    "üü® **2 capas**: Recomendado para la mayor√≠a de problemas")
            elif num_hidden_layers >= 3:
                st.caption(
                    "üü• **3+ capas**: Solo para problemas muy complejos con muchos datos")

            # Configuraci√≥n de cada capa oculta con explicaciones
            hidden_layers = []
            for i in range(num_hidden_layers):
                # Calcular sugerencia inteligente
                suggested_size = max(10, input_size // (i+1))
                if task_type == "Clasificaci√≥n" and num_classes > 2:
                    suggested_size = max(suggested_size, num_classes * 2)

                neurons = st.slider(
                    f"Neuronas en capa oculta {i+1}",
                    min_value=1, max_value=256,
                    value=min(suggested_size, 64),  # Limitar valor por defecto
                    key=f"layer_{i}",
                    help=f"üí° Sugerencia para capa {i+1}: {suggested_size} neuronas"
                )
                hidden_layers.append(neurons)

            # Funci√≥n de activaci√≥n con explicaciones detalladas
            st.markdown("#### üßÆ Funci√≥n de Activaci√≥n (Capas Ocultas)")
            activation = st.selectbox(
                "Funci√≥n de activaci√≥n",
                ["relu", "tanh", "sigmoid"],
                help="üí° ReLU es la m√°s popular y efectiva para la mayor√≠a de problemas"
            )

            # Explicaciones sobre funciones de activaci√≥n
            if activation == "relu":
                st.success(
                    "‚úÖ **ReLU**: R√°pida, evita el problema del gradiente que desaparece. Recomendada.")
            elif activation == "tanh":
                st.info(
                    "‚ÑπÔ∏è **Tanh**: Salida entre -1 y 1. Buena para datos normalizados.")
            elif activation == "sigmoid":
                st.warning(
                    "‚ö†Ô∏è **Sigmoid**: Puede causar gradientes que desaparecen. √ösala solo si es necesario.")

            # Funci√≥n de activaci√≥n de salida - AHORA SELECCIONABLE
            st.markdown("#### üéØ Funci√≥n de Activaci√≥n de Salida")

            # Opciones disponibles seg√∫n el tipo de tarea
            if task_type == "Clasificaci√≥n":
                output_options = ["sigmoid", "softmax", "linear", "tanh"]
                if output_size == 1:  # Clasificaci√≥n binaria
                    recommended = "sigmoid"
                    default_index = 0
                else:  # Clasificaci√≥n multiclase
                    recommended = "softmax"
                    default_index = 1
            else:
                output_options = ["linear", "sigmoid", "tanh", "softmax"]
                recommended = "linear"
                default_index = 0

            output_activation = st.selectbox(
                "Funci√≥n de activaci√≥n de salida",
                output_options,
                index=default_index,
                help=f"üí° Funci√≥n recomendada para {task_type.lower()}: **{recommended}**"
            )

            # Validaciones y avisos
            show_warning = False
            warning_message = ""

            if task_type == "Clasificaci√≥n":
                if output_size == 1:  # Clasificaci√≥n binaria
                    if output_activation == "sigmoid":
                        st.success(
                            "‚úÖ Sigmoid es ideal para clasificaci√≥n binaria")
                    elif output_activation == "softmax":
                        show_warning = True
                        warning_message = "‚ö†Ô∏è Softmax no es recomendada para clasificaci√≥n binaria (1 neurona). Considera usar Sigmoid."
                    elif output_activation == "linear":
                        show_warning = True
                        warning_message = "‚ö†Ô∏è Linear puede causar problemas en clasificaci√≥n. Considera usar Sigmoid."
                    elif output_activation == "tanh":
                        show_warning = True
                        warning_message = "‚ö†Ô∏è Tanh puede funcionar pero Sigmoid es m√°s est√°ndar para clasificaci√≥n binaria."

                else:  # Clasificaci√≥n multiclase
                    if output_activation == "softmax":
                        st.success(
                            "‚úÖ Softmax es ideal para clasificaci√≥n multiclase")
                    elif output_activation == "sigmoid":
                        st.warning(
                            "‚ö†Ô∏è Sigmoid en multiclase requiere 'binary_crossentropy' por clase. Softmax es m√°s est√°ndar.")
                    elif output_activation == "linear":
                        show_warning = True
                        warning_message = "‚ö†Ô∏è Linear no es apropiada para clasificaci√≥n. Usa Softmax."
                    elif output_activation == "tanh":
                        show_warning = True
                        warning_message = "‚ö†Ô∏è Tanh no es est√°ndar para clasificaci√≥n multiclase. Softmax es recomendada."

            else:  # Regresi√≥n
                if output_activation == "linear":
                    st.success("‚úÖ Linear es ideal para regresi√≥n")
                elif output_activation == "sigmoid":
                    st.warning(
                        "‚ö†Ô∏è Sigmoid limita la salida a [0,1]. Solo √∫til si tus valores objetivo est√°n en este rango.")
                elif output_activation == "tanh":
                    st.warning(
                        "‚ö†Ô∏è Tanh limita la salida a [-1,1]. Solo √∫til si tus valores objetivo est√°n en este rango.")
                elif output_activation == "softmax":
                    show_warning = True
                    warning_message = "‚ö†Ô∏è Softmax no es apropiada para regresi√≥n. Las salidas suman 1. Usa Linear."

            # Mostrar advertencia cr√≠tica si es necesario
            if show_warning:
                st.error(warning_message)

        with col2:
            st.markdown("#### üé® Visualizaci√≥n de la Arquitectura")

            # Crear arquitectura completa
            architecture = [input_size] + hidden_layers + [output_size]

            # Guardar configuraci√≥n en session state
            st.session_state.nn_architecture = {
                'layers': architecture,
                'activation': activation,
                'output_activation': output_activation,
                'input_size': input_size,
                'output_size': output_size,
                'task_type': task_type
            }

            # Visualizar la red neuronal din√°micamente
            create_neural_network_visualization(
                architecture, activation, output_activation, task_type)

        # Configuraci√≥n adicional con explicaciones detalladas
        st.markdown("### ‚öôÔ∏è Configuraci√≥n Adicional")

        st.markdown("üìö **Par√°metros importantes para el entrenamiento:**")

        col3, col4, col5 = st.columns(3)

        with col3:
            st.markdown("#### üõ°Ô∏è Regularizaci√≥n")
            dropout_rate = st.slider(
                "Tasa de Dropout",
                min_value=0.0, max_value=0.8, value=0.2, step=0.1,
                help="üí° Dropout previene sobreajuste eliminando aleatoriamente neuronas durante entrenamiento"
            )

            # Explicaci√≥n del dropout
            if dropout_rate == 0.0:
                st.caption("üî¥ **Sin Dropout**: Mayor riesgo de sobreajuste")
            elif dropout_rate <= 0.2:
                st.caption("üü¢ **Dropout Ligero**: Bueno para datasets grandes")
            elif dropout_rate <= 0.5:
                st.caption(
                    "üü° **Dropout Moderado**: Recomendado para la mayor√≠a de casos")
            else:
                st.caption(
                    "üü† **Dropout Alto**: Solo para datasets muy peque√±os")

        with col4:
            st.markdown("#### üì¶ Procesamiento")
            batch_size = st.selectbox(
                "Tama√±o de Batch",
                [16, 32, 64, 128, 256],
                index=2,  # 64 por defecto
                help="üí° N√∫mero de muestras procesadas antes de actualizar los pesos"
            )

            # Sugerencias seg√∫n el tama√±o del dataset
            dataset_size = df.shape[0]
            if batch_size >= dataset_size // 4:
                st.caption(
                    "üî¥ **Batch Grande**: Puede ser lento pero m√°s estable")
            elif batch_size >= 32:
                st.caption(
                    "üü¢ **Batch √ìptimo**: Buen balance velocidad/estabilidad")
            else:
                st.caption("üü° **Batch Peque√±o**: M√°s r√°pido pero m√°s ruidoso")

        with col5:
            st.markdown("#### üöÄ Optimizaci√≥n")
            optimizer = st.selectbox(
                "Optimizador",
                ["adam", "sgd", "rmsprop"],
                help="üí° Algoritmo para actualizar los pesos de la red"
            )

            # Explicaciones sobre optimizadores
            if optimizer == "adam":
                st.caption(
                    "üü¢ **Adam**: Adaptativo, recomendado para la mayor√≠a de casos")
            elif optimizer == "sgd":
                st.caption(
                    "üü° **SGD**: Cl√°sico, requiere ajuste fino del learning rate")
            elif optimizer == "rmsprop":
                st.caption(
                    "üü¶ **RMSprop**: Bueno para RNNs y problemas espec√≠ficos")

        # Tips sobre la configuraci√≥n
        with st.expander("üí° Tips para optimizar tu configuraci√≥n"):
            st.markdown(f"""
            **Para tu dataset espec√≠fico ({dataset_size} muestras):**
            
            üéØ **Batch Size recomendado:**
            - Dataset peque√±o (<1000): 16-32
            - Dataset mediano (1000-10000): 32-64
            - Dataset grande (>10000): 64-128
            - Tu dataset: {dataset_size} muestras ‚Üí Recomendado: {32 if dataset_size < 1000 else 64 if dataset_size < 10000 else 128}
            
            üõ°Ô∏è **Dropout recomendado:**
            - Pocos datos: 0.3-0.5 (m√°s regularizaci√≥n)
            - Muchos datos: 0.1-0.2 (menos regularizaci√≥n)
            - Dataset balanceado: 0.2-0.3
            
            üöÄ **Optimizador:**
            - **Adam**: Mejor opci√≥n general, se adapta autom√°ticamente
            - **SGD**: √ösalo solo si tienes experiencia ajustando learning rates
            - **RMSprop**: Alternativa a Adam, a veces funciona mejor en problemas espec√≠ficos
            """)

        # Guardar configuraci√≥n completa
        st.session_state.nn_config = {
            'architecture': architecture,
            'activation': activation,
            'output_activation': output_activation,
            'dropout_rate': dropout_rate,
            'batch_size': batch_size,
            'optimizer': optimizer,
            'task_type': task_type,
            'input_size': input_size,
            'output_size': output_size
        }

        # Resumen de la configuraci√≥n con an√°lisis
        st.markdown("### üìã Resumen de la Arquitectura")

        total_params = calculate_network_parameters(architecture)

        col6, col7, col8 = st.columns(3)
        with col6:
            st.metric("üî¢ Total de Par√°metros", f"{total_params:,}",
                      help="N√∫mero total de pesos y sesgos que la red aprender√°")
        with col7:
            st.metric("üìö Capas Totales", len(architecture),
                      help="Entrada + Ocultas + Salida")
        with col8:
            complexity_ratio = total_params / dataset_size if dataset_size > 0 else 0
            complexity_level = "Baja" if complexity_ratio < 0.1 else "Media" if complexity_ratio < 1 else "Alta"
            st.metric("‚öñÔ∏è Complejidad", complexity_level,
                      help=f"Ratio par√°metros/datos: {complexity_ratio:.2f}")
        with col8:
            st.metric("üß† Tipo de Red", "Perceptr√≥n Multicapa")

        # Mostrar detalles de cada capa
        st.markdown("#### üìä Detalles por Capa")
        layer_details = []
        for i, (current, next_size) in enumerate(zip(architecture[:-1], architecture[1:])):
            if i == 0:
                layer_type = "Entrada"
                params = 0
            elif i == len(architecture) - 2:
                layer_type = "Salida"
                params = current * next_size + next_size
            else:
                layer_type = f"Oculta {i}"
                params = current * next_size + next_size

            layer_details.append({
                "Capa": layer_type,
                "Neuronas": current if i < len(architecture) - 1 else next_size,
                "Par√°metros": params,
                "Activaci√≥n": "Entrada" if i == 0 else (output_activation if i == len(architecture) - 2 else activation)
            })

        st.dataframe(pd.DataFrame(layer_details), use_container_width=True)

        # An√°lisis de complejidad y recomendaciones
        st.markdown("#### üîç An√°lisis de Complejidad")

        # An√°lisis del ratio par√°metros/datos
        if complexity_ratio < 0.1:
            st.success(
                f"‚úÖ **Complejidad √ìptima**: Tu red tiene {total_params:,} par√°metros para {dataset_size} muestras (ratio: {complexity_ratio:.3f}). Bajo riesgo de sobreajuste.")
        elif complexity_ratio < 1:
            st.warning(
                f"‚ö†Ô∏è **Complejidad Media**: Tu red tiene {total_params:,} par√°metros para {dataset_size} muestras (ratio: {complexity_ratio:.3f}). Monitorea el sobreajuste.")
        else:
            st.error(
                f"üö® **Complejidad Alta**: Tu red tiene {total_params:,} par√°metros para {dataset_size} muestras (ratio: {complexity_ratio:.3f}). Alto riesgo de sobreajuste. Considera reducir el tama√±o de la red.")

        # Bot√≥n para generar c√≥digo Python
        st.markdown("### üíª C√≥digo Python")
        if st.button("üìù Generar C√≥digo de la Arquitectura", use_container_width=True):
            # Generar c√≥digo Python para la arquitectura
            code = generate_neural_network_architecture_code(
                architecture, activation, output_activation, dropout_rate,
                optimizer, batch_size, task_type, st.session_state.nn_feature_names
            )

            st.markdown("#### üêç C√≥digo Python Generado")
            st.code(code, language='python')

            # Bot√≥n para descargar el c√≥digo
            st.download_button(
                label="üíæ Descargar C√≥digo Python",
                data=code,
                file_name=f"red_neuronal_arquitectura_{task_type.lower()}.py",
                mime="text/plain"
            )

        # Botones de navegaci√≥n
        navbar("active_tab_nn", "Volver a Datos", "Continuar a Entrenamiento",
               next_note="**¬øListo para entrenar?** ¬°Tu arquitectura est√° configurada!")

    ###########################################
    #     Pesta√±a de Entrenamiento            #
    ###########################################
    elif st.session_state.active_tab_nn == 2:
        st.header("‚öôÔ∏è Entrenamiento de la Red Neuronal")

        if 'nn_config' not in st.session_state:
            st.warning("‚ö†Ô∏è Primero debes configurar la arquitectura de la red.")
            if st.button("üîô Ir a Arquitectura"):
                st.session_state.active_tab_nn = 1
                st.rerun()
            return

        # Tips educativos sobre entrenamiento
        st.info("""
        üéì **Conceptos Clave del Entrenamiento:**
        - **Learning Rate**: Controla qu√© tan r√°pido aprende la red (muy alto = inestable, muy bajo = lento)
        - **√âpocas**: Cu√°ntas veces la red ve todos los datos (m√°s √©pocas ‚â† siempre mejor)
        - **Validaci√≥n**: Datos separados para monitorear si la red est√° generalizando bien
        - **Early Stopping**: Para evitar sobreajuste, para cuando la validaci√≥n no mejora
        """)

        st.markdown("### üéõÔ∏è Par√°metros de Entrenamiento")

        # Informaci√≥n del dataset para sugerencias
        df = st.session_state.nn_df
        dataset_size = df.shape[0]

        col1, col2, col3 = st.columns(3)

        with col1:
            st.markdown("#### üìà **Learning Rate**")
            learning_rate = st.selectbox(
                "Tasa de Aprendizaje",
                [0.001, 0.01, 0.1, 0.3],
                index=0,
                help="üí° 0.001 es seguro para empezar. Valores m√°s altos pueden acelerar el entrenamiento pero causar inestabilidad"
            )

            # Explicaci√≥n del learning rate seleccionado
            if learning_rate == 0.001:
                st.caption("üü¢ **Conservador**: Aprendizaje lento pero estable")
            elif learning_rate == 0.01:
                st.caption(
                    "üü° **Moderado**: Buen balance velocidad/estabilidad")
            elif learning_rate == 0.1:
                st.caption("üü† **Agresivo**: R√°pido pero puede ser inestable")
            else:
                st.caption("üî¥ **Muy Alto**: Solo para casos especiales")

        with col2:
            st.markdown("#### üîÑ **√âpocas**")
            # Sugerir √©pocas basado en tama√±o del dataset
            suggested_epochs = min(200, max(50, dataset_size // 10))
            epochs = st.slider(
                "√âpocas",
                min_value=10, max_value=500, value=min(100, suggested_epochs), step=10,
                help=f"üí° Sugerencia para tu dataset: ~{suggested_epochs} √©pocas"
            )

            # Explicaci√≥n sobre las √©pocas
            if epochs < 50:
                st.caption(
                    "üü° **Pocas √©pocas**: Puede que no aprenda completamente")
            elif epochs <= 150:
                st.caption("üü¢ **√âpocas adecuadas**: Buen balance")
            else:
                st.caption("üü† **Muchas √©pocas**: Monitorea el sobreajuste")

        with col3:
            st.markdown("#### üéØ **Validaci√≥n**")
            validation_split = st.slider(
                "% Datos de Validaci√≥n",
                min_value=10, max_value=40, value=20,
                help="üí° 20% es est√°ndar. M√°s datos = mejor validaci√≥n, menos datos para entrenar"
            )

            # Calcular tama√±os efectivos
            # 80% del total para entrenamiento
            train_size = int(
                dataset_size * (100 - validation_split) / 100 * 0.8)
            val_size = int(dataset_size * validation_split / 100)
            test_size = dataset_size - train_size - val_size

            st.caption(
                f"üìä **Distribuci√≥n**: Train={train_size}, Val={val_size}, Test={test_size}")

        # Configuraci√≥n avanzada con explicaciones detalladas
        with st.expander("‚öôÔ∏è Configuraci√≥n Avanzada - T√©cnicas para Mejorar el Entrenamiento", expanded=False):
            st.markdown("#### üõ°Ô∏è T√©cnicas de Regularizaci√≥n y Optimizaci√≥n")

            col4, col5 = st.columns(2)

            with col4:
                st.markdown("##### üõë Early Stopping")
                early_stopping = st.checkbox(
                    "Activar Parada Temprana",
                    value=True,
                    help="üí° Recomendado: Evita sobreajuste parando cuando la validaci√≥n no mejora"
                )

                if early_stopping:
                    st.success(
                        "‚úÖ **Early Stopping activado**: La red parar√° autom√°ticamente cuando deje de mejorar")
                    patience = st.slider(
                        "Paciencia (√©pocas)",
                        min_value=5, max_value=50, value=10,
                        help="√âpocas a esperar sin mejora antes de parar. M√°s paciencia = m√°s oportunidades de mejorar"
                    )

                    if patience <= 5:
                        st.caption(
                            "üî¥ **Impatiente**: Para r√°pido, puede interrumpir mejoras tard√≠as")
                    elif patience <= 15:
                        st.caption("üü¢ **Balanceado**: Buen equilibrio")
                    else:
                        st.caption(
                            "üü° **Paciente**: Da muchas oportunidades, pero puede sobreajustar")
                else:
                    st.warning(
                        "‚ö†Ô∏è **Sin Early Stopping**: La red entrenar√° todas las √©pocas. Riesgo de sobreajuste.")

            with col5:
                st.markdown("##### üìâ Learning Rate Scheduler")
                reduce_lr = st.checkbox(
                    "Reducir Learning Rate Autom√°ticamente",
                    value=True,
                    help="üí° Recomendado: Reduce la tasa de aprendizaje cuando no mejora"
                )

                if reduce_lr:
                    st.success(
                        "‚úÖ **Scheduler activado**: La tasa de aprendizaje se reducir√° autom√°ticamente")
                    lr_factor = st.slider(
                        "Factor de Reducci√≥n",
                        min_value=0.1, max_value=0.9, value=0.5,
                        help="Factor por el que se multiplica la tasa. 0.5 = reduce a la mitad"
                    )

                    if lr_factor <= 0.3:
                        st.caption(
                            "üî¥ **Reducci√≥n agresiva**: Cambios dram√°ticos")
                    elif lr_factor <= 0.7:
                        st.caption("üü¢ **Reducci√≥n moderada**: Recomendado")
                    else:
                        st.caption("üü° **Reducci√≥n suave**: Cambios graduales")
                else:
                    st.info(
                        "‚ÑπÔ∏è **Learning rate fijo**: Se mantendr√° constante durante todo el entrenamiento")

            # Explicaci√≥n sobre las t√©cnicas
            st.markdown("---")
            st.markdown("#### üìö ¬øPor qu√© usar estas t√©cnicas?")
            st.markdown("""
            - **Early Stopping**: Evita que la red memorice los datos (sobreajuste) parando cuando la performance en validaci√≥n deja de mejorar
            - **Learning Rate Reduction**: Permite un ajuste fino hacia el final del entrenamiento cuando se est√° cerca del √≥ptimo
            - **Combinadas**: Estas t√©cnicas trabajan juntas para lograr el mejor modelo posible autom√°ticamente
            """)

        # Bot√≥n de entrenamiento con explicaci√≥n
        st.markdown("### üöÄ Iniciar Entrenamiento")
        st.markdown(
            "**¬øTodo listo?** Tu red est√° configurada y lista para aprender de los datos.")

        if st.button("üß† Entrenar Red Neuronal", type="primary", use_container_width=True):
            with st.spinner("üß† Entrenando la red neuronal... Esto puede tomar unos minutos."):
                # Preparar datos
                df = st.session_state.nn_df
                target_col = st.session_state.nn_target_col
                task_type = st.session_state.nn_task_type

                try:
                    # Mostrar progreso del entrenamiento con pasos
                    progress_container = st.empty()

                    # Paso 1: Preparando datos
                    with progress_container.container():
                        st.info(
                            "üîÑ **Paso 1/4**: Preparando y dividiendo los datos...")

                    # Llamar funci√≥n de entrenamiento con callback de progreso
                    def update_progress(step, message):
                        with progress_container.container():
                            if step == 2:
                                st.info(f"üß† **Paso {step}/4**: {message}")
                            elif step == 3:
                                st.info(f"‚öôÔ∏è **Paso {step}/4**: {message}")
                            elif step == 4:
                                st.info(f"üöÄ **Paso {step}/4**: {message}")
                            else:
                                st.info(f"üîÑ **Paso {step}/4**: {message}")

                    # Entrenar el modelo con callback de progreso
                    model, history, X_test, y_test, scaler, label_encoder = train_neural_network(
                        df, target_col, st.session_state.nn_config,
                        learning_rate, epochs, validation_split/100,
                        early_stopping, patience if early_stopping else None,
                        reduce_lr, lr_factor if reduce_lr else None,
                        progress_callback=update_progress
                    )

                    # Guardar resultados
                    st.session_state.nn_model = model
                    st.session_state.nn_history = history
                    st.session_state.nn_test_data = (X_test, y_test)
                    st.session_state.nn_scaler = scaler
                    st.session_state.nn_label_encoder = label_encoder
                    st.session_state.model_trained_nn = True

                    # Limpiar el progreso y mostrar finalizaci√≥n
                    with progress_container.container():
                        if model is not None:
                            st.success(
                                "‚úÖ **¬°Entrenamiento completado!** Red neuronal lista para usar")

                    if model is not None:
                        st.success("üéâ ¬°Red neuronal entrenada exitosamente!")

                except Exception as e:
                    # Limpiar el progreso en caso de error
                    with progress_container.container():
                        st.error("‚ùå **Error durante el entrenamiento**")

                    st.error(f"‚ùå Error durante el entrenamiento: {str(e)}")
                    st.info(
                        "Intenta ajustar los par√°metros o verificar el dataset.")

        # Botones de navegaci√≥n
        if st.session_state.get('model_trained_nn', False):
            navbar("active_tab_nn", "Volver a Arquitectura", "Ver Evaluaci√≥n")
        else:
            navbar("active_tab_nn", "Volver a Arquitectura", None)

    ###########################################
    #     Pesta√±a de Evaluaci√≥n.              #
    ###########################################
    elif st.session_state.active_tab_nn == 3:
        st.header("üìà Evaluaci√≥n del Modelo")
        if not st.session_state.get('model_trained_nn', False):
            st.warning("‚ö†Ô∏è Primero debes entrenar un modelo.")
            if st.button("üîô Ir a Entrenamiento"):
                st.session_state.active_tab_nn = 2
                st.rerun()
        else:
            model = st.session_state.nn_model
            X_test = st.session_state.nn_test_data[0]
            y_test = st.session_state.nn_test_data[1]
            class_names = st.session_state.nn_class_names
            task_type = st.session_state.nn_config.get(
                'task_type', 'Clasificaci√≥n')
            # === Predicciones y preparaci√≥n de etiquetas para evaluaci√≥n ===
            raw_pred = model.predict(X_test, verbose=0)

            if task_type == "Clasificaci√≥n":
                # Determinar formato de y_test (one-hot vs etiquetas)
                y_test_arr = np.asarray(y_test)
                one_hot = (y_test_arr.ndim > 1 and y_test_arr.shape[1] > 1)

                # Procesar predicciones seg√∫n forma de salida del modelo
                if raw_pred.ndim > 1 and raw_pred.shape[1] > 1:
                    # Salida multiclase (softmax / linear / tanh etc.)
                    y_pred_classes = np.argmax(raw_pred, axis=1)
                else:
                    # Salida binaria (1 neurona, normalmente sigmoid)
                    y_pred_classes = (raw_pred.ravel() > 0.5).astype(int)

                # Procesar etiquetas reales
                if one_hot:
                    y_test_classes = np.argmax(y_test_arr, axis=1)
                else:
                    # Ya son etiquetas enteras (binaria o sparse multiclase)
                    y_test_classes = y_test_arr.ravel()

                # Guardar para otras pesta√±as
                st.session_state.nn_y_pred = y_pred_classes

                # Primero evaluar con el formato original (para model.evaluate)
                evaluate_nn(model, X_test, y_test, task_type)
                # Mostrar evaluaci√≥n detallada con clases procesadas
                show_detailed_evaluation(
                    y_test_classes, y_pred_classes, class_names, task_type)
            else:
                # Regresi√≥n
                y_pred_reg = raw_pred.ravel()
                st.session_state.nn_y_pred = y_pred_reg
                evaluate_nn(model, X_test, y_test, task_type)
                show_detailed_evaluation(
                    y_test, y_pred_reg, class_names, task_type)

            history = st.session_state.nn_history
            config = st.session_state.nn_config
            neural_network_diagnostics(history, config)

            navbar("active_tab_nn", "Volver a Entrenamiento", "Ver Visualizaci√≥n")

    ###########################################
    #     Pesta√±a de Visualizaci√≥n            #
    ###########################################
    elif st.session_state.active_tab_nn == 4:
        st.header("üéØ Visualizaciones")
        if not st.session_state.get('model_trained_nn', False):
            st.warning("‚ö†Ô∏è Primero debes entrenar un modelo.")
        else:
            st.info("""
            üéì **Visualizaciones de Redes Neuronales:**
            - **Historial**: Evoluci√≥n del aprendizaje 
            - **Pesos**: Lo que aprendi√≥ cada neurona
            - **Superficie**: C√≥mo separa las clases (2D) 
            - **Capas**: Activaciones internas
            """)

            # Usar botones para seleccionar el tipo de visualizaci√≥n
            viz_options = [
                ("üìä Historial", "Historial", "viz_history"),
                ("üß† An√°lisis de Pesos", "Pesos", "viz_weights"),
                ("üéØ Superficie de Decisi√≥n", "Superficie", "viz_surface"),
                ("üìâ An√°lisis de Activaciones", "Activaciones", "viz_activations"),
            ]

            viz_type = create_button_panel(viz_options)

            if viz_type == "Historial":
                show_training_history_tab()
            elif viz_type == "Pesos":
                show_weights_analysis_tab()
            elif viz_type == "Superficie":
                show_decision_surface_tab()
            elif viz_type == "Activaciones":
                show_layer_activations_tab()

        navbar("active_tab_nn", "Volver a Evaluaci√≥n", "Ver Predicciones")

    ###########################################
    #     Pesta√±a de Predicciones             #
    ###########################################
    elif st.session_state.active_tab_nn == 5:
        st.header("üîÆ Predicciones")
        if not st.session_state.get('model_trained_nn', False):
            st.warning("‚ö†Ô∏è Primero debes entrenar un modelo.")
        else:
            # Usar los datos ORIGINALES (antes de escalar) para preservar tipos/categor√≠as.
            # nn_test_data[0] es un array escalado que pierde informaci√≥n categ√≥rica y provoca sliders.
            try:
                if 'nn_df' in st.session_state and 'nn_target_col' in st.session_state:
                    X_original_for_ui = st.session_state.nn_df.drop(
                        columns=[st.session_state.nn_target_col])
                else:
                    # Fallback: si algo falla, usar el array test (mantiene comportamiento previo)
                    X_original_for_ui = st.session_state.nn_test_data[0]
            except Exception:
                X_original_for_ui = st.session_state.nn_test_data[0]

            create_prediction_interface(
                st.session_state.nn_model,
                st.session_state.nn_feature_names,
                st.session_state.nn_class_names,
                st.session_state.get('nn_task_type', 'Clasificaci√≥n'),
                # Pasar datos originales para que el tipo de feature se detecte correctamente
                X_original_for_ui,
                # Pasar nombre del dataset para metadata
                st.session_state.get('selected_dataset', 'Titanic')
            )

    ###########################################
    #     Pesta√±a de Exportar.                #
    ###########################################
    elif st.session_state.active_tab_nn == 6:
        st.header("üíæ Exportar Modelo")
        if not st.session_state.get('model_trained_nn', False):
            st.warning("‚ö†Ô∏è Primero debes entrenar un modelo.")
        else:
            show_neural_network_export()


# ===== FUNCIONES PARA REDES NEURONALES =====
