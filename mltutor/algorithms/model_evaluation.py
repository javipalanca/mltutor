"""
Este m√≥dulo contiene funciones para evaluar modelos de √°rboles de decisi√≥n.
Incluye funciones para calcular m√©tricas y visualizar resultados de clasificaci√≥n y regresi√≥n.
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, mean_squared_error, confusion_matrix,
    classification_report, precision_recall_fscore_support,
    r2_score, mean_absolute_error

)

from utils import get_image_download_link, show_code_with_download
from algorithms.code_examples import (
    CONFUSION_MATRIX_CODE,
    PRECISION_CODE,
    PRED_VS_REAL_CODE,
    ERROR_DISTRIBUTION_CODE,
    DECISION_PATH_CODE,
)


def evaluate_classification_model(y_test, y_pred, class_names):
    """
    Eval√∫a un modelo de clasificaci√≥n y devuelve las m√©tricas principales.

    Parameters:
    -----------
    y_test : array
        Valores reales
    y_pred : array
        Valores predichos
    class_names : list
        Nombres de las clases

    Returns:
    --------
    dict
        Diccionario con las m√©tricas del modelo
    """
    # Calcular m√©tricas
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(
        y_test, y_pred, target_names=class_names, output_dict=True)
    cm = confusion_matrix(y_test, y_pred)

    # Extraer precision y recall promedio ponderado para compatibilidad
    precision = report.get('weighted avg', {}).get('precision', 0.0)
    recall = report.get('weighted avg', {}).get('recall', 0.0)

    return {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "report": report,
        "confusion_matrix": cm,
        "y_pred": y_pred
    }


def evaluate_regression_model(y_test, y_pred):
    """
    Eval√∫a un modelo de regresi√≥n y devuelve las m√©tricas principales.

    Parameters:
    -----------
    y_test : array
        Valores reales
    y_pred : array
        Valores predichos

    Returns:
    --------
    dict
        Diccionario con las m√©tricas del modelo
    """
    # Calcular m√©tricas
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    # np.mean(np.abs(y_test - y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)  # 1 - np.sum((y_test - y_pred)**2) / \
    # np.sum((y_test - np.mean(y_test))**2)

    return {
        "mse": mse,
        "rmse": rmse,
        "mae": mae,
        "r2": r2,
        "y_pred": y_pred
    }


def show_detailed_evaluation(y_test, y_pred, class_names, model_type):
    """
    Muestra una evaluaci√≥n detallada del modelo con gr√°ficos y explicaciones.

    Parameters:
    -----------
    y_test : array
        Valores reales
    y_pred : array
        Valores predichos
    class_names : list
        Nombres de las clases
    model_type : str
        Tipo de modelo ('Clasificaci√≥n' o 'Regresi√≥n')
    """
    if model_type == "Clasificaci√≥n":
        # Preparar etiquetas y nombres de clase de forma robusta
        if class_names is None:
            labels = sorted(np.unique(y_test))
            target_names = [str(l) for l in labels]
        else:
            # Si class_names est√° presente, asumir que las clases son 0..n-1
            try:
                labels = list(range(len(class_names)))
                target_names = [str(n) for n in class_names]
            except Exception:
                labels = sorted(np.unique(y_test))
                target_names = [str(l) for l in labels]

        # Calcular m√©tricas usando labels y target_names
        report = classification_report(
            y_test, y_pred, labels=labels, target_names=target_names, output_dict=True)
        report_df = pd.DataFrame(report).transpose()

        # M√©tricas globales
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            acc_value = report.get('accuracy', 0.0)
            st.metric("Exactitud (Accuracy)", f"{acc_value:.4f}",
                      help="Proporci√≥n total de predicciones correctas")
            # Indicador de calidad
            if acc_value >= 0.9:
                st.success("Excelente", icon="üåü")
            elif acc_value >= 0.8:
                st.info("Muy bueno", icon="üëç")
            elif acc_value >= 0.7:
                st.warning("Bueno", icon="‚ö†Ô∏è")
            else:
                st.warning("Necesita mejora", icon="‚ùå")
        with col2:
            st.metric("Precisi√≥n media", f"{report.get('weighted avg', {}).get('precision', 0.0):.4f}",
                      help="Media ponderada de la precisi√≥n de cada clase")
        with col3:
            st.metric("Exhaustividad media (Recall)", f"{report.get('weighted avg', {}).get('recall', 0.0):.4f}",
                      help="Media ponderada de la exhaustividad de cada clase")
        with col4:
            st.metric("F1-Score medio", f"{report.get('weighted avg', {}).get('f1-score', 0.0):.4f}",
                      help="Media arm√≥nica de precisi√≥n y exhaustividad")
            f1_score = report.get('weighted avg', {}).get('f1-score', 0.0)
            if f1_score >= 0.8:
                st.success("üåü Excelente balance")
            elif f1_score >= 0.7:
                st.info("üëç Buen balance")
            else:
                st.warning("‚ö†Ô∏è Balance mejorable")

        # M√©tricas por clase
        st.markdown("### ‚öñÔ∏è Matriz de Confusi√≥n")

        # Excluir filas avg y accuracy del dataframe
        report_by_class = report_df.drop(
            ['accuracy', 'macro avg', 'weighted avg'], errors='ignore')

        # Matriz de confusi√≥n
        cm = confusion_matrix(y_test, y_pred, labels=labels)
        fig_cm, ax_cm = plt.subplots(figsize=(6, 5))  # Reducir tama√±o
        xticks = target_names if target_names is not None else None
        yticks = target_names if target_names is not None else None
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax_cm,
                    xticklabels=xticks, yticklabels=yticks)
        ax_cm.set_xlabel('Predicci√≥n')
        ax_cm.set_ylabel('Real')
        ax_cm.set_title('Matriz de Confusi√≥n')

        col1, col2 = st.columns([1, 1])
        with col1:
            # Mostrar con tama√±o reducido
            col_inner1, col_inner2, col_inner3 = st.columns([1, 3, 1])
            with col_inner2:
                st.pyplot(fig_cm, use_container_width=True)

            st.markdown(get_image_download_link(
                fig_cm, "matriz_confusion", "üì• Descargar matriz de confusi√≥n"), unsafe_allow_html=True)

            # Mostrar c√≥digo para generar la matriz de confusi√≥n
            code_cm = CONFUSION_MATRIX_CODE
            show_code_with_download(
                code_cm, "C√≥digo para generar la matriz de confusi√≥n", "matriz_confusion.py")

        with col2:
            st.dataframe(report_by_class.style.format({
                'precision': '{:.4f}',
                'recall': '{:.4f}',
                'f1-score': '{:.4f}',
                'support': '{:.0f}'
            }))

            with st.expander("üìä Explicaci√≥n de m√©tricas"):
                st.markdown("""
                 **Accuracy (Exactitud):**
                    - Porcentaje de predicciones correctas del total
                    - Rango: 0 a 1 (0% a 100%)
                    - **Interpretaci√≥n:** Valores m√°s altos = mejor modelo
                    - **Cuidado:** Puede ser enga√±osa con clases desbalanceadas
                    
                    **Precision (Precisi√≥n):**
                    - De todas las predicciones positivas, cu√°ntas fueron correctas
                    - F√≥rmula: VP / (VP + FP)
                    - Importante cuando los falsos positivos son costosos
                    - **Interpretaci√≥n:** Valores m√°s altos = mejor modelo
                    
                    **Recall (Sensibilidad o Exhaustividad):**
                    - De todos los casos positivos reales, cu√°ntos detect√≥ el modelo
                    - F√≥rmula: VP / (VP + FN)
                    - Importante cuando los falsos negativos son costosos
                    - **Interpretaci√≥n:** Valores m√°s altos = mejor modelo
                    
                    **F1-Score:**
                    - Media arm√≥nica entre precisi√≥n y recall
                    - F√≥rmula: 2 √ó (Precisi√≥n √ó Recall) / (Precisi√≥n + Recall)
                    - √ötil cuando necesitas balance entre precisi√≥n y recall
                    - **Interpretaci√≥n:** Valores m√°s altos = mejor balance
                    
                    **Curva ROC:**
                    - Muestra el rendimiento en diferentes umbrales de decisi√≥n
                    - AUC (√Årea bajo la curva): 0.5 = aleatorio, 1.0 = perfecto
                    
                    **Curva Precision-Recall:**
                    - Especialmente √∫til para clases desbalanceadas
                    - Muestra el trade-off entre precisi√≥n y recall
                    
                    **VP = Verdaderos Positivos, FP = Falsos Positivos, FN = Falsos Negativos**
                """)

        # Visualizaci√≥n avanzada - Predicciones correctas e incorrectas
        st.markdown("### Visualizaci√≥n de Predicciones")

        # Construir mapping seguro label -> name
        label_to_name = {labels[i]: target_names[i]
                         for i in range(len(labels))}

        # Crear dataframe con resultados
        results_df = pd.DataFrame({
            'Real': [label_to_name.get(v, str(v)) for v in y_test],
            'Predicci√≥n': [label_to_name.get(v, str(v)) for v in y_pred],
            'Correcto': (np.array(y_test) == np.array(y_pred))
        })

        # Mostrar algunas muestras
        col_viz1, col_viz2 = st.columns(2)
        with col_viz1:
            st.markdown("#### Muestras correctamente clasificadas")
            correct_samples = results_df[results_df['Correcto']].head(10)
            st.dataframe(correct_samples, height=300)

        with col_viz2:
            st.markdown("#### Muestras incorrectamente clasificadas")
            incorrect_samples = results_df[~results_df['Correcto']].head(10)
            if len(incorrect_samples) > 0:
                st.dataframe(incorrect_samples, height=300)
            else:
                st.info("¬°Todas las muestras fueron clasificadas correctamente!")

        # Gr√°fico de precisi√≥n por clase
        fig_prec, ax_prec = plt.subplots(figsize=(8, 4))  # Reducir altura
        prec_by_class = {}
        for name in target_names:
            prec_by_class[name] = report.get(name, {}).get('precision', 0.0)
        sns.barplot(x=list(prec_by_class.keys()), y=list(
            prec_by_class.values()), ax=ax_prec)
        ax_prec.set_ylim(0, 1)
        ax_prec.set_title('Precisi√≥n por clase')
        ax_prec.set_ylabel('Precisi√≥n')
        ax_prec.set_xlabel('Clase')

        # Mostrar con tama√±o reducido
        col1, col2, col3 = st.columns([1, 3, 1])
        with col2:
            st.pyplot(fig_prec, use_container_width=True)

        st.markdown(get_image_download_link(
            fig_prec, "precision_por_clase", "üì• Descargar gr√°fico de precisi√≥n"), unsafe_allow_html=True)

        # Mostrar c√≥digo para generar el gr√°fico de precisi√≥n por clase
        code_prec = PRECISION_CODE
        show_code_with_download(
            code_prec, "C√≥digo para generar el gr√°fico de precisi√≥n", "precision_por_clase.py")
    else:
        # M√©tricas para regresi√≥n
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Error Cuadr√°tico Medio (MSE)", f"{mse:.4f}",
                      help="Promedio de los errores al cuadrado. Penaliza m√°s los errores grandes.")
        with col2:
            st.metric("RMSE", f"{rmse:.4f}",
                      help="Ra√≠z cuadrada del MSE. En las mismas unidades que la variable objetivo.")
        with col3:
            st.metric("Error Absoluto Medio (MAE)", f"{mae:.4f}",
                      help="Promedio de los errores absolutos. Menos sensible a valores at√≠picos.")
        with col4:
            st.metric("R¬≤ Score", f"{r2:.4f}",
                      help="Proporci√≥n de la varianza explicada por el modelo. 1 es predicci√≥n perfecta.")

        # Gr√°fico de predicciones vs valores reales
        fig, ax = plt.subplots(figsize=(6, 5))  # Reducir tama√±o
        scatter = ax.scatter(y_test, y_pred, alpha=0.5,
                             c=np.abs(y_test - y_pred), cmap='viridis')
        ax.plot([y_test.min(), y_test.max()], [
                y_test.min(), y_test.max()], 'r--')
        ax.set_xlabel('Valores reales')
        ax.set_ylabel('Predicciones')
        ax.set_title('Predicciones vs Valores reales')
        plt.colorbar(scatter, ax=ax, label='Error absoluto')

        # Mostrar con tama√±o reducido
        col1, col2, col3 = st.columns([1, 3, 1])
        with col2:
            st.pyplot(fig, use_container_width=True)

        st.markdown(get_image_download_link(
            fig, "predicciones_vs_reales", "üì• Descargar gr√°fico"), unsafe_allow_html=True)

        # Mostrar el c√≥digo para generar el gr√°fico de predicciones vs valores reales
        code_pred = PRED_VS_REAL_CODE
        show_code_with_download(
            code_pred, "C√≥digo para generar este gr√°fico", "predicciones_vs_reales.py")

        # Distribuci√≥n de errores
        fig_err, ax_err = plt.subplots(figsize=(6, 4))  # Reducir tama√±o
        errors = y_test - y_pred
        sns.histplot(errors, kde=True, ax=ax_err)
        ax_err.axvline(x=0, color='r', linestyle='--')
        ax_err.set_title('Distribuci√≥n de errores')
        ax_err.set_xlabel('Error (Real - Predicci√≥n)')

        # Mostrar con tama√±o reducido
        col1, col2, col3 = st.columns([1, 3, 1])
        with col2:
            st.pyplot(fig_err, use_container_width=True)

        st.markdown(get_image_download_link(
            fig_err, "distribucion_errores", "üì• Descargar gr√°fico"), unsafe_allow_html=True)

        # Mostrar el c√≥digo para generar el gr√°fico de distribuci√≥n de errores
        code_err = ERROR_DISTRIBUTION_CODE
        show_code_with_download(
            code_err, "C√≥digo para generar este gr√°fico", "distribucion_errores.py")

        with st.expander("üìä Explicaci√≥n de m√©tricas de regresi√≥n"):
            st.markdown("""
            **R¬≤ Score (Coeficiente de Determinaci√≥n):**
            - Mide qu√© tan bien el modelo explica la variabilidad de los datos
            - Indica qu√© proporci√≥n de la varianza en la variable dependiente es predecible. 1 es predicci√≥n perfecta, 0 significa que el modelo no es mejor que predecir la media.
            - Rango: 0 a 1 (valores negativos indican un modelo muy malo)
            - **Interpretaci√≥n:**
                - R¬≤ = 1.0: El modelo explica perfectamente toda la variabilidad
                - R¬≤ = 0.8: El modelo explica el 80% de la variabilidad (muy bueno)
                - R¬≤ = 0.5: El modelo explica el 50% de la variabilidad (moderado)
                - R¬≤ = 0.0: El modelo no explica nada de la variabilidad
            
            **MAE (Error Absoluto Medio):**
            - Promedio de las diferencias absolutas entre valores reales y predichos
            - Se expresa en las mismas unidades que la variable objetivo
            - **Interpretaci√≥n:** Valores m√°s bajos = mejor modelo
            
            **RMSE (Ra√≠z del Error Cuadr√°tico Medio):**
            - Similar al MAE pero penaliza m√°s los errores grandes
            - Se expresa en las mismas unidades que la variable objetivo
            - **Interpretaci√≥n:** Valores m√°s bajos = mejor modelo
            """)

    # Mostrar interpretaci√≥n contextual
    st.markdown("### üîç Interpretaci√≥n de Resultados")

    if model_type != "Clasificaci√≥n":
        # M√©tricas para regresi√≥n
        mse = mean_squared_error(y_test, y_pred)
        rmse_value = np.sqrt(mse)
        mae_value = mean_absolute_error(y_test, y_pred)
        r2_value = r2_score(y_test, y_pred)

        interpretation = "**Resumen del Modelo:**\n"
        interpretation += f"- Tu modelo de regresi√≥n lineal explica **{r2_value*100:.1f}%** de la variabilidad en los datos\n"
        interpretation += f"- En promedio, las predicciones se desv√≠an **{mae_value:.2f} unidades** del valor real (MAE)\n"
        interpretation += f"- La ra√≠z del error cuadr√°tico medio es **{rmse_value:.2f} unidades** (RMSE)\n"

        if rmse_value > mae_value * 1.5:
            interpretation += "\n- ‚ö†Ô∏è El RMSE es significativamente mayor que el MAE, lo que indica la presencia de algunos errores grandes"

        st.info(interpretation)

    else:
        acc_value = report["accuracy"]
        prec_value = report['weighted avg']["precision"]
        rec_value = report['weighted avg']["recall"]
        f1_value = report['weighted avg']['f1-score']

        interpretation = "**Resumen del Modelo:**\n"
        interpretation += f"- Tu modelo clasifica correctamente **{acc_value*100:.1f}%** de los casos\n"
        interpretation += f"- De las predicciones positivas, **{prec_value*100:.1f}%** son correctas (Precisi√≥n)\n"
        interpretation += f"- Detecta **{rec_value*100:.1f}%** de todos los casos positivos reales (Recall)\n"

        if f1_value > 0:
            interpretation += f"- El F1-Score (balance entre precisi√≥n y recall) es **{f1_value:.3f}**\n\n"

        # An√°lisis de balance entre precisi√≥n y recall
        if abs(prec_value - rec_value) > 0.1:
            if prec_value > rec_value:
                interpretation += "- ‚öñÔ∏è **Balance:** El modelo es m√°s preciso pero menos sensible (m√°s conservador)\n"
                interpretation += "- üí° **Sugerencia:** Si es importante detectar todos los casos positivos, considera ajustar el umbral de decisi√≥n\n\n"
            else:
                interpretation += "- ‚öñÔ∏è **Balance:** El modelo es m√°s sensible pero menos preciso (m√°s liberal)\n"
                interpretation += "- üí° **Sugerencia:** Si es importante evitar falsos positivos, considera ajustar el umbral de decisi√≥n\n\n"
        else:
            interpretation += "- ‚öñÔ∏è **Balance:** Buen equilibrio entre precisi√≥n y recall\n\n"

        # An√°lisis espec√≠fico del accuracy
        if acc_value < 0.6:
            interpretation += "üîç **Recomendaciones para mejorar:**\n"
            interpretation += "- Revisar la calidad y cantidad de datos de entrenamiento\n"
            interpretation += "- Considerar ingenier√≠a de caracter√≠sticas adicionales\n"
            interpretation += "- Probar diferentes algoritmos de clasificaci√≥n\n"
            interpretation += "- Verificar si hay desbalance de clases\n"

        st.info(interpretation)


def show_prediction_path(tree_model, X_new, feature_names, class_names=None):
    """
    Muestra el camino de decisi√≥n para una predicci√≥n espec√≠fica.

    Parameters:
    -----------
    tree_model : DecisionTreeClassifier o DecisionTreeRegressor
        Modelo entrenado
    X_new : array
        Ejemplo para predecir
    feature_names : list
        Nombres de las caracter√≠sticas
    class_names : list, optional
        Nombres de las clases (solo para clasificaci√≥n)
    """
    # Convertir a numpy array si no lo es ya
    X_new = np.asarray(X_new)

    # Asegurarse de que X_new tenga el formato correcto (2D array con un solo ejemplo)
    if X_new.ndim > 2:
        # Si tiene m√°s de 2 dimensiones, aplanamos a 2D
        X_new = X_new.reshape(1, -1)
    elif X_new.ndim == 1:
        # Si es un vector 1D, lo convertimos a 2D
        X_new = X_new.reshape(1, -1)

    # Obtener informaci√≥n del √°rbol
    feature_idx = tree_model.tree_.feature
    threshold = tree_model.tree_.threshold

    try:
        # Construir el camino de decisi√≥n
        node_indicator = tree_model.decision_path(X_new)
        leaf_id = tree_model.apply(X_new)

        # Obtener los nodos en el camino
        node_index = node_indicator.indices[node_indicator.indptr[0]
            :node_indicator.indptr[1]]

        path_explanation = []
        for node_id in node_index:
            # Detener si es un nodo hoja
            if leaf_id[0] == node_id:
                continue

            # Obtener la caracter√≠stica y el umbral de la decisi√≥n
            feature_id = feature_idx[node_id]
            feature_name = feature_names[feature_id]
            threshold_value = threshold[node_id]

            # Comprobar si la muestra va por la izquierda o derecha
            # Usamos X_new[0, feature_id] porque X_new es un array 2D con un solo ejemplo
            if X_new[0, feature_id] <= threshold_value:
                path_explanation.append(
                    f"- {feature_name} = {X_new[0, feature_id]:.4f} ‚â§ {threshold_value:.4f} ‚úÖ")
            else:
                path_explanation.append(
                    f"- {feature_name} = {X_new[0, feature_id]:.4f} > {threshold_value:.4f} ‚úÖ")

        # Mostrar el camino de decisi√≥n
        st.markdown("\n".join(path_explanation))

        # Mostrar el c√≥digo que genera este camino de decisi√≥n
        code_path = DECISION_PATH_CODE
        show_code_with_download(
            code_path, "C√≥digo para generar el camino de decisi√≥n", "camino_decision.py")
    except Exception as e:
        st.error(f"Error al mostrar el camino de decisi√≥n: {str(e)}")
        st.info(
            "Intenta reformatear los datos de entrada o verificar que el modelo sea compatible.")


def neural_network_diagnostics(history, config):
    st.markdown("### üîç Diagn√≥stico del Entrenamiento")

    # Calcular m√©tricas diagn√≥sticas
    diagnostics = analyze_training_diagnostics(history, config)

    # Mostrar diagn√≥sticos en pesta√±as
    st.markdown("### üéØ Estado General")
    show_general_health(diagnostics, history, config)

    st.markdown("### üìä Tendencias")
    show_trend_analysis(diagnostics, history)

    st.markdown("### ‚ö†Ô∏è Alertas")
    show_training_alerts(diagnostics, history, config)

    # SECCI√ìN 5: RECOMENDACIONES INTELIGENTES
    st.markdown("### üí° Recomendaciones")
    recommendations = generate_training_recommendations(
        diagnostics, history, config)

    if recommendations['excellent']:
        st.success("üåü " + recommendations['excellent'])
    elif recommendations['good']:
        st.info("‚úÖ " + recommendations['good'])
    elif recommendations['warning']:
        st.warning("‚ö†Ô∏è " + recommendations['warning'])
    elif recommendations['critical']:
        st.error("üö® " + recommendations['critical'])

    # SECCI√ìN 6: ACCIONES SUGERIDAS
    if recommendations.get('actions'):
        with st.expander("üîß Acciones Sugeridas", expanded=False):
            for action in recommendations['actions']:
                st.markdown(f"‚Ä¢ {action}")


def analyze_training_diagnostics(history, config):
    """Analiza el historial INCLUYENDO m√©tricas de rendimiento real."""
    diagnostics = {}

    loss_values = history.history['loss']
    epochs = len(loss_values)

    # 1. An√°lisis de convergencia de p√©rdida (como antes)
    if epochs >= 10:
        early_loss = np.mean(loss_values[:epochs//4])
        late_loss = np.mean(loss_values[-epochs//4:])
        convergence_rate = (early_loss - late_loss) / early_loss
    else:
        convergence_rate = (loss_values[0] - loss_values[-1]) / loss_values[0]

    diagnostics['convergence_rate'] = convergence_rate
    diagnostics['loss_converged'] = convergence_rate > 0.01

    # 2. An√°lisis de sobreajuste (como antes)
    if 'val_loss' in history.history:
        val_loss = history.history['val_loss']
        train_loss = loss_values

        window = min(5, epochs//2)
        recent_gap = np.mean(val_loss[-window:]) - \
            np.mean(train_loss[-window:])
        relative_gap = recent_gap / np.mean(train_loss[-window:])

        diagnostics['overfitting_gap'] = relative_gap
        diagnostics['is_overfitting'] = relative_gap > 0.15
    else:
        diagnostics['overfitting_gap'] = 0
        diagnostics['is_overfitting'] = False

    # 3. An√°lisis de estabilidad (como antes)
    if epochs >= 5:
        recent_losses = loss_values[-5:]
        stability = np.std(recent_losses) / np.mean(recent_losses)
        diagnostics['stability'] = stability
        diagnostics['loss_stable'] = stability < 0.05
    else:
        diagnostics['stability'] = float('inf')
        diagnostics['loss_stable'] = False

    # üöÄ 4. NUEVO: AN√ÅLISIS DE RENDIMIENTO REAL
    if config['task_type'] == 'Clasificaci√≥n':
        if 'accuracy' in history.history:
            # Accuracy de entrenamiento
            train_acc = history.history['accuracy'][-1]
            diagnostics['final_train_accuracy'] = train_acc

            # Accuracy de validaci√≥n (si existe)
            if 'val_accuracy' in history.history:
                val_acc = history.history['val_accuracy'][-1]
                diagnostics['final_val_accuracy'] = val_acc

                # üéØ CRITERIOS REALISTAS DE CALIDAD
                diagnostics['good_train_accuracy'] = train_acc > 0.7
                diagnostics['good_val_accuracy'] = val_acc > 0.7
                diagnostics['excellent_val_accuracy'] = val_acc > 0.85

                # Detectar sobreajuste por accuracy
                acc_gap = train_acc - val_acc
                diagnostics['accuracy_overfitting'] = acc_gap > 0.15

            else:
                # Solo tenemos accuracy de entrenamiento
                diagnostics['good_train_accuracy'] = train_acc > 0.7
                diagnostics['good_val_accuracy'] = False  # No disponible
                diagnostics['excellent_val_accuracy'] = False
                diagnostics['accuracy_overfitting'] = False
        else:
            # No hay m√©tricas de accuracy
            diagnostics['final_train_accuracy'] = None
            diagnostics['good_train_accuracy'] = False
            diagnostics['good_val_accuracy'] = False
            diagnostics['excellent_val_accuracy'] = False
            diagnostics['accuracy_overfitting'] = False

    else:  # Regresi√≥n
        if 'mae' in history.history:
            final_mae = history.history['mae'][-1]
            diagnostics['final_mae'] = final_mae

            if 'val_mae' in history.history:
                val_mae = history.history['val_mae'][-1]
                diagnostics['final_val_mae'] = val_mae

                # Para regresi√≥n, necesitamos contexto del rango de datos
                # Por ahora, usamos heur√≠sticas generales
                diagnostics['good_mae'] = val_mae < np.mean(
                    history.history['mae'][:5])
            else:
                diagnostics['good_mae'] = final_mae < np.mean(
                    history.history['mae'][:5])
        else:
            diagnostics['good_mae'] = False

    # üéØ 5. EVALUACI√ìN INTEGRAL DE CALIDAD
    # Ahora consideramos TANTO la curva de p√©rdida COMO el rendimiento real

    if config['task_type'] == 'Clasificaci√≥n':
        # Para clasificaci√≥n, el accuracy es lo m√°s importante
        if diagnostics.get('good_val_accuracy', False):
            diagnostics['overall_quality'] = 'excellent' if diagnostics.get(
                'excellent_val_accuracy', False) else 'good'
        elif diagnostics.get('good_train_accuracy', False):
            diagnostics['overall_quality'] = 'moderate'  # Solo bueno en train
        else:
            diagnostics['overall_quality'] = 'poor'  # Accuracy bajo
    else:
        # Para regresi√≥n
        if diagnostics.get('good_mae', False):
            diagnostics['overall_quality'] = 'good'
        else:
            diagnostics['overall_quality'] = 'poor'

    # 6. Combinar criterios de p√©rdida Y rendimiento
    diagnostics['converged'] = (
        diagnostics['loss_converged'] and
        diagnostics['overall_quality'] in ['excellent', 'good']
    )

    diagnostics['is_stable'] = (
        diagnostics['loss_stable'] and
        diagnostics['overall_quality'] != 'poor'
    )

    return diagnostics


def show_general_health(diagnostics, history, config):
    """Muestra el estado general INCLUYENDO rendimiento real."""

    health_score = 0
    total_checks = 0

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("**üìã Checklist de Salud:**")

        # 1. Convergencia (p√©rdida + rendimiento)
        if diagnostics['converged']:
            st.success("‚úÖ Modelo convergi√≥ con buen rendimiento")
            health_score += 1
        else:
            if diagnostics['loss_converged']:
                st.warning("‚ö†Ô∏è P√©rdida convergi√≥ pero rendimiento bajo")
            else:
                st.error("‚ùå Modelo no convergi√≥ suficientemente")
        total_checks += 1

        # 2. Sobreajuste (p√©rdida + accuracy)
        overfitting_detected = diagnostics['is_overfitting'] or diagnostics.get(
            'accuracy_overfitting', False)
        if not overfitting_detected:
            st.success("‚úÖ Sin signos de sobreajuste")
            health_score += 1
        else:
            if diagnostics['is_overfitting']:
                st.error(
                    f"‚ùå Sobreajuste en p√©rdida (gap: {diagnostics['overfitting_gap']*100:.1f}%)")
            if diagnostics.get('accuracy_overfitting', False):
                train_acc = diagnostics.get('final_train_accuracy', 0)
                val_acc = diagnostics.get('final_val_accuracy', 0)
                st.error(
                    f"‚ùå Sobreajuste en accuracy (train: {train_acc:.3f}, val: {val_acc:.3f})")
        total_checks += 1

        # 3. Estabilidad
        if diagnostics['is_stable']:
            st.success("‚úÖ Entrenamiento estable")
            health_score += 1
        else:
            st.warning(
                f"‚ö†Ô∏è Entrenamiento inestable (CV: {diagnostics['stability']*100:.1f}%)")
        total_checks += 1

        # üöÄ 4. NUEVO: RENDIMIENTO REAL
        if config['task_type'] == 'Clasificaci√≥n':
            if diagnostics.get('excellent_val_accuracy', False):
                st.success("üåü Excelente accuracy de validaci√≥n")
                health_score += 1
            elif diagnostics.get('good_val_accuracy', False):
                st.success("‚úÖ Buen accuracy de validaci√≥n")
                health_score += 1
            elif diagnostics.get('good_train_accuracy', False):
                st.warning("‚ö†Ô∏è Solo buen accuracy en entrenamiento")
            else:
                st.error("‚ùå Accuracy bajo (modelo no est√° aprendiendo bien)")
        else:
            if diagnostics.get('good_mae', False):
                st.success("‚úÖ Error de regresi√≥n aceptable")
                health_score += 1
            else:
                st.error("‚ùå Error de regresi√≥n alto")
        total_checks += 1

    with col2:
        st.markdown("**üéØ Puntuaci√≥n de Salud:**")

        health_percentage = (health_score / total_checks) * 100

        # üéØ NUEVA L√ìGICA: Considerar rendimiento real
        overall_quality = diagnostics.get('overall_quality', 'poor')

        if overall_quality == 'excellent' and health_percentage >= 75:
            st.success(f"üåü Excelente: {health_percentage:.0f}%")
            health_status = "üåü Modelo listo para producci√≥n"
        elif overall_quality == 'good' and health_percentage >= 60:
            st.info(f"üëç Bueno: {health_percentage:.0f}%")
            health_status = "üëç Modelo con buen rendimiento"
        elif overall_quality == 'moderate':
            st.warning(f"‚ö†Ô∏è Moderado: {health_percentage:.0f}%")
            health_status = "‚ö†Ô∏è Modelo necesita validaci√≥n adicional"
        else:
            st.error(f"üö® Cr√≠tico: {health_percentage:.0f}%")
            health_status = "üö® Modelo no est√° funcionando correctamente"

        st.info(health_status)

        # Mostrar m√©tricas espec√≠ficas
        if config['task_type'] == 'Clasificaci√≥n':
            if 'final_val_accuracy' in diagnostics and diagnostics['final_val_accuracy'] is not None:
                val_acc = diagnostics['final_val_accuracy']
                delta_color = "normal" if val_acc > 0.7 else "inverse"
                st.metric("üéØ Accuracy Validaci√≥n",
                          f"{val_acc:.3f}",
                          delta_color=delta_color,
                          help="M√©trica m√°s importante para clasificaci√≥n")
            elif 'final_train_accuracy' in diagnostics and diagnostics['final_train_accuracy'] is not None:
                train_acc = diagnostics['final_train_accuracy']
                st.metric("üéØ Accuracy Entrenamiento",
                          f"{train_acc:.3f}",
                          help="Solo disponible accuracy de entrenamiento")
        else:
            if 'final_val_mae' in diagnostics:
                st.metric("üìè MAE Validaci√≥n",
                          f"{diagnostics['final_val_mae']:.4f}",
                          help="Error promedio en validaci√≥n")


def generate_training_recommendations(diagnostics, history, config):
    """Genera recomendaciones basadas en p√©rdida Y rendimiento real."""

    recommendations = {
        'excellent': None,
        'good': None,
        'warning': None,
        'critical': None,
        'actions': []
    }

    # üéØ NUEVA L√ìGICA: Priorizar rendimiento real sobre curvas
    overall_quality = diagnostics.get('overall_quality', 'poor')

    # Contar problemas reales
    real_issues = []

    if not diagnostics['converged']:
        real_issues.append('convergencia')
    if diagnostics['is_overfitting'] or diagnostics.get('accuracy_overfitting', False):
        real_issues.append('sobreajuste')
    if not diagnostics['is_stable']:
        real_issues.append('estabilidad')
    if overall_quality == 'poor':
        real_issues.append('rendimiento_bajo')

    # Generar recomendaciones basadas en problemas reales
    if overall_quality == 'excellent' and len(real_issues) == 0:
        recommendations['excellent'] = "¬°Entrenamiento excelente! Modelo con alto rendimiento y buenas curvas."
        recommendations['actions'] = [
            "‚úÖ El modelo est√° listo para producci√≥n",
            "üìä Considera hacer validaci√≥n cruzada para confirmar robustez",
            "üöÄ Puedes proceder a hacer predicciones con confianza"
        ]
    elif overall_quality in ['good'] and len(real_issues) <= 1:
        recommendations['good'] = "Entrenamiento bueno con rendimiento satisfactorio."

        # Acciones espec√≠ficas basadas en el problema
        if 'sobreajuste' in real_issues:
            recommendations['actions'].extend([
                "üîß A√±adir m√°s regularizaci√≥n (dropout, L1/L2)",
                "üìà Aumentar datos de entrenamiento si es posible"
            ])
        elif 'estabilidad' in real_issues:
            recommendations['actions'].extend([
                "üìâ Reducir learning rate para mayor estabilidad",
                "üì¶ Aumentar batch size"
            ])
        else:
            recommendations['actions'].append("üëç Continuar con este enfoque")

    elif overall_quality == 'moderate' or len(real_issues) == 2:
        recommendations['warning'] = "Rendimiento moderado. El modelo funciona pero tiene limitaciones importantes."
        recommendations['actions'].extend([
            "üìä Revisar datos de validaci√≥n - pueden no ser representativos",
            "üîÑ Considerar reentrenar con diferentes hiperpar√°metros",
            "üéØ Evaluar si la arquitectura es apropiada para el problema"
        ])
    else:
        # Rendimiento cr√≠tico
        if overall_quality == 'poor':
            recommendations['critical'] = f"‚ö†Ô∏è CR√çTICO: El modelo tiene muy bajo rendimiento (accuracy ‚â§ 70%). Las curvas pueden verse bien pero el modelo no est√° aprendiendo correctamente."
        else:
            recommendations['critical'] = "El entrenamiento tiene m√∫ltiples problemas serios."

        recommendations['actions'].extend([
            "üö® PRIORIDAD: Revisar datos de entrada y preprocesamiento",
            "üèóÔ∏è Simplificar arquitectura del modelo",
            "üìö Verificar que el problema sea realmente solucionable con estos datos",
            "üîÑ Considerar cambiar completamente de enfoque"
        ])

    # A√±adir acciones espec√≠ficas para problemas de rendimiento
    if overall_quality == 'poor':
        recommendations['actions'].insert(
            0, "üéØ El modelo no est√° aprendiendo patrones √∫tiles - revisar datos y arquitectura")

    return recommendations


def show_trend_analysis(diagnostics, history):
    """Muestra an√°lisis de tendencias."""

    loss_values = history.history['loss']
    epochs = len(loss_values)

    st.markdown("### üìà An√°lisis de Tendencias por Fase")
    with st.expander("üìö ¬øQu√© significan las tendencias de p√©rdida?", expanded=False):
        st.markdown("""
        ### üéØ **Interpretaci√≥n de Tendencias por Fase:**
        
        **üü¢ Fase Inicial (Primeras √©pocas):**
        - **Descendente r√°pido:** ‚úÖ Excelente - El modelo est√° aprendiendo patrones b√°sicos
        - **Descendente lento:** ‚ö†Ô∏è Learning rate muy bajo o datos complejos
        - **Ascendente:** üö® Learning rate muy alto o problema en los datos
        
        **üü° Fase Media (√âpocas intermedias):**
        - **Descendente sostenido:** ‚úÖ Aprendizaje progresivo saludable
        - **Plateau temprano:** ‚ö†Ô∏è Posible saturaci√≥n o learning rate muy bajo
        - **Fluctuaciones:** üìä Normal, pero pueden indicar batch size peque√±o
        
        **üî¥ Fase Final (√öltimas √©pocas):**
        - **Estabilizado:** üéØ Ideal - Convergencia alcanzada
        - **Descendente:** üìà A√∫n aprendiendo - Considera m√°s √©pocas
        - **Ascendente:** üö® Sobreajuste - Detener entrenamiento antes
        
        ### üìä **Patrones de Calidad:**
        - **Curva logar√≠tmica suave:** Patr√≥n ideal de aprendizaje
        - **Escalones descendentes:** Learning rate scheduling efectivo
        - **Zigzag descendente:** Normal con batch gradient descent
        - **Valle en U:** Posible learning rate muy alto inicialmente
        """)
    if epochs >= 10:
        col1, col2, col3 = st.columns(3)
        # Dividir en segmentos para an√°lisis
        early_segment = loss_values[:epochs//3]
        middle_segment = loss_values[epochs//3:2*epochs//3]
        late_segment = loss_values[2*epochs//3:]

        early_trend = np.polyfit(
            range(len(early_segment)), early_segment, 1)[0]
        middle_trend = np.polyfit(
            range(len(middle_segment)), middle_segment, 1)[0]
        late_trend = np.polyfit(
            range(len(late_segment)), late_segment, 1)[0]

        with col1:
            # Interpretaci√≥n detallada para fase inicial
            direction = "Descendente" if early_trend < 0 else "Ascendente"

            if early_trend < -0.1:
                trend_quality = "üöÄ Excelente"
                trend_help = "Aprendizaje inicial muy efectivo"
            elif early_trend < -0.01:
                trend_quality = "‚úÖ Bueno"
                trend_help = "Aprendizaje inicial satisfactorio"
            elif early_trend < 0:
                trend_quality = "‚ö†Ô∏è Lento"
                trend_help = "Aprendizaje inicial lento - considera aumentar learning rate"
            else:
                trend_quality = "üö® Problema"
                trend_help = "P√©rdida aumentando - revisar learning rate y datos"

            st.metric("üü¢ Inicio (33%)",
                      f"{early_trend:.6f}",
                      f"{direction} - {trend_quality}",
                      help=trend_help)
        with col2:
            # Interpretaci√≥n detallada para fase media
            direction = "Descendente" if middle_trend < 0 else "Ascendente"

            if middle_trend < -0.01:
                trend_quality = "üìà Progresando"
                trend_help = "Aprendizaje continuo saludable"
            elif middle_trend < 0:
                trend_quality = "üìä Lento"
                trend_help = "Aprendizaje desacelerando - normal en fases medias"
            elif abs(middle_trend) < 0.001:
                trend_quality = "üéØ Plateau"
                trend_help = "Posible convergencia temprana"
            else:
                trend_quality = "‚ö†Ô∏è Subiendo"
                trend_help = "P√©rdida aumentando - posible sobreajuste"

            st.metric("üü° Medio (33%)",
                      f"{middle_trend:.6f}",
                      f"{direction} - {trend_quality}",
                      help=trend_help)
        with col3:
            # Interpretaci√≥n detallada para fase final
            direction = "Descendente" if late_trend < 0 else "Ascendente"

            if abs(late_trend) < 0.001:
                trend_quality = "üéØ Convergido"
                trend_help = "Excelente - modelo estabilizado"
            elif late_trend < -0.01:
                trend_quality = "üìà Mejorando"
                trend_help = "A√∫n aprendiendo - considera m√°s √©pocas"
            elif late_trend < 0:
                trend_quality = "üìä Lento"
                trend_help = "Mejora marginal - cerca de convergencia"
            else:
                trend_quality = "üö® Sobreajuste"
                trend_help = "P√©rdida aumentando - detener entrenamiento"

            st.metric("üî¥ Final (33%)",
                      f"{late_trend:.6f}",
                      f"{direction} - {trend_quality}",
                      help=trend_help)

         # An√°lisis comparativo entre fases
        st.markdown("### üîÑ An√°lisis Comparativo")

        col_comp1, col_comp2 = st.columns(2)

        with col_comp1:
            st.markdown("**üìä Velocidad de Aprendizaje:**")

            # Comparar velocidades de aprendizaje
            speeds = [abs(early_trend), abs(middle_trend), abs(late_trend)]
            phase_names = ["Inicial", "Media", "Final"]
            fastest_phase = phase_names[speeds.index(max(speeds))]

            st.info(f"üèÉ‚Äç‚ôÇÔ∏è **Fase m√°s activa:** {fastest_phase}")

            # Detectar aceleraci√≥n o desaceleraci√≥n
            if abs(early_trend) > abs(middle_trend) > abs(late_trend):
                st.success("‚úÖ Desaceleraci√≥n natural - Patr√≥n ideal")
            elif abs(late_trend) > abs(early_trend):
                st.warning("‚ö†Ô∏è Aceleraci√≥n tard√≠a - Revisar par√°metros")
            else:
                st.info("üìä Patr√≥n mixto de aprendizaje")

        with col_comp2:
            st.markdown("**üéØ Consistencia:**")

            # An√°lisis de consistencia
            trend_consistency = np.std([early_trend, middle_trend, late_trend])

            if trend_consistency < 0.01:
                st.success("üéØ Alta consistencia - Aprendizaje estable")
            elif trend_consistency < 0.1:
                st.info("üìä Consistencia moderada - Normal")
            else:
                st.warning("‚ö†Ô∏è Baja consistencia - Aprendizaje irregular")

            # Detectar patrones espec√≠ficos
            if early_trend < 0 and middle_trend < 0 and late_trend >= 0:
                st.error("üö® Patr√≥n de sobreajuste detectado")
            elif all(t < 0 for t in [early_trend, middle_trend, late_trend]):
                st.success("‚úÖ Mejora sostenida en todas las fases")
    else:
        st.info("üìä Historial muy corto para an√°lisis detallado")
        st.markdown("""
        **üîç Para un an√°lisis completo necesitas:**
        - ‚úÖ Al menos 10 √©pocas de entrenamiento
        - üìä Datos de validaci√≥n (recomendado)
        - üéØ M√©tricas adicionales seg√∫n el tipo de problema
        """)

    # Secci√≥n de patrones detectados con m√°s detalle
    st.markdown("### üîÑ Patrones de Comportamiento Detectados")

    # Detectar patrones m√°s espec√≠ficos
    patterns = []
    recommendations = []

    # An√°lisis de plateau
    if diagnostics.get('plateau', False):
        patterns.append(
            "üéØ **Plateau alcanzado:** El modelo ha llegado a su l√≠mite de aprendizaje")
        recommendations.append(
            "üí° Considera early stopping o cambiar arquitectura")

    # An√°lisis de convergencia
    convergence_rate = diagnostics.get('convergence_rate', 0)
    if convergence_rate > 0.5:
        patterns.append(
            "üöÄ **Convergencia r√°pida:** Excelente capacidad de aprendizaje")
        recommendations.append("‚úÖ Par√°metros bien ajustados")
    elif convergence_rate > 0.1:
        patterns.append(
            "üìà **Convergencia moderada:** Aprendizaje progresivo saludable")
        recommendations.append("üëç Rendimiento satisfactorio")
    elif convergence_rate > 0.01:
        patterns.append("üêå **Convergencia lenta:** Aprendizaje gradual")
        recommendations.append(
            "‚ö†Ô∏è Considera aumentar learning rate o revisar datos")
    else:
        patterns.append(
            "‚ùå **Sin convergencia:** Modelo no est√° aprendiendo efectivamente")
        recommendations.append("üö® Revisar completamente configuraci√≥n y datos")

    # An√°lisis de sobreajuste
    if diagnostics.get('is_overfitting', False):
        overfitting_gap = diagnostics.get('overfitting_gap', 0) * 100
        patterns.append(
            f"üìä **Sobreajuste progresivo:** Gap train/val del {overfitting_gap:.1f}%")
        recommendations.append("üõë Implementar regularizaci√≥n o early stopping")

    # An√°lisis de estabilidad
    if not diagnostics.get('is_stable', True):
        stability = diagnostics.get('stability', 0) * 100
        patterns.append(
            f"üìà **Entrenamiento inestable:** Variabilidad del {stability:.1f}%")
        recommendations.append("üîß Reducir learning rate o aumentar batch size")

    # Mostrar patrones y recomendaciones
    if patterns:
        col_pat1, col_pat2 = st.columns(2)

        with col_pat1:
            st.markdown("**üîç Patrones Identificados:**")
            for pattern in patterns:
                st.markdown(f"‚Ä¢ {pattern}")

        with col_pat2:
            st.markdown("**üí° Recomendaciones:**")
            for recommendation in recommendations:
                st.markdown(f"‚Ä¢ {recommendation}")
    else:
        st.success(
            "üéâ No se detectaron patrones problem√°ticos - ¬°Entrenamiento saludable!")

    # Secci√≥n de contexto educativo adicional
    with st.expander("üéì Contexto Educativo: ¬øC√≥mo interpretar estos n√∫meros?", expanded=False):
        st.markdown("""
        ### üìê **Entendiendo los Valores de Tendencia:**
        
        **Valores Negativos (Descendente):**
        - `-0.1` o menor: üöÄ Aprendizaje muy r√°pido
        - `-0.01` a `-0.1`: ‚úÖ Aprendizaje saludable  
        - `-0.001` a `-0.01`: üìä Aprendizaje gradual
        - `-0.0001` a `-0.001`: üéØ Convergencia fina
        
        **Valores Cerca de Cero:**
        - `-0.0001` a `+0.0001`: üéØ Convergencia ideal
        
        **Valores Positivos (Ascendente):**
        - `+0.0001` a `+0.001`: ‚ö†Ô∏è Ligero deterioro
        - `+0.001` a `+0.01`: üö® Problema moderado
        - `+0.01` o mayor: üí• Problema grave
        
        ### üî¨ **Factores que Afectan las Tendencias:**
        
        **Learning Rate:**
        - Muy alto ‚Üí Oscilaciones o divergencia
        - Muy bajo ‚Üí Convergencia lenta
        - Optimal ‚Üí Descenso suave y r√°pido
        
        **Batch Size:**
        - Peque√±o ‚Üí M√°s ruido, convergencia irregular
        - Grande ‚Üí Menos ruido, convergencia suave
        - Optimal ‚Üí Balance entre velocidad y estabilidad
        
        **Arquitectura del Modelo:**
        - Muy simple ‚Üí Plateau temprano (underfitting)
        - Muy compleja ‚Üí Sobreajuste tard√≠o
        - Apropiada ‚Üí Convergencia saludable sin sobreajuste
        """)


def show_training_alerts(diagnostics, history, config):
    """Muestra alertas y problemas detectados."""

    alerts = []

    # Alertas cr√≠ticas
    if not diagnostics['converged']:
        alerts.append({
            'level': 'error',
            'message': 'Modelo no convergi√≥ suficientemente',
            'action': 'Aumentar n√∫mero de √©pocas o ajustar learning rate'
        })

    if diagnostics['is_overfitting']:
        alerts.append({
            'level': 'error',
            'message': f'Sobreajuste detectado (gap: {diagnostics["overfitting_gap"]*100:.1f}%)',
            'action': 'Reducir complejidad del modelo, a√±adir regularizaci√≥n o m√°s datos'
        })

    # Alertas de advertencia
    if not diagnostics['is_stable']:
        alerts.append({
            'level': 'warning',
            'message': f'Entrenamiento inestable (variabilidad: {diagnostics["stability"]*100:.1f}%)',
            'action': 'Reducir learning rate o aumentar batch size'
        })

    if diagnostics.get('plateau', False):
        alerts.append({
            'level': 'info',
            'message': 'Plateau detectado en las √∫ltimas √©pocas',
            'action': 'El modelo puede haber alcanzado su l√≠mite de aprendizaje'
        })

    # Mostrar alertas
    if not alerts:
        st.success("üéâ ¬°No se detectaron problemas significativos!")
    else:
        for alert in alerts:
            if alert['level'] == 'error':
                st.error(f"üö® **{alert['message']}**\nüí° {alert['action']}")
            elif alert['level'] == 'warning':
                st.warning(f"‚ö†Ô∏è **{alert['message']}**\nüí° {alert['action']}")
            else:
                st.info(f"‚ÑπÔ∏è **{alert['message']}**\nüí° {alert['action']}")
